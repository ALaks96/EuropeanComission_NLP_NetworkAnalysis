{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below would have been the perfect pyspark ML data stream, although the results are extremely poor. The olny function that can be applied to two different spark dataframes are the approxSimilarityJoin functions for MinHashLSH and RandomBucketizedProjection algorithms. These two methods work on Term Frequency vectors that are then hashed and compute a similarity metric which much ressembles the Jaccard similarity. It seems the results are extremely poor, although this is the fastest and most efficient code we could come up with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only if needed (local):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "your_path = os.getcwd()\n",
    "findspark.init(your_path + 'spark-2.4.3-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from pyspark.sql import udf\n",
    "from pyspark.sql.types import StructType,StringType,StructField,FloatType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram, Word2Vec, HashingTF, MinHashLSH\n",
    "\n",
    "pyspark.autoBroadcastJoinThreshold = -1\n",
    "pyspark.broadcastTimeout = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading spark dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately the spark.read directly from the path gives broken up spark dataframes... We'll have to pass onto pandas then specify schema and load into spark dataframe. Ineffeicency point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['University', 'Program', 'Courses']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courseSchema = StructType([ #StructField('index', StringType(), True),\n",
    "                           StructField('University', StringType(), True)\\\n",
    "                           ,StructField(\"Program\", StringType(), True)\\\n",
    "                           ,StructField(\"Courses\", StringType(), True)\\\n",
    "                           ,StructField(\"text\", StringType(), True)])\n",
    "\n",
    "courses_pd = pd.read_csv(\"proto1/universities_full.csv\",index_col = 0)\n",
    "courses = spark.createDataFrame(courses_pd,schema=courseSchema)\n",
    "\n",
    "#courses = (spark.read\n",
    "#    .schema(courseSchema)\n",
    "#    .option(\"header\", \"true\")\n",
    "#    .option(\"mode\", \"DROPMALFORMED\")\n",
    "#    .csv(\"proto1/universities_full.csv\"))\n",
    "['University',\"Program\",\"Courses\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skillSchema = StructType([ #StructField('index', StringType(), True)\\,\n",
    "                          StructField(\"Skill\", StringType(), True)\\\n",
    "                          ,StructField(\"text\", StringType(), True)])\n",
    "#skillSchema = StructType([ StructField(\"text\", StringType(), True)])\n",
    "\n",
    "skills_pd = pd.read_csv(\"proto1/skills_full.csv\",index_col = 0)\n",
    "skills = spark.createDataFrame(skills_pd,schema=skillSchema)\n",
    "\n",
    "#skills = (spark.read\n",
    "#    .schema(skillSchema)\n",
    "#    .option(\"header\", \"true\")\n",
    "#    .option(\"mode\", \"DROPMALFORMED\")\n",
    "#    .csv(\"proto1/skills_full.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupation schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occupationSchema = StructType([ StructField(\"Occupation\", StringType(), True)\\\n",
    "#                               ,StructField(\"Skill\", StringType(), True)])\n",
    "\n",
    "occupations_pd = pd.read_csv('proto1/occupations_full.csv')\n",
    "#occupations = spark.createDataFrame(occupations_pd,schema=occupationSchema)\n",
    "\n",
    "#occupations = (spark.read\n",
    "#    .schema(occupationSchema)\n",
    "#    .option(\"header\", \"true\")\n",
    "#    .option(\"mode\", \"DROPMALFORMED\")\n",
    "#    .csv(\"proto1/occupations_full.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining pipeline for string similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining stopwords for pre-processing (faster than loading from nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopW = ['i','me','my', 'myself','we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    " 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these',\n",
    " 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    " 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    " 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    " 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    " 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
    " 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm',\n",
    " 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\",\n",
    " 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan',\n",
    " \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\",\"pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a small check first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = courses.select(\"text\").toDF(\"text\").filter(col('text').isNotNull())\n",
    "skill = skills.select(\"text\").toDF(\"text\").filter(col('text').isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the pipeline on the courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec trained on course strings\n",
    "model_course = Pipeline(stages=[\n",
    "    RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'text', outputCol = 'tokens'),\n",
    "    StopWordsRemover(stopWords = stopW, inputCol = 'tokens', outputCol = 'tokens_sw'),\n",
    "    HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\"),\n",
    "    #IDF(inputCol=\"rawFeatures\", outputCol=\"features\"),\n",
    "    MinHashLSH(inputCol=\"vectors\", outputCol=\"lsh\")\n",
    "]).fit(course)\n",
    "\n",
    "# Generating columns defined above\n",
    "course_hashed = model_course.transform(course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying them to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_hashed = model_course.transform(skill)\n",
    "course_hashed = model_course.transform(course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating dataframe with matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = model_skill.stages[-1].approxSimilarityJoin(course_hashed, skill_hashed, threshold = 100, distCol=\"Distance\")#.select('datasetA','datasetB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for Graph construction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting only text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches.withColumn(\"courses\", matches[\"datasetA\"][\"text\"]).withColumn(\"skills\", matches[\"datasetB\"][\"text\"]).select(\"courses\",'skills')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching course labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26072"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_join = matches.join(courses, courses.text == matches.courses, how='left') \n",
    "\n",
    "# Free space in cache !\n",
    "#matches.unpersist()\n",
    "#courses.unpersist()\n",
    "\n",
    "# Keeping only the interesting info\n",
    "left_join = left_join.select('University','Program','skills')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching skill labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = left_join.join(skills, skills.text == left_join.skills, how = 'left')\n",
    "\n",
    "# Free space in cache !\n",
    "left_join.unpersist()\n",
    "skills.unpersist()\n",
    "\n",
    "# Keeping only the interesting info\n",
    "full_data = full_data.select(\"University\",\"Program\",\"Skill\").withColumnRenamed(\"Skill\", \"Skill_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching occupation labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype1_data = full_data.join(occupations, occupations.Skill == full_data.Skill_description, how = 'left')\n",
    "\n",
    "# Free space in cache !\n",
    "full_data.unpersist()\n",
    "occupations.unpersist()\n",
    "\n",
    "prototype1_data = prototype1_data.select(\"University\",\"Program\",\"Skill\",\"Occupation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype1_data = prototype1_data.filter(col('Program').isNotNull()).filter(col('Skill').isNotNull()).filter(col('University').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|          University|             Program|Skill|          Occupation|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|telecommunication...|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|    software analyst|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|integration engineer|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|embedded system d...|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|     software tester|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|data warehouse de...|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|chief ICT securit...|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|enterprise architect|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|mobile applicatio...|\n",
      "|University of Ant...|Bachelor of Bioch...|   C#|ICT intelligent s...|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prototype1_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prototype1_data.write\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(\"pyspark__results_proto1_2.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas coversion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably the most time consuming process of all... If we could somehow avoid having to transfer data as pandas that would be great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = prototype1_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('pyspark__results_proto1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
