"X","Sector","Courses","Score","Syllabus","AI_topics","Matches_BySector","Matches_ByCourse","Avg_score_ByCourse","Matches_BySector_ByCourse","Avg_score_BySector_ByCourse"
423,"Agriculture, forestry and fishing","Applied data analysis",0.855847120285034,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A review on the practice of big data analysis in agriculture to the other research areas employing big data analysis, agriculture ranks at  stations, humans
as sensors, web-based data, GIS geospatial data, feeds from  web services, mobile applications,
statistical analysis, modeling, simulation, benchmarking, big data storage, message  
",180,120,0.829422693451246,28,0.836855694651604
429,"Agriculture, forestry and fishing","Applied data analysis",0.791546583175659,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Publicising food: big data, precision agriculture, and co-experimental techniques of addition A number of technological forms are thus investigated: eg, big data (big soil data, big climatedata, etc.), precision agriculture, and a variety of internet-based platforms utilised by
self-described activists and proponents of more local and regional based foodscapes  
",180,120,0.829422693451246,28,0.836855694651604
433,"Agriculture, forestry and fishing","Applied data analysis",0.796323657035828,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[HTML][HTML] Analysis of agriculture data using data mining techniques: application of big dataIn agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estimation for the ",180,120,0.829422693451246,28,0.836855694651604
438,"Agriculture, forestry and fishing","Applied data analysis",0.795054256916046,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","IoT based agriculture as a cloud and big data service: the beginning of digital India Agriculture System Mechanism QoS-aware (Parameter) Domains Data Classification Resource
Management Big Data  thedifferent classlabelsofusers.K-NNissupervisedmachinelearning
techniquewhich  Thefinalstepistointerprettheagriculturedatasubmittedbydifferentusersof  
",180,120,0.829422693451246,28,0.836855694651604
444,"Agriculture, forestry and fishing","Applied data analysis",0.845205843448639,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","IoT, big data science & analytics, cloud computing and mobile app based hybrid system for smart agriculture Aadhar linked agricultural information network can be easily conceptualized using mobile
communication and big data analytics operating on geo-spatial data already available with Ministry
of agriculture, ISRO, Survey of India to optimize resource availability, spread and  
",180,120,0.829422693451246,28,0.836855694651604
445,"Agriculture, forestry and fishing","Applied data analysis",0.846612691879272,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Estimating the environmental impact of agriculture by means of geospatial and Big Data analysis: The case of Catalonia develop online tools that allow policymakers to perceive, visualize and analyze the impact ofagriculture, facilitating decision making towards mitigating or eliminating negative effects on the
environment. Big data analysis is crucial for analyzing vast amounts of data (eg weather  
",180,120,0.829422693451246,28,0.836855694651604
451,"Agriculture, forestry and fishing","Applied data analysis",0.84677791595459,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Effective use of Big Data Analytics in Crop planning to increase Agriculture Production in India use of multi-sensor data such as satellites, IoT, and drones, and artificial intelligence algorithms
to  that collect data from internet and store it to an open Agriculture database with  The data clusteringalgorithm will run inside of Hadoop platform that offers parallel processing and  
",180,120,0.829422693451246,28,0.836855694651604
462,"Agriculture, forestry and fishing","Applied data analysis",0.808684527873993,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","From the Dust Bowl to Drones to Big Data: The Next Revolution in Agriculture Precision Agriculture and Big Data While precision agriculture (PA) and big data are related,
they are not  across years) variability asso- ciated with all aspects of agricultural pro- duction (Figure
1). Big data refers to the collection, analysis, and synthesis of large data sets that  
",180,120,0.829422693451246,28,0.836855694651604
468,"Agriculture, forestry and fishing","Applied data analysis",0.79755574464798,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Machine learning and data mining advance predictive big data analysis in precision animal agriculturePrecision animal agriculture is poised to rise to prominence in the livestock enterprise in the domains of management, production, welfare, sustainability, health surveillance, and environmental footprint. Considerable progress has been made in the use of tools to ",180,120,0.829422693451246,28,0.836855694651604
471,"Agriculture, forestry and fishing","Applied data analysis",0.84603214263916,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big Data and Climate Smart Agriculture-Review of Current Status and Implications for Agricultural Research and Innovation in India 23,24,25 . This difference from essentially data driven business analytics is most critical for
applications of big data analytics in scientific knowledge discovery domains such as agriculture.
Perhaps no other area is so alluring for big data-based innovations than  
",180,120,0.829422693451246,28,0.836855694651604
477,"Agriculture, forestry and fishing","Applied data analysis",0.806334495544434,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","0245 Big data and occupational health vigilance: use of french medico-administrative databases for hypothesis generation regarding occupational risks in agriculture Poster Presentation. Methodology. 0245 Big data and occupational health vigilance: use of 
medico-administrative databases for hypothesis generation regarding occupational risks inagriculture  complementary methods relying on exploitation of already existing data, such as  
",180,120,0.829422693451246,28,0.836855694651604
483,"Agriculture, forestry and fishing","Applied data analysis",0.797851800918579,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Is big data driving a paradigm shift in precision agriculture Much of the discussion focuses on whether the arrival of big data signals the emergence of or
need for a new  Adoption, profitability, and making better use of precision farming data  The research
status on precision agriculture by use of bibliometric analysis from three databases  
",180,120,0.829422693451246,28,0.836855694651604
486,"Agriculture, forestry and fishing","Applied data analysis",0.836458146572113,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM: Data to decisionsBig data are frequently used in many facets of business and agronomy to enhance knowledge needed to improve operational decisions. Livestock operations collect data of sufficient quantity to perform predictive analytics. Predictive analytics can be defined as a ",180,120,0.829422693451246,28,0.836855694651604
512,"Agriculture, forestry and fishing","Applied data analysis",0.828519105911255,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Application of Data Warehouse and Big Data Technology in Agriculture in IndiaIn the recent years, it is observed that the scientist, planners, executives across the globe are using data collected from traditional record keeping by government agencies, data collected using sensors and satellite imagery technologies and combining it with predictive weather ",180,120,0.829422693451246,28,0.836855694651604
519,"Agriculture, forestry and fishing","Applied data analysis",0.806545853614807,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Research and Application of Spark Platform on Big Data Processing in Intelligent Agriculture of Jilin Province (2) The agricultural big data processing method of intelligent agriculture in Jilin  algorithm
characterized by dynamic and rapid expansion, combining the Spark streaming flow calculation
framework, able to real-time analyze continuous and rapid changes in the massive data  
",180,120,0.829422693451246,28,0.836855694651604
521,"Agriculture, forestry and fishing","Applied data analysis",0.879506289958954,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Agriculture Big Data (AgBD) Challenges and Opportunities From Farm To Table: A Midwest Big Data Hub Community Whitepaper of learning samples) underlying common machine learning and big data analytics methods  need
to develop computationally scalable methods to analyze spatiotemporal datasets in agriculture 
to support researchers to develop scalable spatiotemporal data analytics methods  
",180,120,0.829422693451246,28,0.836855694651604
525,"Agriculture, forestry and fishing","Applied data analysis",0.866490423679352,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] A review on big data analytics in the field of agriculture Processing OLTP Analytical Big data Processing  We plan to work on precisionagriculture techniques. DISTRIBUTED NOSQL DATABASE FILE SYSTEM
PROGRAMMING COLUMN-DATA MODEL DOCUMENT-DATA MODEL  
",180,120,0.829422693451246,28,0.836855694651604
531,"Agriculture, forestry and fishing","Applied data analysis",0.834749281406403,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Research on the Multi-agent Synergic Mechanism for the Opening and Sharing of Big Data in Chinese Agriculture The American agricultural big data system has the characteristics of taking the official data of
the ministry of agriculture as the core and rich data content [6] [7]. In order to integrate public data
of member states, the EU has built a normalized and standardized data sharing  
",180,120,0.829422693451246,28,0.836855694651604
542,"Agriculture, forestry and fishing","Applied data analysis",0.864127457141876,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big data analytics in agriculture and distribution channel with map reduce using ct image analysis provide best result [3] Precision agriculture presents
great  My proposed solution for this all type of problem use big data for distributed computing  as
use bossiness analytical application like pentaho BI that give 3d data visualization with  
",180,120,0.829422693451246,28,0.836855694651604
556,"Agriculture, forestry and fishing","Applied data analysis",0.831111013889313,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Productivity improvement in agriculture sector using big data tools Data of various benefits and polices provided by government from ministry of agriculture  second
part we implement prediction function for establish forecast data through kYmeans  2017
International Conference On Big Data Analytics and computational Intelligence (ICBDACI)  
",180,120,0.829422693451246,28,0.836855694651604
560,"Agriculture, forestry and fishing","Applied data analysis",0.86376941204071,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Processing of Big Data in Internet of Things and Precision Agriculture.The main focus of the paper is the analysis of various types of agriculture data and open source
operational databases and platforms for data collection and data warehousing suitable for storing
data obtained from the Internet of Things and Precision Agriculture. The methodical approach  
",180,120,0.829422693451246,28,0.836855694651604
566,"Agriculture, forestry and fishing","Applied data analysis",0.855144143104553,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Visualisation of Big Data in Agriculture and Rural Development There is also plan to implement a graphical user interface to allow user to add his/her
own data without a need of coding. 6.2 Developed applications 3D visualisation is bringing
new potential into analysis of Big data in the field of agriculture  
",180,120,0.829422693451246,28,0.836855694651604
570,"Agriculture, forestry and fishing","Applied data analysis",0.86067670583725,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry Predictive models developed using Big Data identify best management practices for achieving
the best  machine learning algorithms and rooted in comprehensive and reliable data- sets, provide 
recent decades, farmers have been intro- duced to precision agriculture, which is a  
",180,120,0.829422693451246,28,0.836855694651604
575,"Agriculture, forestry and fishing","Applied data analysis",0.833625018596649,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A Big Data Virtualization Role in Agriculture: A Comprehensive Review Page 11. A Big Data Virtualization Role in Agriculture  Fuzzy reasoning provides
uncertainty in both data and output.  Machine learning: It is another artificial intelligence
technique that allows both supervised and unsupervised methods  
",180,120,0.829422693451246,28,0.836855694651604
582,"Agriculture, forestry and fishing","Applied data analysis",0.849940180778503,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[CITATION][C] Analysis of agriculture data using data mining techniques: application of big data in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",180,120,0.829422693451246,28,0.836855694651604
585,"Agriculture, forestry and fishing","Applied data analysis",0.892282247543335,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback."," analyzes satellite data, market data, and weather data us- ing machine learning and big data
analytics to  is a relatively new company and evaluations of its program are forthcoming
(e-Agriculture, 2017  From Data to Decision All types of big data must go through a series of steps  
[PDF][PDF] How'Big Data'affects competition law analysis in Online Platforms and Agriculture: does one size fit all?",180,120,0.829422693451246,28,0.836855694651604
593,"Agriculture, forestry and fishing","Applied data analysis",0.840983629226685,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[CITATION][C]  Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning Techniques with Retail Scanner DataBig data analytics framework for agriculture",180,120,0.829422693451246,28,0.836855694651604
597,"Agriculture, forestry and fishing","Applied data analysis",0.854203760623932,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback."," for Big Data scholarship in the realm of food and agriculture  Data inconsistency: Data is misplaced
during capturing and filing hence the information is prone to errors  Big data analytics framework
for agricultural services system is a solution that enables farmers  
[CITATION][C] Big Data: Managing the Future's Agriculture and Natural Resource Systems",180,120,0.829422693451246,28,0.836855694651604
422,"Agriculture, forestry and fishing","Lab in data science",0.858559608459473,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","A review on the practice of big data analysis in agriculture to the other research areas employing big data analysis, agriculture ranks at  stations, humans
as sensors, web-based data, GIS geospatial data, feeds from  web services, mobile applications,
statistical analysis, modeling, simulation, benchmarking, big data storage, message  
",180,78,0.825876659307724,27,0.836768452767973
426,"Agriculture, forestry and fishing","Lab in data science",0.800044476985931,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Publicising food: big data, precision agriculture, and co-experimental techniques of addition A number of technological forms are thus investigated: eg, big data (big soil data, big climatedata, etc.), precision agriculture, and a variety of internet-based platforms utilised by
self-described activists and proponents of more local and regional based foodscapes  
",180,78,0.825876659307724,27,0.836768452767973
435,"Agriculture, forestry and fishing","Lab in data science",0.811487853527069,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","IoT based agriculture as a cloud and big data service: the beginning of digital India Agriculture System Mechanism QoS-aware (Parameter) Domains Data Classification Resource
Management Big Data  thedifferent classlabelsofusers.K-NNissupervisedmachinelearning
techniquewhich  Thefinalstepistointerprettheagriculturedatasubmittedbydifferentusersof  
",180,78,0.825876659307724,27,0.836768452767973
440,"Agriculture, forestry and fishing","Lab in data science",0.867414712905884,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","IoT, big data science & analytics, cloud computing and mobile app based hybrid system for smart agriculture Aadhar linked agricultural information network can be easily conceptualized using mobile
communication and big data analytics operating on geo-spatial data already available with Ministry
of agriculture, ISRO, Survey of India to optimize resource availability, spread and  
",180,78,0.825876659307724,27,0.836768452767973
449,"Agriculture, forestry and fishing","Lab in data science",0.821749091148376,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Estimating the environmental impact of agriculture by means of geospatial and Big Data analysis: The case of Catalonia develop online tools that allow policymakers to perceive, visualize and analyze the impact ofagriculture, facilitating decision making towards mitigating or eliminating negative effects on the
environment. Big data analysis is crucial for analyzing vast amounts of data (eg weather  
",180,78,0.825876659307724,27,0.836768452767973
450,"Agriculture, forestry and fishing","Lab in data science",0.871080636978149,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Effective use of Big Data Analytics in Crop planning to increase Agriculture Production in India use of multi-sensor data such as satellites, IoT, and drones, and artificial intelligence algorithms
to  that collect data from internet and store it to an open Agriculture database with  The data clusteringalgorithm will run inside of Hadoop platform that offers parallel processing and  
",180,78,0.825876659307724,27,0.836768452767973
460,"Agriculture, forestry and fishing","Lab in data science",0.821094393730164,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","From the Dust Bowl to Drones to Big Data: The Next Revolution in Agriculture Precision Agriculture and Big Data While precision agriculture (PA) and big data are related,
they are not  across years) variability asso- ciated with all aspects of agricultural pro- duction (Figure
1). Big data refers to the collection, analysis, and synthesis of large data sets that  
",180,78,0.825876659307724,27,0.836768452767973
465,"Agriculture, forestry and fishing","Lab in data science",0.805520713329315,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Machine learning and data mining advance predictive big data analysis in precision animal agriculturePrecision animal agriculture is poised to rise to prominence in the livestock enterprise in the domains of management, production, welfare, sustainability, health surveillance, and environmental footprint. Considerable progress has been made in the use of tools to ",180,78,0.825876659307724,27,0.836768452767973
473,"Agriculture, forestry and fishing","Lab in data science",0.836766064167023,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big Data and Climate Smart Agriculture-Review of Current Status and Implications for Agricultural Research and Innovation in India 23,24,25 . This difference from essentially data driven business analytics is most critical for
applications of big data analytics in scientific knowledge discovery domains such as agriculture.
Perhaps no other area is so alluring for big data-based innovations than  
",180,78,0.825876659307724,27,0.836768452767973
484,"Agriculture, forestry and fishing","Lab in data science",0.782299518585205,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Is big data driving a paradigm shift in precision agriculture Much of the discussion focuses on whether the arrival of big data signals the emergence of or
need for a new  Adoption, profitability, and making better use of precision farming data  The research
status on precision agriculture by use of bibliometric analysis from three databases  
",180,78,0.825876659307724,27,0.836768452767973
488,"Agriculture, forestry and fishing","Lab in data science",0.822557389736176,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM: Data to decisionsBig data are frequently used in many facets of business and agronomy to enhance knowledge needed to improve operational decisions. Livestock operations collect data of sufficient quantity to perform predictive analytics. Predictive analytics can be defined as a ",180,78,0.825876659307724,27,0.836768452767973
502,"Agriculture, forestry and fishing","Lab in data science",0.782825767993927,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Application of big data in precision agriculture.Precision agriculture is the future way for agricultural modernization. The rapid expansion of
agricultural data and the development of big data technology provide a new method for the
development of precision agriculture, and become an important force leading to the development  
",180,78,0.825876659307724,27,0.836768452767973
510,"Agriculture, forestry and fishing","Lab in data science",0.843094229698181,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Application of Data Warehouse and Big Data Technology in Agriculture in IndiaIn the recent years, it is observed that the scientist, planners, executives across the globe are using data collected from traditional record keeping by government agencies, data collected using sensors and satellite imagery technologies and combining it with predictive weather ",180,78,0.825876659307724,27,0.836768452767973
515,"Agriculture, forestry and fishing","Lab in data science",0.857653021812439,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Research and Application of Spark Platform on Big Data Processing in Intelligent Agriculture of Jilin Province (2) The agricultural big data processing method of intelligent agriculture in Jilin  algorithm
characterized by dynamic and rapid expansion, combining the Spark streaming flow calculation
framework, able to real-time analyze continuous and rapid changes in the massive data  
",180,78,0.825876659307724,27,0.836768452767973
520,"Agriculture, forestry and fishing","Lab in data science",0.884474098682404,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Agriculture Big Data (AgBD) Challenges and Opportunities From Farm To Table: A Midwest Big Data Hub Community Whitepaper of learning samples) underlying common machine learning and big data analytics methods  need
to develop computationally scalable methods to analyze spatiotemporal datasets in agriculture 
to support researchers to develop scalable spatiotemporal data analytics methods  
",180,78,0.825876659307724,27,0.836768452767973
526,"Agriculture, forestry and fishing","Lab in data science",0.850742220878601,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] A review on big data analytics in the field of agriculture Processing OLTP Analytical Big data Processing  We plan to work on precisionagriculture techniques. DISTRIBUTED NOSQL DATABASE FILE SYSTEM
PROGRAMMING COLUMN-DATA MODEL DOCUMENT-DATA MODEL  
",180,78,0.825876659307724,27,0.836768452767973
532,"Agriculture, forestry and fishing","Lab in data science",0.828789055347443,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Research on the Multi-agent Synergic Mechanism for the Opening and Sharing of Big Data in Chinese Agriculture The American agricultural big data system has the characteristics of taking the official data of
the ministry of agriculture as the core and rich data content [6] [7]. In order to integrate public data
of member states, the EU has built a normalized and standardized data sharing  
",180,78,0.825876659307724,27,0.836768452767973
541,"Agriculture, forestry and fishing","Lab in data science",0.86544144153595,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big data analytics in agriculture and distribution channel with map reduce using ct image analysis provide best result [3] Precision agriculture presents
great  My proposed solution for this all type of problem use big data for distributed computing  as
use bossiness analytical application like pentaho BI that give 3d data visualization with  
",180,78,0.825876659307724,27,0.836768452767973
551,"Agriculture, forestry and fishing","Lab in data science",0.829692482948303,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","What is cyber-physical system driven agriculture?-Redesign of big data for outstanding farmer management Theoretically, precision agriculture tools coupled with innovative data- mining procedures and
predictive models based on artificial intelligence, will be able to deliver personalized
recommendations at an appropriate spatial scale, so that agricultural productivity  
",180,78,0.825876659307724,27,0.836768452767973
557,"Agriculture, forestry and fishing","Lab in data science",0.817561507225037,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Productivity improvement in agriculture sector using big data tools Data of various benefits and polices provided by government from ministry of agriculture  second
part we implement prediction function for establish forecast data through kYmeans  2017
International Conference On Big Data Analytics and computational Intelligence (ICBDACI)  
",180,78,0.825876659307724,27,0.836768452767973
562,"Agriculture, forestry and fishing","Lab in data science",0.861434936523438,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Processing of Big Data in Internet of Things and Precision Agriculture.The main focus of the paper is the analysis of various types of agriculture data and open source
operational databases and platforms for data collection and data warehousing suitable for storing
data obtained from the Internet of Things and Precision Agriculture. The methodical approach  
",180,78,0.825876659307724,27,0.836768452767973
565,"Agriculture, forestry and fishing","Lab in data science",0.855307042598724,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Visualisation of Big Data in Agriculture and Rural Development There is also plan to implement a graphical user interface to allow user to add his/her
own data without a need of coding. 6.2 Developed applications 3D visualisation is bringing
new potential into analysis of Big data in the field of agriculture  
",180,78,0.825876659307724,27,0.836768452767973
573,"Agriculture, forestry and fishing","Lab in data science",0.848460674285889,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry Predictive models developed using Big Data identify best management practices for achieving
the best  machine learning algorithms and rooted in comprehensive and reliable data- sets, provide 
recent decades, farmers have been intro- duced to precision agriculture, which is a  
",180,78,0.825876659307724,27,0.836768452767973
577,"Agriculture, forestry and fishing","Lab in data science",0.819439947605133,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","A Big Data Virtualization Role in Agriculture: A Comprehensive Review Page 11. A Big Data Virtualization Role in Agriculture  Fuzzy reasoning provides
uncertainty in both data and output.  Machine learning: It is another artificial intelligence
technique that allows both supervised and unsupervised methods  
",180,78,0.825876659307724,27,0.836768452767973
583,"Agriculture, forestry and fishing","Lab in data science",0.841179490089417,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[CITATION][C] Analysis of agriculture data using data mining techniques: application of big data in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",180,78,0.825876659307724,27,0.836768452767973
587,"Agriculture, forestry and fishing","Lab in data science",0.85562938451767,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report."," analyzes satellite data, market data, and weather data us- ing machine learning and big data
analytics to  is a relatively new company and evaluations of its program are forthcoming
(e-Agriculture, 2017  From Data to Decision All types of big data must go through a series of steps  
[PDF][PDF] How'Big Data'affects competition law analysis in Online Platforms and Agriculture: does one size fit all?",180,78,0.825876659307724,27,0.836768452767973
598,"Agriculture, forestry and fishing","Lab in data science",0.852448463439941,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report."," for Big Data scholarship in the realm of food and agriculture  Data inconsistency: Data is misplaced
during capturing and filing hence the information is prone to errors  Big data analytics framework
for agricultural services system is a solution that enables farmers  
[CITATION][C] Big Data: Managing the Future's Agriculture and Natural Resource Systems",180,78,0.825876659307724,27,0.836768452767973
81,"Accomodation and food service activities","Applied data analysis",0.813414454460144,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Publicising food: big data, precision agriculture, and co-experimental techniques of addition interviews, involving 18 Iowa farmers, 14 individuals from big data industry (those involved in
the sale and promotion of large-scale data acquisition, predictive analytic software, and/or
precision agriculture technologies), and 19 interviews of regional food system entrepreneurs  
",145,120,0.829422693451246,24,0.823808113733927
85,"Accomodation and food service activities","Applied data analysis",0.873424232006073,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big data in food safety: An overview A list of the most used analysis methods for big data is shown in Table 3. These meth  These
systems are developed using data mining techniques (collaborative filtering, content based filtering
and hybrid  To the author's knowledge, these systems are not yet applied in food safety  
",145,120,0.829422693451246,24,0.823808113733927
90,"Accomodation and food service activities","Applied data analysis",0.812011003494263,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Facilitating knowledge management through filtered big data: SME competitiveness in an agri-food sector knowledge management (KM) process that utilises filtered big data within an agri-food supply
chain  The specific big data consumer analytics examined is those of the Tesco Clubcard data,
otherwise  17 million customers), with 10 per cent of this customer data being processed  
",145,120,0.829422693451246,24,0.823808113733927
96,"Accomodation and food service activities","Applied data analysis",0.852552175521851,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Alternative data mining/machine learning methods for the analytical evaluation of food quality and authenticityA reviewIn recent years, the variety and volume of data acquired by modern analytical instruments in order to conduct a better authentication of food has dramatically increased. Several pattern recognition tools have been developed to deal with the large volume and complexity of ",145,120,0.829422693451246,24,0.823808113733927
105,"Accomodation and food service activities","Applied data analysis",0.817241013050079,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big Data and the transformation of food and beverage marketing: undermining efforts to reduce obesity? of digital marketing, harnessing the power of Big Data analytics, artificial intelligence, and powerful 
rich views of behavioral patterns that can be highly valuable to food and beverage  For example,
leading data company Neustar can identify people who display the following  
",145,120,0.829422693451246,24,0.823808113733927
113,"Accomodation and food service activities","Applied data analysis",0.784634351730347,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Satellite data and machine learning for weather risk management and food securityThe increase in frequency and severity of extreme weather events poses challenges for the agricultural sector in developing economies and for food security globally. In this article, we demonstrate how machine learning can be used to mine satellite data and identify pixel ",145,120,0.829422693451246,24,0.823808113733927
116,"Accomodation and food service activities","Applied data analysis",0.800747454166412,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A semantic network analysis of big data regarding food exhibition at convention center The purpose of this study was to visualize the semantic network with big data related to food
exhibition at convention center. For this, this study collected data containing 'coex food
exhibition/bexco food exhibition' keywords from web pages and news on Google during one  
",145,120,0.829422693451246,24,0.823808113733927
122,"Accomodation and food service activities","Applied data analysis",0.829785525798798,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning Taking advantage of a geo-big data approach and multilevel regression model, we make a
contribution to current literature by  should offer deeper spatial insights into intra-urban foodscape
and provide more nuanced understanding of food deserts  2. Methodology and data. 2.1  
",145,120,0.829422693451246,24,0.823808113733927
128,"Accomodation and food service activities","Applied data analysis",0.804859101772308,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Nutritional Culturomics and Big Data: Macroscopic Patterns of Change in Food, Nutrition and Diet Choices and diet, their knowledge, awareness and understanding of the interdepend- ence of food
consumption, health  [15]), and mathematical tools to handle complex data sets (eg  application
of Artificial Intelligence for large-scale content analysis [18, 19] or random fractal theory to  
",145,120,0.829422693451246,24,0.823808113733927
131,"Accomodation and food service activities","Applied data analysis",0.840458273887634,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Research and application of big data-based co-regulation model in food safety governance.This paper is aimed to construct a food safety and nutrition information collection and analysis
platform with new internet technologies including big data to improve the analytic capacity and
the data-mining capability, and thereby to provide more accurate and comprehensive food  
",145,120,0.829422693451246,24,0.823808113733927
135,"Accomodation and food service activities","Applied data analysis",0.823520720005035,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Combining open data and machine learning to predict food security in EthiopiaFood security is commonly measured by means of surveys, requiring substantial time and budget. Open data can possibly serve as a cost-effective alternative to predict food security. In this paper a method is proposed that uses open data related to food insecurity drivers to ",145,120,0.829422693451246,24,0.823808113733927
142,"Accomodation and food service activities","Applied data analysis",0.808134317398071,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Can business generated big food data be used to understand food consumption behaviour and can a research infrastructure be generated around such data  platforms, (b) stakeholders along the food chain, and (c) policy actors in the agricultural-food
and nutrition-health  behaviour can be extracted from existing business generated data and on
which conditions these data might feed into a future RICHFIELDS big data platform  
",145,120,0.829422693451246,24,0.823808113733927
147,"Accomodation and food service activities","Applied data analysis",0.788235604763031,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big Data Analytics for Traceability in Food Supply Chain WAINA 2019: Web, Artificial Intelligence and Network Applications pp 880-884 | Cite as. Big Data
Analytics for Traceability in Food Supply Chain  The amount of socio-economic data generated
every day has grown dramatically in recent years thanks to the widespread use of the  
",145,120,0.829422693451246,24,0.823808113733927
152,"Accomodation and food service activities","Applied data analysis",0.824255287647247,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big data mining for predicting stochastic variables in food supply chains at different scales Abstract. Food products supply chains incorporate multiple scales in space and time in demand
and supply sides  Modern big data based data mining and machine learning and tools in the larger
domain of artificial intelligence are ideal for such complex problems  
",145,120,0.829422693451246,24,0.823808113733927
156,"Accomodation and food service activities","Applied data analysis",0.809002757072449,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Design and Realization of Food Safety Monitoring and Pre-Control System Based on Multi-Source and Big Data efficiency,poor timeliness,and incomplete data. In order to realize the sharing of resources and
information in the process of food safety monitoring,this study designed and developed a food
safety monitoring and control system based on the multi-source and big data under the  
",145,120,0.829422693451246,24,0.823808113733927
166,"Accomodation and food service activities","Applied data analysis",0.835298597812653,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Discussion on the Reform of Teaching Methods for Specialized Courses under the Background of Big Data-Taking Animal Food Technology as an Example E. Make full use of big data, help better feedback students' learning situation, and establish more 
feedback for the leaning situation of students using ""Xuexitong"" on the Animal Food Technology
Mooc  These detailed and specific data to be obtained, on the one hand, can help  
",145,120,0.829422693451246,24,0.823808113733927
174,"Accomodation and food service activities","Applied data analysis",0.839899837970734,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Systems Approach to Link Big Socio-ecological Geo-data to Food Systems Sustainability of (i) what information commonly needed by food system actors to response and adapt to
socio-ecological change and enhance the system performance, (ii) interoperability between
different types of data across scales, and (iii) sufficient guidance to utilize big data resources  
",145,120,0.829422693451246,24,0.823808113733927
175,"Accomodation and food service activities","Applied data analysis",0.845894634723663,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Emerging Data Governance Issues in Big Data Applications for Food SafetyThe food industry and food safety authorities show an increasing interest in Big Data applications. On the one hand, Big Data strengthens data storage, data mashup, and methodology of risk assessment; on the other hand, the presence of risks and challenges ",145,120,0.829422693451246,24,0.823808113733927
185,"Accomodation and food service activities","Applied data analysis",0.828786849975586,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A review on use of big data in warehousing to enhance accessibility of food and outreach across the agricultural value- chain is ensured by Big- Data methods and practices.
This information is spread across input providers and produce buyers. This paper will analyze
the lacunas in data accessibility which render the efficacy of adequate supply of food  
",145,120,0.829422693451246,24,0.823808113733927
190,"Accomodation and food service activities","Applied data analysis",0.83674830198288,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry make decisions that will increase yields and deliver safe, nutritious food to communities  Predictive
models developed using Big Data identify best management practices for achieving the  machinelearning algorithms and rooted in comprehensive and reliable data- sets, provide  
",145,120,0.829422693451246,24,0.823808113733927
196,"Accomodation and food service activities","Applied data analysis",0.871576368808746,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Food trend based on social media for big data analysis using K-mean clustering and SAW: A case study on yogyakarta culinary industry 1. Fig. 1. Big Data Processing Pipeline  From the data that has been done cleaning the data, then
convert in a bag of words matrix to be applied machine learning algorithm. Then perform feature
extraction to find relevant features in classifying trendy food data  
",145,120,0.829422693451246,24,0.823808113733927
200,"Accomodation and food service activities","Applied data analysis",0.839213907718658,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[CITATION][C] Big data applications in food safety and quality in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",145,120,0.829422693451246,24,0.823808113733927
207,"Accomodation and food service activities","Applied data analysis",0.799069821834564,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[CITATION][C]  -land Algorithm Based on Neighborhood and Temporal Anomalies (FANTA) to Map Planted Versus Fallowed Croplands Using MODIS Data to Assist in [CITATION][C] Identify how big data enables forecasting and demand planning in food and beverages industry",145,120,0.829422693451246,24,0.823808113733927
217,"Accomodation and food service activities","Applied data analysis",0.792630136013031,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[CITATION][C] Application of a business intelligence tool within the context of big data in a food industry companySmart ManufacturingPotential of New Digital Technologies and Big Data in the Food Industry",145,120,0.829422693451246,24,0.823808113733927
93,"Accomodation and food service activities","Big Data",0.80487984418869,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Facilitating knowledge management through filtered big data: SME competitiveness in an agri-food sector knowledge management (KM) process that utilises filtered big data within an agri-food supply
chain  The specific big data consumer analytics examined is those of the Tesco Clubcard data,
otherwise  17 million customers), with 10 per cent of this customer data being processed  
",145,99,0.823880515315316,21,0.822008445149376
98,"Accomodation and food service activities","Big Data",0.845151245594025,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Alternative data mining/machine learning methods for the analytical evaluation of food quality and authenticityA reviewIn recent years, the variety and volume of data acquired by modern analytical instruments in order to conduct a better authentication of food has dramatically increased. Several pattern recognition tools have been developed to deal with the large volume and complexity of ",145,99,0.823880515315316,21,0.822008445149376
103,"Accomodation and food service activities","Big Data",0.828841030597687,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","A big data and cloud computing specification, standards and architecture: agricultural and food informatics data and cloud computing specification, standards and architecture: agricultural and food
informatics  The real-time data storage and management architecture plays important role. This
paper introduces big data, includes the background and definitions, characteristics, related  
",145,99,0.823880515315316,21,0.822008445149376
106,"Accomodation and food service activities","Big Data",0.809598982334137,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Big Data and the transformation of food and beverage marketing: undermining efforts to reduce obesity? of digital marketing, harnessing the power of Big Data analytics, artificial intelligence, and powerful 
rich views of behavioral patterns that can be highly valuable to food and beverage  For example,
leading data company Neustar can identify people who display the following  
",145,99,0.823880515315316,21,0.822008445149376
117,"Accomodation and food service activities","Big Data",0.785262942314148,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","A semantic network analysis of big data regarding food exhibition at convention center The purpose of this study was to visualize the semantic network with big data related to food
exhibition at convention center. For this, this study collected data containing 'coex food
exhibition/bexco food exhibition' keywords from web pages and news on Google during one  
",145,99,0.823880515315316,21,0.822008445149376
124,"Accomodation and food service activities","Big Data",0.82272881269455,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning Taking advantage of a geo-big data approach and multilevel regression model, we make a
contribution to current literature by  should offer deeper spatial insights into intra-urban foodscape
and provide more nuanced understanding of food deserts  2. Methodology and data. 2.1  
",145,99,0.823880515315316,21,0.822008445149376
133,"Accomodation and food service activities","Big Data",0.828930020332336,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Research and application of big data-based co-regulation model in food safety governance.This paper is aimed to construct a food safety and nutrition information collection and analysis
platform with new internet technologies including big data to improve the analytic capacity and
the data-mining capability, and thereby to provide more accurate and comprehensive food  
",145,99,0.823880515315316,21,0.822008445149376
136,"Accomodation and food service activities","Big Data",0.811435222625732,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Combining open data and machine learning to predict food security in EthiopiaFood security is commonly measured by means of surveys, requiring substantial time and budget. Open data can possibly serve as a cost-effective alternative to predict food security. In this paper a method is proposed that uses open data related to food insecurity drivers to ",145,99,0.823880515315316,21,0.822008445149376
141,"Accomodation and food service activities","Big Data",0.821690380573273,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Can business generated big food data be used to understand food consumption behaviour and can a research infrastructure be generated around such data  platforms, (b) stakeholders along the food chain, and (c) policy actors in the agricultural-food
and nutrition-health  behaviour can be extracted from existing business generated data and on
which conditions these data might feed into a future RICHFIELDS big data platform  
",145,99,0.823880515315316,21,0.822008445149376
145,"Accomodation and food service activities","Big Data",0.802339136600494,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Big Data Analytics for Traceability in Food Supply Chain WAINA 2019: Web, Artificial Intelligence and Network Applications pp 880-884 | Cite as. Big Data
Analytics for Traceability in Food Supply Chain  The amount of socio-economic data generated
every day has grown dramatically in recent years thanks to the widespread use of the  
",145,99,0.823880515315316,21,0.822008445149376
150,"Accomodation and food service activities","Big Data",0.857210218906403,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Big data mining for predicting stochastic variables in food supply chains at different scales Abstract. Food products supply chains incorporate multiple scales in space and time in demand
and supply sides  Modern big data based data mining and machine learning and tools in the larger
domain of artificial intelligence are ideal for such complex problems  
",145,99,0.823880515315316,21,0.822008445149376
162,"Accomodation and food service activities","Big Data",0.840840697288513,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Recommender System Based Tensor Candecomp Parafact Algorithm-ALS to Handle Sparse Data In Food Commerce Information ServicesRecommender systems have been widely researched in many applications especially in e-commerce services with the aim to make clear and easy communication between consumer and provider. Simple examples of Recommender systems would include personal and ",145,99,0.823880515315316,21,0.822008445149376
171,"Accomodation and food service activities","Big Data",0.857988238334656,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Systems Approach to Link Big Socio-ecological Geo-data to Food Systems Sustainability of (i) what information commonly needed by food system actors to response and adapt to
socio-ecological change and enhance the system performance, (ii) interoperability between
different types of data across scales, and (iii) sufficient guidance to utilize big data resources  
",145,99,0.823880515315316,21,0.822008445149376
178,"Accomodation and food service activities","Big Data",0.826638221740723,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Emerging Data Governance Issues in Big Data Applications for Food SafetyThe food industry and food safety authorities show an increasing interest in Big Data applications. On the one hand, Big Data strengthens data storage, data mashup, and methodology of risk assessment; on the other hand, the presence of risks and challenges ",145,99,0.823880515315316,21,0.822008445149376
182,"Accomodation and food service activities","Big Data",0.808901369571686,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector of currently available studies on emerging technologies in agriculture and food sector (A&F  The
opportunities of the new big-data-augmented methodology are shown in comparison to existing 
with special attention to use of bigger volumes of data, machine learning and ontology  
",145,99,0.823880515315316,21,0.822008445149376
187,"Accomodation and food service activities","Big Data",0.816635370254517,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","A review on use of big data in warehousing to enhance accessibility of food and outreach across the agricultural value- chain is ensured by Big- Data methods and practices.
This information is spread across input providers and produce buyers. This paper will analyze
the lacunas in data accessibility which render the efficacy of adequate supply of food  
",145,99,0.823880515315316,21,0.822008445149376
191,"Accomodation and food service activities","Big Data",0.819434940814972,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry make decisions that will increase yields and deliver safe, nutritious food to communities  Predictive
models developed using Big Data identify best management practices for achieving the  machinelearning algorithms and rooted in comprehensive and reliable data- sets, provide  
",145,99,0.823880515315316,21,0.822008445149376
197,"Accomodation and food service activities","Big Data",0.853877127170563,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Food trend based on social media for big data analysis using K-mean clustering and SAW: A case study on yogyakarta culinary industry 1. Fig. 1. Big Data Processing Pipeline  From the data that has been done cleaning the data, then
convert in a bag of words matrix to be applied machine learning algorithm. Then perform feature
extraction to find relevant features in classifying trendy food data  
",145,99,0.823880515315316,21,0.822008445149376
204,"Accomodation and food service activities","Big Data",0.814481616020203,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[CITATION][C] Big data applications in food safety and quality in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",145,99,0.823880515315316,21,0.822008445149376
209,"Accomodation and food service activities","Big Data",0.792293131351471,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[CITATION][C]  -land Algorithm Based on Neighborhood and Temporal Anomalies (FANTA) to Map Planted Versus Fallowed Croplands Using MODIS Data to Assist in [CITATION][C] Identify how big data enables forecasting and demand planning in food and beverages industry",145,99,0.823880515315316,21,0.822008445149376
215,"Accomodation and food service activities","Big Data",0.813018798828125,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[CITATION][C] Application of a business intelligence tool within the context of big data in a food industry companySmart ManufacturingPotential of New Digital Technologies and Big Data in the Food Industry",145,99,0.823880515315316,21,0.822008445149376
238,"Human health and social work activities","Machine Learning",0.847951591014862,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[HTML][HTML] Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives The deep learning based approach we evaluate in this paper does not require any hand 
MIMIC-III contains de-identified clinical data of over 53,000 hospital admissions for adult patients 
approaches then use relevant concepts in a note as input to machine learning algorithms to  
",195,79,0.825409763975988,20,0.831634992361069
244,"Human health and social work activities","Machine Learning",0.87176388502121,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Comparing rule-based and deep learning models for patient phenotyping modern methods use relevant concepts in a note as input to a machine learning algorithm to  As
we mentioned before, the goal with our data is to understand phenotypes that are  validation of
this approach in other types of clinical notes such as social work assessment to  
",195,79,0.825409763975988,20,0.831634992361069
248,"Human health and social work activities","Machine Learning",0.841194927692413,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Predictive and prescriptive analytics, machine learning and child welfare risk assessment: The Broward County experience resonates well with both the American ethos of encouraging strong families and social work ideals
of  making and actuarial methods points to the need for computational and artificial intelligence
models that  are applied to an unseen sample set of data, the testing data set, to  
",195,79,0.825409763975988,20,0.831634992361069
253,"Human health and social work activities","Machine Learning",0.83177638053894,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Can Predictive Algorithms Assist Decision-Making in Social Work with Children and Families? (1985) explored the potential of what they refer to as 'artificial intelligence' to develop 'expert  every
time' Decision Support Systems and Child and Family Social Work  In another instance where
a big data approach has been developed but has yet to be applied, Schwartz et al  
",195,79,0.825409763975988,20,0.831634992361069
258,"Human health and social work activities","Machine Learning",0.868724584579468,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","A review of existing applications and techniques for narrative text analysis in electronic medical records ARC (Automated Retrieval Console): An algorithm based on an artificial intelligence program,
which  That result suggested that the major variety and contexts for the PHI in the social work notes
is more difficult to model  Data quality is an important barrier to NLP and text mining  
",195,79,0.825409763975988,20,0.831634992361069
279,"Human health and social work activities","Machine Learning",0.824308812618256,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Singularity academics across the globe, gives a precise definition of artificial intelligence, its constituency 
large degree of integration and cross-fertilization among AI, machine learning, statistics, control 
of shared theoretical frameworks, combined with the availability of data and processing  
",195,79,0.825409763975988,20,0.831634992361069
298,"Human health and social work activities","Machine Learning",0.783043503761292,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Health maintenance advisory technology for specific business sectors, eg utilities or tourism; G06Q50/10Services; G06Q50/22Socialwork  be passed to the query engine 72 as a confirmed result for machine learning algorithms
that  The user condition may be sensed by accessing such data from the server system, at  
",195,79,0.825409763975988,20,0.831634992361069
315,"Human health and social work activities","Machine Learning",0.837227821350098,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Methods and systems for growing and retaining the value of brand drugs by computer predictive model methods specially adapted for specific business sectors, eg utilities or tourism; G06Q50/10
Services; G06Q50/22Social work  [0042]. Learning Machinerefers to a  or semi-automated
process of generating a prediction based on a model, typically combining software and data  
",195,79,0.825409763975988,20,0.831634992361069
320,"Human health and social work activities","Machine Learning",0.883810877799988,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Hands on the wheel: Navigating algorithmic management and Uber drivers' autonomy More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,79,0.825409763975988,20,0.831634992361069
325,"Human health and social work activities","Machine Learning",0.849941730499268,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Sentiment analysis for hate speech detection on social media: TF-IDF weighted N-Grams based approach As such, preprocessing unstructured data is a very important role in the text classification  The
number of features can therefore be quite big for a corpus that is average sized  problems and
poses a significant problem to many machine learning algorithms (Yang & Pedersen  
",195,79,0.825409763975988,20,0.831634992361069
333,"Human health and social work activities","Machine Learning",0.811438620090485,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Analysing large volumes of complex qualitative data-Reflections from a group of international experts Georgia Philip is a Research Fellow in the School of Social Work, at the University of East Anglia 
analysis?' In so doing, he considers the advantages and challenges of using Machine Learning
to assist with coding and help researchers handle large volumes of data in a  
",195,79,0.825409763975988,20,0.831634992361069
340,"Human health and social work activities","Machine Learning",0.886739730834961,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[PDF][PDF] Hands on the wheel: Navigating algorithmic management and Uber drivers' More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,79,0.825409763975988,20,0.831634992361069
356,"Human health and social work activities","Machine Learning",0.805892527103424,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Evaluation of support and training sign language services at Setotolwane Secondary School DATA PRESENTATION, ANALYSIS AND INTERPRTATION ..... 27  Starner, Weaver and
Pentland (1998) present two real-time machine systems for  learners with hearing impairments
in using an educational game for learning the Sign Language  
",195,79,0.825409763975988,20,0.831634992361069
368,"Human health and social work activities","Machine Learning",0.787894010543823,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","[PDF][PDF] STATE OF THE NORTH There are five big challenges which this generation will need to meet as they gradually 
Digitalisation, artificial intelligence, machine learning and advanced robotics are of particular
focus for economists, and are starting to  Source: HMRC, 'Summary data tables' (HMRC 2017)  
",195,79,0.825409763975988,20,0.831634992361069
372,"Human health and social work activities","Machine Learning",0.813124001026154,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","The Future of MarketingAn Investigation into Disruption and Innovation Hoanca, 2015, p. 45) for market research, which will itself become automated. Privacy may
fall by the wayside as marketing applications use deep learning to  expected innovations around
analytics, Big Data, machine learning, and artificial intelligence  
",195,79,0.825409763975988,20,0.831634992361069
374,"Human health and social work activities","Machine Learning",0.808017075061798,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","The Future of MarketingAn Investigation into Disruption and Innovation Hoanca, 2015, p. 45) for market research, which will itself become automated. Privacy may
fall by the wayside as marketing applications use deep learning to  expected innovations around
analytics, Big Data, machine learning, and artificial intelligence  
",195,79,0.825409763975988,20,0.831634992361069
385,"Human health and social work activities","Machine Learning",0.836685717105865,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[PDF][PDF] Coding Over the Cracks: Predictive Analytics and Child Protection families and families of color.3 In this age of automation and artificial intelligence, a tempting  are
building and deploying tools that pull together vast quantities of data stored by  analytics and
explains the fundamentally human process of developing a machine learning algorithm  
",195,79,0.825409763975988,20,0.831634992361069
406,"Human health and social work activities","Machine Learning",0.83377742767334,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Re-examining and re-conceptualising Enterprise Search and Discovery capability: Towards a model for the factors and generative mechanisms for search task  225 5.6.2.3 Suboptimal Learning/Sharing culture  These include, the Statistical Machine (Goldberg
1927), Mundaneum (Otlet 1934), World Brain(Wells 1937), Universal  Organizations seek to exploit
'big data' volumes for differentiating insights supporting wealth creation  
",195,79,0.825409763975988,20,0.831634992361069
414,"Human health and social work activities","Machine Learning",0.784353375434875,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Suicide attempts of community adolescents and young adults: an explanatory and predictive epidemiological approach My first association with (co-)authorship is intensive learning (again). That's not the worst first
association  the focus away from risk factors towards risk algorithms, ie Machine Learning (ML).
ML is deemed to be well suited to investigate complex associations in data  
",195,79,0.825409763975988,20,0.831634992361069
417,"Human health and social work activities","Machine Learning",0.825033247470856,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Future Implications of the Psychopathy Construct for Criminology and Criminal Justice Policy and Practice Torture, 6 For example, Rhodes (2002) provides participation observation data showing that 
Sensors and machine learning algorithms have been designed that can measure affective
information  In addition, artificial intelligence advances that can make robots feel so to speak  
",195,79,0.825409763975988,20,0.831634992361069
425,"Agriculture, forestry and fishing","Big Data",0.812692523002625,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Publicising food: big data, precision agriculture, and co-experimental techniques of addition A number of technological forms are thus investigated: eg, big data (big soil data, big climatedata, etc.), precision agriculture, and a variety of internet-based platforms utilised by
self-described activists and proponents of more local and regional based foodscapes  
",180,99,0.823880515315316,20,0.826711839437485
432,"Agriculture, forestry and fishing","Big Data",0.798189759254456,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[HTML][HTML] Analysis of agriculture data using data mining techniques: application of big dataIn agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estimation for the ",180,99,0.823880515315316,20,0.826711839437485
436,"Agriculture, forestry and fishing","Big Data",0.810634195804596,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","IoT based agriculture as a cloud and big data service: the beginning of digital India Agriculture System Mechanism QoS-aware (Parameter) Domains Data Classification Resource
Management Big Data  thedifferent classlabelsofusers.K-NNissupervisedmachinelearning
techniquewhich  Thefinalstepistointerprettheagriculturedatasubmittedbydifferentusersof  
",180,99,0.823880515315316,20,0.826711839437485
448,"Agriculture, forestry and fishing","Big Data",0.824921846389771,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Estimating the environmental impact of agriculture by means of geospatial and Big Data analysis: The case of Catalonia develop online tools that allow policymakers to perceive, visualize and analyze the impact ofagriculture, facilitating decision making towards mitigating or eliminating negative effects on the
environment. Big data analysis is crucial for analyzing vast amounts of data (eg weather  
",180,99,0.823880515315316,20,0.826711839437485
453,"Agriculture, forestry and fishing","Big Data",0.836009562015533,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Effective use of Big Data Analytics in Crop planning to increase Agriculture Production in India use of multi-sensor data such as satellites, IoT, and drones, and artificial intelligence algorithms
to  that collect data from internet and store it to an open Agriculture database with  The data clusteringalgorithm will run inside of Hadoop platform that offers parallel processing and  
",180,99,0.823880515315316,20,0.826711839437485
459,"Agriculture, forestry and fishing","Big Data",0.78152322769165,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","New trends in precision agriculture: a novel cloud-based system for enabling data storage and agricultural task planning and automationIt is well-known that information and communication technologies enable many tasks in the context of precision agriculture. In fact, more and more farmers and food and agriculture companies are using precision agriculture-based systems to enhance not only their products ",180,99,0.823880515315316,20,0.826711839437485
461,"Agriculture, forestry and fishing","Big Data",0.819895565509796,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","From the Dust Bowl to Drones to Big Data: The Next Revolution in Agriculture Precision Agriculture and Big Data While precision agriculture (PA) and big data are related,
they are not  across years) variability asso- ciated with all aspects of agricultural pro- duction (Figure
1). Big data refers to the collection, analysis, and synthesis of large data sets that  
",180,99,0.823880515315316,20,0.826711839437485
472,"Agriculture, forestry and fishing","Big Data",0.841716527938843,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Big Data and Climate Smart Agriculture-Review of Current Status and Implications for Agricultural Research and Innovation in India 23,24,25 . This difference from essentially data driven business analytics is most critical for
applications of big data analytics in scientific knowledge discovery domains such as agriculture.
Perhaps no other area is so alluring for big data-based innovations than  
",180,99,0.823880515315316,20,0.826711839437485
480,"Agriculture, forestry and fishing","Big Data",0.804339826107025,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Is big data driving a paradigm shift in precision agriculture Much of the discussion focuses on whether the arrival of big data signals the emergence of or
need for a new  Adoption, profitability, and making better use of precision farming data  The research
status on precision agriculture by use of bibliometric analysis from three databases  
",180,99,0.823880515315316,20,0.826711839437485
500,"Agriculture, forestry and fishing","Big Data",0.784601807594299,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Application of big data in precision agriculture.Precision agriculture is the future way for agricultural modernization. The rapid expansion of
agricultural data and the development of big data technology provide a new method for the
development of precision agriculture, and become an important force leading to the development  
",180,99,0.823880515315316,20,0.826711839437485
517,"Agriculture, forestry and fishing","Big Data",0.816164076328278,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Research and Application of Spark Platform on Big Data Processing in Intelligent Agriculture of Jilin Province (2) The agricultural big data processing method of intelligent agriculture in Jilin  algorithm
characterized by dynamic and rapid expansion, combining the Spark streaming flow calculation
framework, able to real-time analyze continuous and rapid changes in the massive data  
",180,99,0.823880515315316,20,0.826711839437485
523,"Agriculture, forestry and fishing","Big Data",0.871011912822723,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Agriculture Big Data (AgBD) Challenges and Opportunities From Farm To Table: A Midwest Big Data Hub Community Whitepaper of learning samples) underlying common machine learning and big data analytics methods  need
to develop computationally scalable methods to analyze spatiotemporal datasets in agriculture 
to support researchers to develop scalable spatiotemporal data analytics methods  
",180,99,0.823880515315316,20,0.826711839437485
533,"Agriculture, forestry and fishing","Big Data",0.826619565486908,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Research on the Multi-agent Synergic Mechanism for the Opening and Sharing of Big Data in Chinese Agriculture The American agricultural big data system has the characteristics of taking the official data of
the ministry of agriculture as the core and rich data content [6] [7]. In order to integrate public data
of member states, the EU has built a normalized and standardized data sharing  
",180,99,0.823880515315316,20,0.826711839437485
552,"Agriculture, forestry and fishing","Big Data",0.829149067401886,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","What is cyber-physical system driven agriculture?-Redesign of big data for outstanding farmer management Theoretically, precision agriculture tools coupled with innovative data- mining procedures and
predictive models based on artificial intelligence, will be able to deliver personalized
recommendations at an appropriate spatial scale, so that agricultural productivity  
",180,99,0.823880515315316,20,0.826711839437485
561,"Agriculture, forestry and fishing","Big Data",0.861899495124817,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Processing of Big Data in Internet of Things and Precision Agriculture.The main focus of the paper is the analysis of various types of agriculture data and open source
operational databases and platforms for data collection and data warehousing suitable for storing
data obtained from the Internet of Things and Precision Agriculture. The methodical approach  
",180,99,0.823880515315316,20,0.826711839437485
569,"Agriculture, forestry and fishing","Big Data",0.835592746734619,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Visualisation of Big Data in Agriculture and Rural Development There is also plan to implement a graphical user interface to allow user to add his/her
own data without a need of coding. 6.2 Developed applications 3D visualisation is bringing
new potential into analysis of Big data in the field of agriculture  
",180,99,0.823880515315316,20,0.826711839437485
574,"Agriculture, forestry and fishing","Big Data",0.846613585948944,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry Predictive models developed using Big Data identify best management practices for achieving
the best  machine learning algorithms and rooted in comprehensive and reliable data- sets, provide 
recent decades, farmers have been intro- duced to precision agriculture, which is a  
",180,99,0.823880515315316,20,0.826711839437485
576,"Agriculture, forestry and fishing","Big Data",0.828875601291656,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","A Big Data Virtualization Role in Agriculture: A Comprehensive Review Page 11. A Big Data Virtualization Role in Agriculture  Fuzzy reasoning provides
uncertainty in both data and output.  Machine learning: It is another artificial intelligence
technique that allows both supervised and unsupervised methods  
",180,99,0.823880515315316,20,0.826711839437485
586,"Agriculture, forestry and fishing","Big Data",0.864622533321381,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course."," analyzes satellite data, market data, and weather data us- ing machine learning and big data
analytics to  is a relatively new company and evaluations of its program are forthcoming
(e-Agriculture, 2017  From Data to Decision All types of big data must go through a series of steps  
[PDF][PDF] How'Big Data'affects competition law analysis in Online Platforms and Agriculture: does one size fit all?",180,99,0.823880515315316,20,0.826711839437485
599,"Agriculture, forestry and fishing","Big Data",0.839163362979889,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course."," for Big Data scholarship in the realm of food and agriculture  Data inconsistency: Data is misplaced
during capturing and filing hence the information is prone to errors  Big data analytics framework
for agricultural services system is a solution that enables farmers  
[CITATION][C] Big Data: Managing the Future's Agriculture and Natural Resource Systems",180,99,0.823880515315316,20,0.826711839437485
80,"Accomodation and food service activities","Lab in data science",0.816579043865204,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Publicising food: big data, precision agriculture, and co-experimental techniques of addition interviews, involving 18 Iowa farmers, 14 individuals from big data industry (those involved in
the sale and promotion of large-scale data acquisition, predictive analytic software, and/or
precision agriculture technologies), and 19 interviews of regional food system entrepreneurs  
",145,78,0.825876659307724,18,0.817527231242922
86,"Accomodation and food service activities","Lab in data science",0.858418047428131,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big data in food safety: An overview A list of the most used analysis methods for big data is shown in Table 3. These meth  These
systems are developed using data mining techniques (collaborative filtering, content based filtering
and hybrid  To the author's knowledge, these systems are not yet applied in food safety  
",145,78,0.825876659307724,18,0.817527231242922
94,"Accomodation and food service activities","Lab in data science",0.80475527048111,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Facilitating knowledge management through filtered big data: SME competitiveness in an agri-food sector knowledge management (KM) process that utilises filtered big data within an agri-food supply
chain  The specific big data consumer analytics examined is those of the Tesco Clubcard data,
otherwise  17 million customers), with 10 per cent of this customer data being processed  
",145,78,0.825876659307724,18,0.817527231242922
109,"Accomodation and food service activities","Lab in data science",0.804791510105133,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big Data and the transformation of food and beverage marketing: undermining efforts to reduce obesity? of digital marketing, harnessing the power of Big Data analytics, artificial intelligence, and powerful 
rich views of behavioral patterns that can be highly valuable to food and beverage  For example,
leading data company Neustar can identify people who display the following  
",145,78,0.825876659307724,18,0.817527231242922
115,"Accomodation and food service activities","Lab in data science",0.802383363246918,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","A semantic network analysis of big data regarding food exhibition at convention center The purpose of this study was to visualize the semantic network with big data related to food
exhibition at convention center. For this, this study collected data containing 'coex food
exhibition/bexco food exhibition' keywords from web pages and news on Google during one  
",145,78,0.825876659307724,18,0.817527231242922
134,"Accomodation and food service activities","Lab in data science",0.827193140983582,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Research and application of big data-based co-regulation model in food safety governance.This paper is aimed to construct a food safety and nutrition information collection and analysis
platform with new internet technologies including big data to improve the analytic capacity and
the data-mining capability, and thereby to provide more accurate and comprehensive food  
",145,78,0.825876659307724,18,0.817527231242922
137,"Accomodation and food service activities","Lab in data science",0.807846665382385,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Combining open data and machine learning to predict food security in EthiopiaFood security is commonly measured by means of surveys, requiring substantial time and budget. Open data can possibly serve as a cost-effective alternative to predict food security. In this paper a method is proposed that uses open data related to food insecurity drivers to ",145,78,0.825876659307724,18,0.817527231242922
140,"Accomodation and food service activities","Lab in data science",0.823404014110565,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Can business generated big food data be used to understand food consumption behaviour and can a research infrastructure be generated around such data  platforms, (b) stakeholders along the food chain, and (c) policy actors in the agricultural-food
and nutrition-health  behaviour can be extracted from existing business generated data and on
which conditions these data might feed into a future RICHFIELDS big data platform  
",145,78,0.825876659307724,18,0.817527231242922
146,"Accomodation and food service activities","Lab in data science",0.797933101654053,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big Data Analytics for Traceability in Food Supply Chain WAINA 2019: Web, Artificial Intelligence and Network Applications pp 880-884 | Cite as. Big Data
Analytics for Traceability in Food Supply Chain  The amount of socio-economic data generated
every day has grown dramatically in recent years thanks to the widespread use of the  
",145,78,0.825876659307724,18,0.817527231242922
151,"Accomodation and food service activities","Lab in data science",0.834455072879791,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big data mining for predicting stochastic variables in food supply chains at different scales Abstract. Food products supply chains incorporate multiple scales in space and time in demand
and supply sides  Modern big data based data mining and machine learning and tools in the larger
domain of artificial intelligence are ideal for such complex problems  
",145,78,0.825876659307724,18,0.817527231242922
159,"Accomodation and food service activities","Lab in data science",0.78914886713028,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Design and Realization of Food Safety Monitoring and Pre-Control System Based on Multi-Source and Big Data efficiency,poor timeliness,and incomplete data. In order to realize the sharing of resources and
information in the process of food safety monitoring,this study designed and developed a food
safety monitoring and control system based on the multi-source and big data under the  
",145,78,0.825876659307724,18,0.817527231242922
177,"Accomodation and food service activities","Lab in data science",0.82670933008194,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Emerging Data Governance Issues in Big Data Applications for Food SafetyThe food industry and food safety authorities show an increasing interest in Big Data applications. On the one hand, Big Data strengthens data storage, data mashup, and methodology of risk assessment; on the other hand, the presence of risks and challenges ",145,78,0.825876659307724,18,0.817527231242922
188,"Accomodation and food service activities","Lab in data science",0.803188741207123,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","A review on use of big data in warehousing to enhance accessibility of food and outreach across the agricultural value- chain is ensured by Big- Data methods and practices.
This information is spread across input providers and produce buyers. This paper will analyze
the lacunas in data accessibility which render the efficacy of adequate supply of food  
",145,78,0.825876659307724,18,0.817527231242922
192,"Accomodation and food service activities","Lab in data science",0.806086719036102,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry make decisions that will increase yields and deliver safe, nutritious food to communities  Predictive
models developed using Big Data identify best management practices for achieving the  machinelearning algorithms and rooted in comprehensive and reliable data- sets, provide  
",145,78,0.825876659307724,18,0.817527231242922
195,"Accomodation and food service activities","Lab in data science",0.87466698884964,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Food trend based on social media for big data analysis using K-mean clustering and SAW: A case study on yogyakarta culinary industry 1. Fig. 1. Big Data Processing Pipeline  From the data that has been done cleaning the data, then
convert in a bag of words matrix to be applied machine learning algorithm. Then perform feature
extraction to find relevant features in classifying trendy food data  
",145,78,0.825876659307724,18,0.817527231242922
202,"Accomodation and food service activities","Lab in data science",0.826745092868805,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[CITATION][C] Big data applications in food safety and quality in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",145,78,0.825876659307724,18,0.817527231242922
205,"Accomodation and food service activities","Lab in data science",0.820608496665955,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[CITATION][C]  -land Algorithm Based on Neighborhood and Temporal Anomalies (FANTA) to Map Planted Versus Fallowed Croplands Using MODIS Data to Assist in [CITATION][C] Identify how big data enables forecasting and demand planning in food and beverages industry",145,78,0.825876659307724,18,0.817527231242922
218,"Accomodation and food service activities","Lab in data science",0.790576696395874,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[CITATION][C] Application of a business intelligence tool within the context of big data in a food industry companySmart ManufacturingPotential of New Digital Technologies and Big Data in the Food Industry",145,78,0.825876659307724,18,0.817527231242922
228,"Human health and social work activities","Applied data analysis",0.787346243858337,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Where is technology taking the economy the Internet, the cloud, big data, robotics, machine learning, and now artificial intelligence
together powerful  And data can't easily be owned either, it can be garnered from nonproprietary 
will still have jobs, especially those like kindergarten teaching or social work that require  
",195,120,0.829422693451246,17,0.837382106220021
233,"Human health and social work activities","Applied data analysis",0.828753352165222,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Identifying child abuse through text mining and machine learning is related to work in the area of data exploration and supervised classification based  risk modeling
(PRM) tools coupled with data mining and machine-learning algorithms should  a linear prediction
model (45.2% sensitivity, 82.4% specificity) using administrative data from 716  
",195,120,0.829422693451246,17,0.837382106220021
245,"Human health and social work activities","Applied data analysis",0.862288653850555,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Predictive and prescriptive analytics, machine learning and child welfare risk assessment: The Broward County experience resonates well with both the American ethos of encouraging strong families and social work ideals
of  making and actuarial methods points to the need for computational and artificial intelligence
models that  are applied to an unseen sample set of data, the testing data set, to  
",195,120,0.829422693451246,17,0.837382106220021
257,"Human health and social work activities","Applied data analysis",0.872301042079926,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A review of existing applications and techniques for narrative text analysis in electronic medical records ARC (Automated Retrieval Console): An algorithm based on an artificial intelligence program,
which  That result suggested that the major variety and contexts for the PHI in the social work notes
is more difficult to model  Data quality is an important barrier to NLP and text mining  
",195,120,0.829422693451246,17,0.837382106220021
276,"Human health and social work activities","Applied data analysis",0.828541219234467,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Singularity academics across the globe, gives a precise definition of artificial intelligence, its constituency 
large degree of integration and cross-fertilization among AI, machine learning, statistics, control 
of shared theoretical frameworks, combined with the availability of data and processing  
",195,120,0.829422693451246,17,0.837382106220021
292,"Human health and social work activities","Applied data analysis",0.837111949920654,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Web analytics enhancing Project Planning: the case of Digital Marketing campaigns: Qualitative study of structured Web analytics data in Project Management enable the process of trace and read virtual traffic, by learning how the user interacts  time and
date at which it occurred, and the characteristics of the machine from which  research on project
planning with Web mining, especially looking at Web analytics data: this combination  
",195,120,0.829422693451246,17,0.837382106220021
321,"Human health and social work activities","Applied data analysis",0.88329941034317,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Hands on the wheel: Navigating algorithmic management and Uber drivers' autonomy More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,120,0.829422693451246,17,0.837382106220021
328,"Human health and social work activities","Applied data analysis",0.845154106616974,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Sentiment analysis for hate speech detection on social media: TF-IDF weighted N-Grams based approach As such, preprocessing unstructured data is a very important role in the text classification  The
number of features can therefore be quite big for a corpus that is average sized  problems and
poses a significant problem to many machine learning algorithms (Yang & Pedersen  
",195,120,0.829422693451246,17,0.837382106220021
330,"Human health and social work activities","Applied data analysis",0.820464789867401,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Analysing large volumes of complex qualitative data-Reflections from a group of international experts Georgia Philip is a Research Fellow in the School of Social Work, at the University of East Anglia 
analysis?' In so doing, he considers the advantages and challenges of using Machine Learning
to assist with coding and help researchers handle large volumes of data in a  
",195,120,0.829422693451246,17,0.837382106220021
341,"Human health and social work activities","Applied data analysis",0.885516941547394,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Hands on the wheel: Navigating algorithmic management and Uber drivers' More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,120,0.829422693451246,17,0.837382106220021
352,"Human health and social work activities","Applied data analysis",0.8296879529953,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Countering Expansion and Organization of Terrorism in Cyberspace making processes. I collected empirical data on the situational factors and the thought and the
decision-making processes of experts by performing secondary data analysis and  DefinitionsArtificial intelligence (AI): The science and engineering of creating intelligent  
",195,120,0.829422693451246,17,0.837382106220021
357,"Human health and social work activities","Applied data analysis",0.802848815917969,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Evaluation of support and training sign language services at Setotolwane Secondary School DATA PRESENTATION, ANALYSIS AND INTERPRTATION ..... 27  Starner, Weaver and
Pentland (1998) present two real-time machine systems for  learners with hearing impairments
in using an educational game for learning the Sign Language  
",195,120,0.829422693451246,17,0.837382106220021
366,"Human health and social work activities","Applied data analysis",0.796828746795654,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] STATE OF THE NORTH There are five big challenges which this generation will need to meet as they gradually 
Digitalisation, artificial intelligence, machine learning and advanced robotics are of particular
focus for economists, and are starting to  Source: HMRC, 'Summary data tables' (HMRC 2017)  
",195,120,0.829422693451246,17,0.837382106220021
371,"Human health and social work activities","Applied data analysis",0.814522087574005,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","The Future of MarketingAn Investigation into Disruption and Innovation Hoanca, 2015, p. 45) for market research, which will itself become automated. Privacy may
fall by the wayside as marketing applications use deep learning to  expected innovations around
analytics, Big Data, machine learning, and artificial intelligence  
",195,120,0.829422693451246,17,0.837382106220021
375,"Human health and social work activities","Applied data analysis",0.853038609027863,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Value co-creation and potential benefits through big data analytics: Health benefit analysis practice where the newer healthcare delivery models depend on user-friendly, real-time big data
analytics, artificial intelligence (AI) and machine learning (ML) tools, and that millions  regarding
the Finnish health data environment. However, they do not provide any  
",195,120,0.829422693451246,17,0.837382106220021
380,"Human health and social work activities","Applied data analysis",0.856261551380157,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[BOOK][B] Toward information justice: Technology, politics, and policy for data in higher education administration 31 2.2 Big Data in Higher Education  scientists and users to accept current data practices and
outcomes as natural or inevitable, and to make data use the  Even in manual technolo- gies, the
technique reduces the human to machine, carrying out tasks as if human practitioners  
",195,120,0.829422693451246,17,0.837382106220021
387,"Human health and social work activities","Applied data analysis",0.831530332565308,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Coding Over the Cracks: Predictive Analytics and Child Protection families and families of color.3 In this age of automation and artificial intelligence, a tempting  are
building and deploying tools that pull together vast quantities of data stored by  analytics and
explains the fundamentally human process of developing a machine learning algorithm  
",195,120,0.829422693451246,17,0.837382106220021
420,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.871715247631073,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","A review on the practice of big data analysis in agriculture to the other research areas employing big data analysis, agriculture ranks at  stations, humans
as sensors, web-based data, GIS geospatial data, feeds from  web services, mobile applications,
statistical analysis, modeling, simulation, benchmarking, big data storage, message  
",180,67,0.828709795403836,17,0.84216985281776
441,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.86358767747879,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","IoT, big data science & analytics, cloud computing and mobile app based hybrid system for smart agriculture Aadhar linked agricultural information network can be easily conceptualized using mobile
communication and big data analytics operating on geo-spatial data already available with Ministry
of agriculture, ISRO, Survey of India to optimize resource availability, spread and  
",180,67,0.828709795403836,17,0.84216985281776
446,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.82792729139328,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Estimating the environmental impact of agriculture by means of geospatial and Big Data analysis: The case of Catalonia develop online tools that allow policymakers to perceive, visualize and analyze the impact ofagriculture, facilitating decision making towards mitigating or eliminating negative effects on the
environment. Big data analysis is crucial for analyzing vast amounts of data (eg weather  
",180,67,0.828709795403836,17,0.84216985281776
452,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.843052208423615,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] Effective use of Big Data Analytics in Crop planning to increase Agriculture Production in India use of multi-sensor data such as satellites, IoT, and drones, and artificial intelligence algorithms
to  that collect data from internet and store it to an open Agriculture database with  The data clusteringalgorithm will run inside of Hadoop platform that offers parallel processing and  
",180,67,0.828709795403836,17,0.84216985281776
466,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.800446331501007,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] Machine learning and data mining advance predictive big data analysis in precision animal agriculturePrecision animal agriculture is poised to rise to prominence in the livestock enterprise in the domains of management, production, welfare, sustainability, health surveillance, and environmental footprint. Considerable progress has been made in the use of tools to ",180,67,0.828709795403836,17,0.84216985281776
470,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.846091568470001,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big Data and Climate Smart Agriculture-Review of Current Status and Implications for Agricultural Research and Innovation in India 23,24,25 . This difference from essentially data driven business analytics is most critical for
applications of big data analytics in scientific knowledge discovery domains such as agriculture.
Perhaps no other area is so alluring for big data-based innovations than  
",180,67,0.828709795403836,17,0.84216985281776
485,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.859070420265198,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM: Data to decisionsBig data are frequently used in many facets of business and agronomy to enhance knowledge needed to improve operational decisions. Livestock operations collect data of sufficient quantity to perform predictive analytics. Predictive analytics can be defined as a ",180,67,0.828709795403836,17,0.84216985281776
490,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.807149887084961,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Multi-sensor Data Fusion Algorithm of Wisdom Agriculture Based on Fusion SetIn wisdom agriculture, the advanced high-tech equipment is applied and human input is reduced to lower the operation and management costs and enhance agricultural management efficiency. In this thesis, a multi-sensor data fusion algorithm based on fusion ",180,67,0.828709795403836,17,0.84216985281776
497,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.818511605262756,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM intelligence dedicated to the study of algorithms for prediction and inference. Learning  about
the data-generating mechanism in practical scenarios. Precision animal agriculture allows
farmers to formulate prompt management practices, and a predictive  
",180,67,0.828709795403836,17,0.84216985281776
513,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.826250195503235,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] Application of Data Warehouse and Big Data Technology in Agriculture in IndiaIn the recent years, it is observed that the scientist, planners, executives across the globe are using data collected from traditional record keeping by government agencies, data collected using sensors and satellite imagery technologies and combining it with predictive weather ",180,67,0.828709795403836,17,0.84216985281776
522,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.872619330883026,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] Agriculture Big Data (AgBD) Challenges and Opportunities From Farm To Table: A Midwest Big Data Hub Community Whitepaper of learning samples) underlying common machine learning and big data analytics methods  need
to develop computationally scalable methods to analyze spatiotemporal datasets in agriculture 
to support researchers to develop scalable spatiotemporal data analytics methods  
",180,67,0.828709795403836,17,0.84216985281776
528,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.832061409950256,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] A review on big data analytics in the field of agriculture Processing OLTP Analytical Big data Processing  We plan to work on precisionagriculture techniques. DISTRIBUTED NOSQL DATABASE FILE SYSTEM
PROGRAMMING COLUMN-DATA MODEL DOCUMENT-DATA MODEL  
",180,67,0.828709795403836,17,0.84216985281776
540,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.872045278549194,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big data analytics in agriculture and distribution channel with map reduce using ct image analysis provide best result [3] Precision agriculture presents
great  My proposed solution for this all type of problem use big data for distributed computing  as
use bossiness analytical application like pentaho BI that give 3d data visualization with  
",180,67,0.828709795403836,17,0.84216985281776
554,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.826891303062439,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","What is cyber-physical system driven agriculture?-Redesign of big data for outstanding farmer management Theoretically, precision agriculture tools coupled with innovative data- mining procedures and
predictive models based on artificial intelligence, will be able to deliver personalized
recommendations at an appropriate spatial scale, so that agricultural productivity  
",180,67,0.828709795403836,17,0.84216985281776
555,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.84680038690567,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Productivity improvement in agriculture sector using big data tools Data of various benefits and polices provided by government from ministry of agriculture  second
part we implement prediction function for establish forecast data through kYmeans  2017
International Conference On Big Data Analytics and computational Intelligence (ICBDACI)  
",180,67,0.828709795403836,17,0.84216985281776
588,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.841052949428558,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements."," analyzes satellite data, market data, and weather data us- ing machine learning and big data
analytics to  is a relatively new company and evaluations of its program are forthcoming
(e-Agriculture, 2017  From Data to Decision All types of big data must go through a series of steps  
[PDF][PDF] How'Big Data'affects competition law analysis in Online Platforms and Agriculture: does one size fit all?",180,67,0.828709795403836,17,0.84216985281776
596,"Agriculture, forestry and fishing","Data Analystics for Smart Grids",0.861614406108856,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements."," for Big Data scholarship in the realm of food and agriculture  Data inconsistency: Data is misplaced
during capturing and filing hence the information is prone to errors  Big data analytics framework
for agricultural services system is a solution that enables farmers  
[CITATION][C] Big Data: Managing the Future's Agriculture and Natural Resource Systems",180,67,0.828709795403836,17,0.84216985281776
239,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.845491826534271,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[HTML][HTML] Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives The deep learning based approach we evaluate in this paper does not require any hand 
MIMIC-III contains de-identified clinical data of over 53,000 hospital admissions for adult patients 
approaches then use relevant concepts in a note as input to machine learning algorithms to  
",195,33,0.829086807641116,16,0.832922168076038
242,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.875458955764771,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Comparing rule-based and deep learning models for patient phenotyping modern methods use relevant concepts in a note as input to a machine learning algorithm to  As
we mentioned before, the goal with our data is to understand phenotypes that are  validation of
this approach in other types of clinical notes such as social work assessment to  
",195,33,0.829086807641116,16,0.832922168076038
246,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.853491127490997,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Predictive and prescriptive analytics, machine learning and child welfare risk assessment: The Broward County experience resonates well with both the American ethos of encouraging strong families and social work ideals
of  making and actuarial methods points to the need for computational and artificial intelligence
models that  are applied to an unseen sample set of data, the testing data set, to  
",195,33,0.829086807641116,16,0.832922168076038
250,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.852550387382507,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Can Predictive Algorithms Assist Decision-Making in Social Work with Children and Families? (1985) explored the potential of what they refer to as 'artificial intelligence' to develop 'expert  every
time' Decision Support Systems and Child and Family Social Work  In another instance where
a big data approach has been developed but has yet to be applied, Schwartz et al  
",195,33,0.829086807641116,16,0.832922168076038
262,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.848671078681946,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[PDF][PDF] Digital economics Strong artificial intelligence  Deep learning A machine learning technique that learns features
and tasks directly from data using an architecture of layers of neural networks. Big data Refers
to voluminous amounts of structured or unstructured data  
",195,33,0.829086807641116,16,0.832922168076038
268,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.830758631229401,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[BOOK][B] Evidence-Based Decision-Making: How to Leverage Available Data and Avoid Cognitive Biases often felt like the proverbial salmon swimming upstream, especially when my data analytic findings 
a typically large set of possibilities) nature, chess naturally lends itself to machine learning in
the  systems roughly parallels the history of what is known as 'artificial intelligence' (AI  
",195,33,0.829086807641116,16,0.832922168076038
277,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.826305747032166,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Singularity academics across the globe, gives a precise definition of artificial intelligence, its constituency 
large degree of integration and cross-fertilization among AI, machine learning, statistics, control 
of shared theoretical frameworks, combined with the availability of data and processing  
",195,33,0.829086807641116,16,0.832922168076038
285,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.834800124168396,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[HTML][HTML] Automated screening for Fragile X premutation carriers based on linguistic and cognitive computational phenotypes Building on prior approaches that analyze data among multiple genotypes and
multiple phenotypes 22 , we used statistical and machine-learning methods to develop
a feature selection module and a data-driven classifier (Fig  
",195,33,0.829086807641116,16,0.832922168076038
306,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.805994749069214,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","The promise and the challenge of technology-facilitated methods for assessing behavioral and cognitive markers of risk for suicide among US Army National Guard  Salt Lake City, UT 84108, USA 4 Department of Social Work, University of  BSP also involves
additional data processing steps prior to generating behavioral markers  Recent developments
have extended these efforts by incorporating artificial intelligence techniques resulting in  
",195,33,0.829086807641116,16,0.832922168076038
311,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.798963725566864,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","A Cognitive Perspective For example, when thinking about artificial intelligence (AI) in healthcare, Christopher Khoury,
vice president of  was driven by the question of What if you could use data science to  around those
tendencies? Using more than 700 variables and machine learning, the company  
",195,33,0.829086807641116,16,0.832922168076038
318,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.823810398578644,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Methods and systems for growing and retaining the value of brand drugs by computer predictive model methods specially adapted for specific business sectors, eg utilities or tourism; G06Q50/10
Services; G06Q50/22Social work  [0042]. Learning Machinerefers to a  or semi-automated
process of generating a prediction based on a model, typically combining software and data  
",195,33,0.829086807641116,16,0.832922168076038
327,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.84705251455307,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Sentiment analysis for hate speech detection on social media: TF-IDF weighted N-Grams based approach As such, preprocessing unstructured data is a very important role in the text classification  The
number of features can therefore be quite big for a corpus that is average sized  problems and
poses a significant problem to many machine learning algorithms (Yang & Pedersen  
",195,33,0.829086807641116,16,0.832922168076038
379,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.825629532337189,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Value co-creation and potential benefits through big data analytics: Health benefit analysis practice where the newer healthcare delivery models depend on user-friendly, real-time big data
analytics, artificial intelligence (AI) and machine learning (ML) tools, and that millions  regarding
the Finnish health data environment. However, they do not provide any  
",195,33,0.829086807641116,16,0.832922168076038
386,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.832635164260864,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[PDF][PDF] Coding Over the Cracks: Predictive Analytics and Child Protection families and families of color.3 In this age of automation and artificial intelligence, a tempting  are
building and deploying tools that pull together vast quantities of data stored by  analytics and
explains the fundamentally human process of developing a machine learning algorithm  
",195,33,0.829086807641116,16,0.832922168076038
405,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.840078175067902,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Re-examining and re-conceptualising Enterprise Search and Discovery capability: Towards a model for the factors and generative mechanisms for search task  225 5.6.2.3 Suboptimal Learning/Sharing culture  These include, the Statistical Machine (Goldberg
1927), Mundaneum (Otlet 1934), World Brain(Wells 1937), Universal  Organizations seek to exploit
'big data' volumes for differentiating insights supporting wealth creation  
",195,33,0.829086807641116,16,0.832922168076038
413,"Human health and social work activities","DD2437 Artificial Neural Networks and Deep Architectures",0.785062551498413,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Suicide attempts of community adolescents and young adults: an explanatory and predictive epidemiological approach My first association with (co-)authorship is intensive learning (again). That's not the worst first
association  the focus away from risk factors towards risk algorithms, ie Machine Learning (ML).
ML is deemed to be well suited to investigate complex associations in data  
",195,33,0.829086807641116,16,0.832922168076038
636,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.851701319217682,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big data analytics for discovering electricity consumption patterns in smart citiesNew technologies such as sensor networks have been incorporated into the management of buildings for organizations and cities. Sensor networks have led to an exponential increase in the volume of data available in recent years, which can be used to extract consumption ",105,67,0.828709795403836,16,0.839550066739321
645,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.820557236671448,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Forecasting Residential Electricity Demand Through Machine Learning and Model SynthesisThis paper aims to develop a predictive model of residential electricity demand using techniques from statistical science, data analysis and econometrics. Residential energy intensity is investigated as a critical component of demand and evaluated as a predictor of ",105,67,0.828709795403836,16,0.839550066739321
650,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.845141232013702,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Clustering Electricity Big Data for Consumption Modeling Using Comparative Strainer Method for High Accuracy Attainment and Dimensionality ReductionIn smart grid, the relation between grid and customer is bidirectional. Therefore, analyzing load consumption patterns is essential for optimal and efficient operation and planning of smart grid in addition to precise load forecasting. However, emergence of the advanced ",105,67,0.828709795403836,16,0.839550066739321
660,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.828346848487854,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Winning the Big Data Technologies Horizon Prize: Fast and reliable forecasting of electricity grid traffic by identification of recurrent fluctuationsThis paper provides a description of the approach and methodology I used in winning the European Union Big Data Technologies Horizon Prize on data-driven prediction of electricity grid traffic. The methodology relies on identifying typical short-term recurrent fluctuations ",105,67,0.828709795403836,16,0.839550066739321
666,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.845302104949951,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Realization and Research of Intelligent system of client electricity information based on the Big Data Processing TechnologyBig data is the focus in power system currently. In order to analyze and classify the electricity model of clients, identify the avoiding peak space intelligently, extract value-added information of clients and control the electric load actively, it is extremely necessary to ",105,67,0.828709795403836,16,0.839550066739321
671,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.829763948917389,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Revealing Household Characteristics from Electricity Meter Data with Grade Analysis and Machine Learning AlgorithmsIn this article, the Grade Correspondence Analysis (GCA) with posterior clustering and visualization is introduced and applied to extract important features to reveal households' characteristics based on electricity usage data. The main goal of the analysis is to ",105,67,0.828709795403836,16,0.839550066739321
679,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.841191530227661,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Machine learning based electricity demand forecastingIn this empirical study we develop forecasting models for electricity demand using publicly available data and three models based on machine learning algorithms. It compares accuracy of these models using different evaluation metrics. The data consist of several ",105,67,0.828709795403836,16,0.839550066739321
682,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.805139541625977,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Sliding Time Window Electricity Consumption Optimization Algorithm for Communities in the Context of Big Data ProcessingBig data frameworks enable companies from various fields to build models that allow them to increase profit margins by improving decision making at different levels (middle management, senior management, and board) or by attempting to boost sales by ",105,67,0.828709795403836,16,0.839550066739321
685,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.901739180088043,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Design of big data analytics electricity collecting data analysis and intelligent monitoring systemsIn order to strengthen the power of users better characteristics, measurement device and distribution network equipment condition monitoring and analysis, based on electricity acquisition data analysis and intelligent monitoring system based on the integration of ",105,67,0.828709795403836,16,0.839550066739321
691,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.868881225585938,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","The Design of Distributed Power Big Data Analysis Framework and Its Application in Residential Electricity AnalysisWith the development of digital, information and intelligent process of power system, more and more data sources appear. The traditional standalone environment has been difficult to adapt to the need of the analysis of massive data. The power industry also needs to use real ",105,67,0.828709795403836,16,0.839550066739321
697,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.82227087020874,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Visualization as a Mean of Big Data Management: Using Qatar's Electricity Consumption DataVisualization as a mean of big data management is the new century revolution. Managing data has become a great challenge today, as the amount of raw data size is increasing rapidly. For data like electricity consumption, a new data value is received every minute from ",105,67,0.828709795403836,16,0.839550066739321
700,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.849972426891327,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big Data Analytics for Electricity Price ForecastElectricity Price forecast is a major task in smart grid operation. There is a massive amount of data flowing in the power system including the data collection by control systems, sensors, etc. In addition, there are many data points which are not captured and processed by the ",105,67,0.828709795403836,16,0.839550066739321
707,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.816360890865326,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] Artificial Intelligence and Nord Pool's intraday electricity market Elbas: a demonstration and pragmatic evaluation of employing deep learning for price prediction This thesis demonstrates the use of deep learning for automating hourly price forecasts in continuous intraday electricity markets, using various types of neural networks on comprehensive sequential market data and cutting-edge image processing networks on ",105,67,0.828709795403836,16,0.839550066739321
720,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.854667782783508,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","A Big Data Analytics Model for Household Electricity Consumption Tracking and MonitoringThe abundance of data nowadays can offer infinite opportunities and possibilities if being systematically explored. Exploration of the data can be achieved through the application of big data analytics (BDA). Consequently, a number of BDA models are seen developed in a ",105,67,0.828709795403836,16,0.839550066739321
725,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.854976713657379,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","IArchitecture based on multivariate big data platform for analyzing electricity consumption behavior,""With the development of smart grid, more and more measuring devices extend to bottom layer. The development of advanced measurement system and distribution network inevitably leads to the geometric increase of user data. On the other hand, the power grid is ",105,67,0.828709795403836,16,0.839550066739321
734,"Electricity, gas, steam and air conditioning supply","Data Analystics for Smart Grids",0.796788215637207,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements."," In many developed countries, such as America, England, Japan and so on those used the
technology of Artificial Intelligence (AI), such as Neural Network, Data Mining, Machine Learning
and so on to apply to use in electricity energy forecasting for gain the best performance  
Study of electricity load forecasting based on multiple kernels learning and weighted support vector regression machine",105,67,0.828709795403836,16,0.839550066739321
769,"Financial service activities, except insurance and pension funding","Machine Learning",0.818016588687897,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","QuantCloud: big data infrastructure for quantitative finance on the cloudIn this paper, we present the QuantCloud infrastructure, designed for performing big data analytics in modern quantitative finance. Through analyzing market observations, quantitative finance (QF) utilizes mathematical models to search for subtle patterns and ",135,79,0.825409763975988,15,0.828442883491516
771,"Financial service activities, except insurance and pension funding","Machine Learning",0.838835179805756,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[PDF][PDF] Machine learning in finance: the case of deep learning for option pricingModern advancements in mathematical analysis, computational hardware and software, and availability of big data have made possible commoditized machines that can learn to operate as investment managers, financial analysts, and traders. We briefly survey how and ",135,79,0.825409763975988,15,0.828442883491516
786,"Financial service activities, except insurance and pension funding","Machine Learning",0.835663378238678,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Quantcloud: Enabling big data complex event processing for quantitative finance through a data-driven executionQuantitative Finance (QF) utilizes increasingly sophisticated mathematic models and advanced computer techniques to predict the movement of global markets and price the derivatives. Today, the rise of QF requires an integrated toolchain of enabling technologies ",135,79,0.825409763975988,15,0.828442883491516
802,"Financial service activities, except insurance and pension funding","Machine Learning",0.790295660495758,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Compressed Sensing and its Applications in Risk Assessment for Internet Supply Chain Finance under Big DataPlenty of research focuses on supply chain finance and its risk, qualitatively or quantitatively. However, only a little literature studies on internet supply chain finance (ISCF), especially on its risk by quantitative analysis. After analyzing the information of partners' panorama data ",135,79,0.825409763975988,15,0.828442883491516
807,"Financial service activities, except insurance and pension funding","Machine Learning",0.871679365634918,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Introduction to the Minitrack on Machine Learning and Network Analytics in FinanceRecent years have seen a rapid evolution of methodologies in artificial intelligence and machine learning, and as a result, increasingly widespread use of these techniques in different domains. One of the most important application areas is finance, offering ",135,79,0.825409763975988,15,0.828442883491516
808,"Financial service activities, except insurance and pension funding","Machine Learning",0.870182991027832,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Introduction to the Minitrack on Machine Learning and Network Analytics in FinanceRecent years have seen a rapid evolution of methodologies in artificial intelligence and machine learning, and as a result, increasingly widespread use of these techniques in different domains. One of the most important application areas is finance, offering ",135,79,0.825409763975988,15,0.828442883491516
810,"Financial service activities, except insurance and pension funding","Machine Learning",0.837307035923004,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Introduction to Machine Learning and Network Analytics in Finance MinitrackWe are experiencing enormous growth in the interest of application of various computational methods in finance, which is the consequence of various developments in the last 15 years. As a result, the number and importance of contributions utilizing various machine learning  ",135,79,0.825409763975988,15,0.828442883491516
811,"Financial service activities, except insurance and pension funding","Machine Learning",0.82215279340744,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Introduction to Machine Learning and Network Analytics in Finance MinitrackWe are experiencing enormous growth in the interest of application of various computational methods in finance, which is the consequence of various developments in the last 15 years. As a result, the number and importance of contributions utilizing various machine learning  ",135,79,0.825409763975988,15,0.828442883491516
830,"Financial service activities, except insurance and pension funding","Machine Learning",0.819386005401611,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Appropriate machine learning techniques for credit scoring and bankruptcy prediction in banking and finance: a comparative studyAbstract Machine learning techniques have been used successfully in several areas such as banking and finance. These techniques are used mainly for prediction, classification and partitioning data into different groups according to a certain common characteristic. In this ",135,79,0.825409763975988,15,0.828442883491516
831,"Financial service activities, except insurance and pension funding","Machine Learning",0.813425123691559,"To acquire basic knowledge about machine learning in general, and about several machine learning techniques. To acquire the capacity to use those techniques in applications and to choose the techniques that are more adequate for each situation.Concept of machine learning. Supervised and unsupervised learning. Historical perspective. Multilayer perceptrons. Statistical aspects of supervised learning. The problem of generalization. Support vector machines Decision trees. Clustering and vector quantization. Estimation of probability densities. Principal components analysis.","Appropriate machine learning techniques for credit scoring and bankruptcy prediction in banking and finance: a comparative studyAbstract Machine learning techniques have been used successfully in several areas such as banking and finance. These techniques are used mainly for prediction, classification and partitioning data into different groups according to a certain common characteristic. In this ",135,79,0.825409763975988,15,0.828442883491516
834,"Financial service activities, except insurance and pension funding","Machine Learning",0.804355084896088,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Appropriate machine learning techniques for credit scoring and bankruptcy prediction in banking and finance: a comparative studyAbstract Machine learning techniques have been used successfully in several areas such as banking and finance. These techniques are used mainly for prediction, classification and partitioning data into different groups according to a certain common characteristic. In this ",135,79,0.825409763975988,15,0.828442883491516
847,"Financial service activities, except insurance and pension funding","Machine Learning",0.816167414188385,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Machine Learning for Structured FinanceMachine learning and artificial intelligence have evolved beyond simple hype and have integrated themselves in business and in popular conversation as an increasing number of smart applications profoundly transform the way we work and live. This article defines ",135,79,0.825409763975988,15,0.828442883491516
854,"Financial service activities, except insurance and pension funding","Machine Learning",0.829505980014801,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Essays on machine learning for economics and financeEconometrics and machine learning are quite close and related concepts. Nowadays, it is always more important to extract value from raw data, and distilling actionable insights from quantitative values as well as qualitative features. In order to deal with these topics, the first ",135,79,0.825409763975988,15,0.828442883491516
864,"Financial service activities, except insurance and pension funding","Machine Learning",0.793811142444611,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[PDF][PDF] Applications of Big Data methods in Finance: Index TrackingRESEARCH OBJECTIVES Although the curse of dimensionality does not relate to most financial settings, high-dimensional methods gained some relevance in the recent finance literature. Index tracking aims at finding an optimal sample of stocks able to mimic the ",135,79,0.825409763975988,15,0.828442883491516
870,"Financial service activities, except insurance and pension funding","Machine Learning",0.865859508514404,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Machine learning with applications to financeThe impact of data driven, machine learning technologies across a wide variety of fields is undeniable. The financial industry, which relies heavily on predictive modeling being no exception. In this work we summarize two widely used machine learning models: support ",135,79,0.825409763975988,15,0.828442883491516
434,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.783209502696991,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[HTML][HTML] Analysis of agriculture data using data mining techniques: application of big dataIn agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estimation for the ",180,53,0.824819772873285,14,0.817902360643659
447,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.82647430896759,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Estimating the environmental impact of agriculture by means of geospatial and Big Data analysis: The case of Catalonia develop online tools that allow policymakers to perceive, visualize and analyze the impact ofagriculture, facilitating decision making towards mitigating or eliminating negative effects on the
environment. Big data analysis is crucial for analyzing vast amounts of data (eg weather  
",180,53,0.824819772873285,14,0.817902360643659
463,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.79716032743454,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","From the Dust Bowl to Drones to Big Data: The Next Revolution in Agriculture Precision Agriculture and Big Data While precision agriculture (PA) and big data are related,
they are not  across years) variability asso- ciated with all aspects of agricultural pro- duction (Figure
1). Big data refers to the collection, analysis, and synthesis of large data sets that  
",180,53,0.824819772873285,14,0.817902360643659
474,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.835600018501282,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Big Data and Climate Smart Agriculture-Review of Current Status and Implications for Agricultural Research and Innovation in India 23,24,25 . This difference from essentially data driven business analytics is most critical for
applications of big data analytics in scientific knowledge discovery domains such as agriculture.
Perhaps no other area is so alluring for big data-based innovations than  
",180,53,0.824819772873285,14,0.817902360643659
482,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.80035674571991,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] Is big data driving a paradigm shift in precision agriculture Much of the discussion focuses on whether the arrival of big data signals the emergence of or
need for a new  Adoption, profitability, and making better use of precision farming data  The research
status on precision agriculture by use of bibliometric analysis from three databases  
",180,53,0.824819772873285,14,0.817902360643659
487,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.827115476131439,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM: Data to decisionsBig data are frequently used in many facets of business and agronomy to enhance knowledge needed to improve operational decisions. Livestock operations collect data of sufficient quantity to perform predictive analytics. Predictive analytics can be defined as a ",180,53,0.824819772873285,14,0.817902360643659
499,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.815376281738281,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM intelligence dedicated to the study of algorithms for prediction and inference. Learning  about
the data-generating mechanism in practical scenarios. Precision animal agriculture allows
farmers to formulate prompt management practices, and a predictive  
",180,53,0.824819772873285,14,0.817902360643659
503,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.780993461608887,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Application of big data in precision agriculture.Precision agriculture is the future way for agricultural modernization. The rapid expansion of
agricultural data and the development of big data technology provide a new method for the
development of precision agriculture, and become an important force leading to the development  
",180,53,0.824819772873285,14,0.817902360643659
514,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.821405291557312,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] Application of Data Warehouse and Big Data Technology in Agriculture in IndiaIn the recent years, it is observed that the scientist, planners, executives across the globe are using data collected from traditional record keeping by government agencies, data collected using sensors and satellite imagery technologies and combining it with predictive weather ",180,53,0.824819772873285,14,0.817902360643659
516,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.817299425601959,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Research and Application of Spark Platform on Big Data Processing in Intelligent Agriculture of Jilin Province (2) The agricultural big data processing method of intelligent agriculture in Jilin  algorithm
characterized by dynamic and rapid expansion, combining the Spark streaming flow calculation
framework, able to real-time analyze continuous and rapid changes in the massive data  
",180,53,0.824819772873285,14,0.817902360643659
524,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.864035665988922,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] Agriculture Big Data (AgBD) Challenges and Opportunities From Farm To Table: A Midwest Big Data Hub Community Whitepaper of learning samples) underlying common machine learning and big data analytics methods  need
to develop computationally scalable methods to analyze spatiotemporal datasets in agriculture 
to support researchers to develop scalable spatiotemporal data analytics methods  
",180,53,0.824819772873285,14,0.817902360643659
535,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.84760993719101,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Artificial Intelligence on Remote Sensing Data for Precision Agriculture ApplicationsPrecision agriculture benefits greatly from information provided by high spatial resolution and high temporal frequency remotely sensed images. It requires effective methodologies and algorithms to exact information from the huge volume, dimension and variety of raw ",180,53,0.824819772873285,14,0.817902360643659
550,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.830113172531128,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","What is cyber-physical system driven agriculture?-Redesign of big data for outstanding farmer management Theoretically, precision agriculture tools coupled with innovative data- mining procedures and
predictive models based on artificial intelligence, will be able to deliver personalized
recommendations at an appropriate spatial scale, so that agricultural productivity  
",180,53,0.824819772873285,14,0.817902360643659
559,"Agriculture, forestry and fishing","A Network Tour of Data Science",0.80388343334198,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Productivity improvement in agriculture sector using big data tools Data of various benefits and polices provided by government from ministry of agriculture  second
part we implement prediction function for establish forecast data through kYmeans  2017
International Conference On Big Data Analytics and computational Intelligence (ICBDACI)  
",180,53,0.824819772873285,14,0.817902360643659
229,"Human health and social work activities","Artificial Intelligence and Data Science",0.781076192855835,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[PDF][PDF] Where is technology taking the economy the Internet, the cloud, big data, robotics, machine learning, and now artificial intelligence
together powerful  And data can't easily be owned either, it can be garnered from nonproprietary 
will still have jobs, especially those like kindergarten teaching or social work that require  
",195,47,0.827168932620515,13,0.82586839565864
264,"Human health and social work activities","Artificial Intelligence and Data Science",0.838693141937256,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[PDF][PDF] Digital economics Strong artificial intelligence  Deep learning A machine learning technique that learns features
and tasks directly from data using an architecture of layers of neural networks. Big data Refers
to voluminous amounts of structured or unstructured data  
",195,47,0.827168932620515,13,0.82586839565864
275,"Human health and social work activities","Artificial Intelligence and Data Science",0.829969763755798,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Singularity academics across the globe, gives a precise definition of artificial intelligence, its constituency 
large degree of integration and cross-fertilization among AI, machine learning, statistics, control 
of shared theoretical frameworks, combined with the availability of data and processing  
",195,47,0.827168932620515,13,0.82586839565864
284,"Human health and social work activities","Artificial Intelligence and Data Science",0.815015912055969,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[BOOK][B] The future of work in Australia: anticipating how new technologies will reshape labour markets, occupations and skill requirements He holds qualifications in political science, social policy, and social work  recent developments
in information and communication technology (ICT), computer-based technologies (CBT) andartificial intelligence (AI) have  Big data has helped to facilitate significant advances in AI  
",195,47,0.827168932620515,13,0.82586839565864
322,"Human health and social work activities","Artificial Intelligence and Data Science",0.879016637802124,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Hands on the wheel: Navigating algorithmic management and Uber drivers' autonomy More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,47,0.827168932620515,13,0.82586839565864
339,"Human health and social work activities","Artificial Intelligence and Data Science",0.78465211391449,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[BOOK][B] Scientific Reasoning and Argumentation: The Roles of Domain-specific and Domain-general Knowledge Library of Congress Cataloging-in-Publication Data Names: Fischer, Frank, 1942- editor  Christian
Ghanem, Theories and Methods of Social Work, Katholische Stiftungshochschule München
(KSH  for the limits of domain-generality based on work in machine learning and natural  
",195,47,0.827168932620515,13,0.82586839565864
342,"Human health and social work activities","Artificial Intelligence and Data Science",0.882572591304779,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[PDF][PDF] Hands on the wheel: Navigating algorithmic management and Uber drivers' More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,47,0.827168932620515,13,0.82586839565864
354,"Human health and social work activities","Artificial Intelligence and Data Science",0.819411158561707,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Countering Expansion and Organization of Terrorism in Cyberspace making processes. I collected empirical data on the situational factors and the thought and the
decision-making processes of experts by performing secondary data analysis and  DefinitionsArtificial intelligence (AI): The science and engineering of creating intelligent  
",195,47,0.827168932620515,13,0.82586839565864
358,"Human health and social work activities","Artificial Intelligence and Data Science",0.802753746509552,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Evaluation of support and training sign language services at Setotolwane Secondary School DATA PRESENTATION, ANALYSIS AND INTERPRTATION ..... 27  Starner, Weaver and
Pentland (1998) present two real-time machine systems for  learners with hearing impairments
in using an educational game for learning the Sign Language  
",195,47,0.827168932620515,13,0.82586839565864
367,"Human health and social work activities","Artificial Intelligence and Data Science",0.791867852210999,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[PDF][PDF] STATE OF THE NORTH There are five big challenges which this generation will need to meet as they gradually 
Digitalisation, artificial intelligence, machine learning and advanced robotics are of particular
focus for economists, and are starting to  Source: HMRC, 'Summary data tables' (HMRC 2017)  
",195,47,0.827168932620515,13,0.82586839565864
370,"Human health and social work activities","Artificial Intelligence and Data Science",0.819617569446564,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","The Future of MarketingAn Investigation into Disruption and Innovation Hoanca, 2015, p. 45) for market research, which will itself become automated. Privacy may
fall by the wayside as marketing applications use deep learning to  expected innovations around
analytics, Big Data, machine learning, and artificial intelligence  
",195,47,0.827168932620515,13,0.82586839565864
377,"Human health and social work activities","Artificial Intelligence and Data Science",0.837653458118439,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Value co-creation and potential benefits through big data analytics: Health benefit analysis practice where the newer healthcare delivery models depend on user-friendly, real-time big data
analytics, artificial intelligence (AI) and machine learning (ML) tools, and that millions  regarding
the Finnish health data environment. However, they do not provide any  
",195,47,0.827168932620515,13,0.82586839565864
381,"Human health and social work activities","Artificial Intelligence and Data Science",0.853989005088806,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[BOOK][B] Toward information justice: Technology, politics, and policy for data in higher education administration 31 2.2 Big Data in Higher Education  scientists and users to accept current data practices and
outcomes as natural or inevitable, and to make data use the  Even in manual technolo- gies, the
technique reduces the human to machine, carrying out tasks as if human practitioners  
",195,47,0.827168932620515,13,0.82586839565864
236,"Human health and social work activities","Machine Perception",0.858272552490234,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","[HTML][HTML] Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives The deep learning based approach we evaluate in this paper does not require any hand 
MIMIC-III contains de-identified clinical data of over 53,000 hospital admissions for adult patients 
approaches then use relevant concepts in a note as input to machine learning algorithms to  
",195,24,0.819486998021603,12,0.818991815050443
240,"Human health and social work activities","Machine Perception",0.879512131214142,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Comparing rule-based and deep learning models for patient phenotyping modern methods use relevant concepts in a note as input to a machine learning algorithm to  As
we mentioned before, the goal with our data is to understand phenotypes that are  validation of
this approach in other types of clinical notes such as social work assessment to  
",195,24,0.819486998021603,12,0.818991815050443
261,"Human health and social work activities","Machine Perception",0.85393875837326,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","[PDF][PDF] Digital economics Strong artificial intelligence  Deep learning A machine learning technique that learns features
and tasks directly from data using an architecture of layers of neural networks. Big data Refers
to voluminous amounts of structured or unstructured data  
",195,24,0.819486998021603,12,0.818991815050443
269,"Human health and social work activities","Machine Perception",0.829798460006714,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","[BOOK][B] Evidence-Based Decision-Making: How to Leverage Available Data and Avoid Cognitive Biases often felt like the proverbial salmon swimming upstream, especially when my data analytic findings 
a typically large set of possibilities) nature, chess naturally lends itself to machine learning in
the  systems roughly parallels the history of what is known as 'artificial intelligence' (AI  
",195,24,0.819486998021603,12,0.818991815050443
303,"Human health and social work activities","Machine Perception",0.807547092437744,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Do citations and readership identify seminal publications? deep learning paper which has caused a shift in the area of artificial intelligence/computer vision 
To do this, we use the threshold which achieves the best accuracy on the training data  The reason
why we chose the this simple model instead of a machine learning model such  
",195,24,0.819486998021603,12,0.818991815050443
308,"Human health and social work activities","Machine Perception",0.800992906093597,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","The promise and the challenge of technology-facilitated methods for assessing behavioral and cognitive markers of risk for suicide among US Army National Guard  Salt Lake City, UT 84108, USA 4 Department of Social Work, University of  BSP also involves
additional data processing steps prior to generating behavioral markers  Recent developments
have extended these efforts by incorporating artificial intelligence techniques resulting in  
",195,24,0.819486998021603,12,0.818991815050443
313,"Human health and social work activities","Machine Perception",0.796758472919464,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","A Cognitive Perspective For example, when thinking about artificial intelligence (AI) in healthcare, Christopher Khoury,
vice president of  was driven by the question of What if you could use data science to  around those
tendencies? Using more than 700 variables and machine learning, the company  
",195,24,0.819486998021603,12,0.818991815050443
359,"Human health and social work activities","Machine Perception",0.802061021327972,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Evaluation of support and training sign language services at Setotolwane Secondary School DATA PRESENTATION, ANALYSIS AND INTERPRTATION ..... 27  Starner, Weaver and
Pentland (1998) present two real-time machine systems for  learners with hearing impairments
in using an educational game for learning the Sign Language  
",195,24,0.819486998021603,12,0.818991815050443
397,"Human health and social work activities","Machine Perception",0.786871194839478,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Efficiency, Correctness, and the Authority of Automation: Technology in College Basic Writing Instruction 101 DATA ANALYSIS  Yet there also are moments of authentic possibility for broader learning
and understanding through the use of the automated system  Virtually all remedial English at
the college level could be handled by automation, with the machine as an impartial judge  
",195,24,0.819486998021603,12,0.818991815050443
403,"Human health and social work activities","Machine Perception",0.791060149669647,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","[BOOK][B] Virtual Reality and the Criminal Justice System: Exploring the Possibilities for Correctional Rehabilitation Page 13. Introduction 5 Figure 0.1. Sensorama machine. Image courtesy of Katalin Heilig. goggles
for the purpose of streaming data or images to the user  Semi- and fully-immersive systems have
been commonly used when the participant is learning or practicing new skills  
",195,24,0.819486998021603,12,0.818991815050443
410,"Human health and social work activities","Machine Perception",0.792028307914734,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Suicide attempts of community adolescents and young adults: an explanatory and predictive epidemiological approach My first association with (co-)authorship is intensive learning (again). That's not the worst first
association  the focus away from risk factors towards risk algorithms, ie Machine Learning (ML).
ML is deemed to be well suited to investigate complex associations in data  
",195,24,0.819486998021603,12,0.818991815050443
416,"Human health and social work activities","Machine Perception",0.829060733318329,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Future Implications of the Psychopathy Construct for Criminology and Criminal Justice Policy and Practice Torture, 6 For example, Rhodes (2002) provides participation observation data showing that 
Sensors and machine learning algorithms have been designed that can measure affective
information  In addition, artificial intelligence advances that can make robots feel so to speak  
",195,24,0.819486998021603,12,0.818991815050443
756,"Financial service activities, except insurance and pension funding","Big Data",0.805391252040863,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","How Do the Global Stock Markets Influence One Another? Evidence from Finance Big Data and Granger Causality Directed NetworkThe recent financial network analysis approach reveals that the topologies of financial markets have an important influence on market dynamics. However, the majority of existing Finance Big Data networks are built as undirected networks without information on the ",135,99,0.823880515315316,12,0.82094644010067
758,"Financial service activities, except insurance and pension funding","Applied data analysis",0.792959272861481,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","How Do the Global Stock Markets Influence One Another? Evidence from Finance Big Data and Granger Causality Directed NetworkThe recent financial network analysis approach reveals that the topologies of financial markets have an important influence on market dynamics. However, the majority of existing Finance Big Data networks are built as undirected networks without information on the ",135,120,0.829422693451246,12,0.824004049102465
761,"Financial service activities, except insurance and pension funding","Applied data analysis",0.792262196540833,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big Data Finance and Financial MarketsFinancial markets are always the most aggressive adopters of new information technologies. The recent boom in big data has enhanced the effect of information diffusion in financial markets since the physical cost of participation has been reduced and interactions among ",135,120,0.829422693451246,12,0.824004049102465
763,"Financial service activities, except insurance and pension funding","Big Data",0.787165582180023,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Big Data Finance and Financial MarketsFinancial markets are always the most aggressive adopters of new information technologies. The recent boom in big data has enhanced the effect of information diffusion in financial markets since the physical cost of participation has been reduced and interactions among ",135,99,0.823880515315316,12,0.82094644010067
768,"Financial service activities, except insurance and pension funding","Applied data analysis",0.821355760097504,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","QuantCloud: big data infrastructure for quantitative finance on the cloudIn this paper, we present the QuantCloud infrastructure, designed for performing big data analytics in modern quantitative finance. Through analyzing market observations, quantitative finance (QF) utilizes mathematical models to search for subtle patterns and ",135,120,0.829422693451246,12,0.824004049102465
770,"Financial service activities, except insurance and pension funding","Applied data analysis",0.842482626438141,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Machine learning in finance: the case of deep learning for option pricingModern advancements in mathematical analysis, computational hardware and software, and availability of big data have made possible commoditized machines that can learn to operate as investment managers, financial analysts, and traders. We briefly survey how and ",135,120,0.829422693451246,12,0.824004049102465
773,"Financial service activities, except insurance and pension funding","Big Data",0.830458521842957,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Machine learning in finance: the case of deep learning for option pricingModern advancements in mathematical analysis, computational hardware and software, and availability of big data have made possible commoditized machines that can learn to operate as investment managers, financial analysts, and traders. We briefly survey how and ",135,99,0.823880515315316,12,0.82094644010067
776,"Financial service activities, except insurance and pension funding","Applied data analysis",0.845908999443054,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big Data Analytics and Visualization: FinanceAll finance institutions have seen an explosion in their velocity, variety and volume of their internal 
datasets. New federal regulations requirement require leveraging internal and external data 
linking: [1] Customer service and transactional level data; [2] Social Media activity analysis (Sentimental ",135,120,0.829422693451246,12,0.824004049102465
780,"Financial service activities, except insurance and pension funding","Applied data analysis",0.828987598419189,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Signal Processing for Finance, Economics, and Marketing: Concepts, framework, and big data applicationsEconomic data and financial markets are intriguing to researchers working on data and quantitative models. With rapid growth of and increasing access to data in digital form, finance, economics, and marketing data are poised to become one of the most important ",135,120,0.829422693451246,12,0.824004049102465
783,"Financial service activities, except insurance and pension funding","Big Data",0.823102474212646,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Signal Processing for Finance, Economics, and Marketing: Concepts, framework, and big data applicationsEconomic data and financial markets are intriguing to researchers working on data and quantitative models. With rapid growth of and increasing access to data in digital form, finance, economics, and marketing data are poised to become one of the most important ",135,99,0.823880515315316,12,0.82094644010067
787,"Financial service activities, except insurance and pension funding","Big Data",0.832807600498199,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Quantcloud: Enabling big data complex event processing for quantitative finance through a data-driven executionQuantitative Finance (QF) utilizes increasingly sophisticated mathematic models and advanced computer techniques to predict the movement of global markets and price the derivatives. Today, the rise of QF requires an integrated toolchain of enabling technologies ",135,99,0.823880515315316,12,0.82094644010067
799,"Financial service activities, except insurance and pension funding","Big Data",0.806129157543182,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Application of information systems aimed at big data use in the sphere of state finance management: Concept schemeThe article is devoted to the application of big data technologies in public administration, in particular, in public financial management. The paper outlines the basic principles of effective public financial management and describes the development trends of state ",135,99,0.823880515315316,12,0.82094644010067
803,"Financial service activities, except insurance and pension funding","Applied data analysis",0.785826504230499,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Compressed Sensing and its Applications in Risk Assessment for Internet Supply Chain Finance under Big DataPlenty of research focuses on supply chain finance and its risk, qualitatively or quantitatively. However, only a little literature studies on internet supply chain finance (ISCF), especially on its risk by quantitative analysis. After analyzing the information of partners' panorama data ",135,120,0.829422693451246,12,0.824004049102465
820,"Financial service activities, except insurance and pension funding","Big Data",0.879531383514404,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Big Data: Epistemological Reflections and Impacts in Finance and Capital Market Studies.Objective and method: Access to data series plays a central role in the area of Finance. The increasing availability of large volumes of data, in different formats and at high frequency, combined with the technological advances in data storage and processing tools, have ",135,99,0.823880515315316,12,0.82094644010067
822,"Financial service activities, except insurance and pension funding","Applied data analysis",0.868486642837524,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big Data: Epistemological Reflections and Impacts in Finance and Capital Market Studies.Objective and method: Access to data series plays a central role in the area of Finance. The increasing availability of large volumes of data, in different formats and at high frequency, combined with the technological advances in data storage and processing tools, have ",135,120,0.829422693451246,12,0.824004049102465
825,"Financial service activities, except insurance and pension funding","Applied data analysis",0.849561989307404,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Research on the Core Competence and Training System of Computer Speciality for Big Data Processing in Colleges and Universities of Finance and EconomicsIn big data era, the computer professionals should be equipped with the ability of big data processing and analysis. And multi-disciplinary collaborative innovation and interdisciplinary cross-learning ability is becoming more and more important. According to ",135,120,0.829422693451246,12,0.824004049102465
829,"Financial service activities, except insurance and pension funding","Big Data",0.825164914131165,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Research on the Core Competence and Training System of Computer Speciality for Big Data Processing in Colleges and Universities of Finance and EconomicsIn big data era, the computer professionals should be equipped with the ability of big data processing and analysis. And multi-disciplinary collaborative innovation and interdisciplinary cross-learning ability is becoming more and more important. According to ",135,99,0.823880515315316,12,0.82094644010067
841,"Financial service activities, except insurance and pension funding","Applied data analysis",0.815088152885437,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Reflection on Big Data Technology: Problems and Countermeasures in"" Big Data Credit Reporting"" of Internet Finance in ChinaWith the rapid development of Internet finance in China for the past few years, big data credit reporting agencies specifically for network credit information have been initially established. An analysis on the technology application, characteristics and operational difficulties of big  ",135,120,0.829422693451246,12,0.824004049102465
844,"Financial service activities, except insurance and pension funding","Big Data",0.79169225692749,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Reflection on Big Data Technology: Problems and Countermeasures in"" Big Data Credit Reporting"" of Internet Finance in ChinaWith the rapid development of Internet finance in China for the past few years, big data credit reporting agencies specifically for network credit information have been initially established. An analysis on the technology application, characteristics and operational difficulties of big  ",135,99,0.823880515315316,12,0.82094644010067
850,"Financial service activities, except insurance and pension funding","Big Data",0.839977860450745,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Essays on machine learning for economics and financeEconometrics and machine learning are quite close and related concepts. Nowadays, it is always more important to extract value from raw data, and distilling actionable insights from quantitative values as well as qualitative features. In order to deal with these topics, the first ",135,99,0.823880515315316,12,0.82094644010067
855,"Financial service activities, except insurance and pension funding","Applied data analysis",0.833711385726929,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Survey of Big Data applications: health, education, business & finance, and security & privacyNowadays, Big Data is experiencing an exponential growth in all domains of life. The total amount of data created in the world from the beginning of time up until 2005 is now created every 48 hours! Big Data represents large datasets that cannot be analyzed using traditional ",135,120,0.829422693451246,12,0.824004049102465
856,"Financial service activities, except insurance and pension funding","Big Data",0.829720079898834,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Survey of Big Data applications: health, education, business & finance, and security & privacyNowadays, Big Data is experiencing an exponential growth in all domains of life. The total amount of data created in the world from the beginning of time up until 2005 is now created every 48 hours! Big Data represents large datasets that cannot be analyzed using traditional ",135,99,0.823880515315316,12,0.82094644010067
860,"Financial service activities, except insurance and pension funding","Applied data analysis",0.811417460441589,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[PDF][PDF] Applications of Big Data methods in Finance: Index TrackingRESEARCH OBJECTIVES Although the curse of dimensionality does not relate to most financial settings, high-dimensional methods gained some relevance in the recent finance literature. Index tracking aims at finding an optimal sample of stocks able to mimic the ",135,120,0.829422693451246,12,0.824004049102465
862,"Financial service activities, except insurance and pension funding","Big Data",0.800216197967529,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Applications of Big Data methods in Finance: Index TrackingRESEARCH OBJECTIVES Although the curse of dimensionality does not relate to most financial settings, high-dimensional methods gained some relevance in the recent finance literature. Index tracking aims at finding an optimal sample of stocks able to mimic the ",135,99,0.823880515315316,12,0.82094644010067
82,"Accomodation and food service activities","Information Retrieval",0.806685209274292,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Publicising food: big data, precision agriculture, and co-experimental techniques of addition interviews, involving 18 Iowa farmers, 14 individuals from big data industry (those involved in
the sale and promotion of large-scale data acquisition, predictive analytic software, and/or
precision agriculture technologies), and 19 interviews of regional food system entrepreneurs  
",145,48,0.813798369218906,11,0.816569349982522
89,"Accomodation and food service activities","Information Retrieval",0.847309947013855,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Big data in food safety: An overview A list of the most used analysis methods for big data is shown in Table 3. These meth  These
systems are developed using data mining techniques (collaborative filtering, content based filtering
and hybrid  To the author's knowledge, these systems are not yet applied in food safety  
",145,48,0.813798369218906,11,0.816569349982522
107,"Accomodation and food service activities","Information Retrieval",0.805386483669281,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Big Data and the transformation of food and beverage marketing: undermining efforts to reduce obesity? of digital marketing, harnessing the power of Big Data analytics, artificial intelligence, and powerful 
rich views of behavioral patterns that can be highly valuable to food and beverage  For example,
leading data company Neustar can identify people who display the following  
",145,48,0.813798369218906,11,0.816569349982522
111,"Accomodation and food service activities","Information Retrieval",0.789357304573059,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Satellite data and machine learning for weather risk management and food securityThe increase in frequency and severity of extreme weather events poses challenges for the agricultural sector in developing economies and for food security globally. In this article, we demonstrate how machine learning can be used to mine satellite data and identify pixel ",145,48,0.813798369218906,11,0.816569349982522
130,"Accomodation and food service activities","Information Retrieval",0.853417694568634,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Research and application of big data-based co-regulation model in food safety governance.This paper is aimed to construct a food safety and nutrition information collection and analysis
platform with new internet technologies including big data to improve the analytic capacity and
the data-mining capability, and thereby to provide more accurate and comprehensive food  
",145,48,0.813798369218906,11,0.816569349982522
148,"Accomodation and food service activities","Information Retrieval",0.78814023733139,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Big Data Analytics for Traceability in Food Supply Chain WAINA 2019: Web, Artificial Intelligence and Network Applications pp 880-884 | Cite as. Big Data
Analytics for Traceability in Food Supply Chain  The amount of socio-economic data generated
every day has grown dramatically in recent years thanks to the widespread use of the  
",145,48,0.813798369218906,11,0.816569349982522
163,"Accomodation and food service activities","Information Retrieval",0.837509572505951,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","[PDF][PDF] Recommender System Based Tensor Candecomp Parafact Algorithm-ALS to Handle Sparse Data In Food Commerce Information ServicesRecommender systems have been widely researched in many applications especially in e-commerce services with the aim to make clear and easy communication between consumer and provider. Simple examples of Recommender systems would include personal and ",145,48,0.813798369218906,11,0.816569349982522
180,"Accomodation and food service activities","Information Retrieval",0.833102881908417,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector of currently available studies on emerging technologies in agriculture and food sector (A&F  The
opportunities of the new big-data-augmented methodology are shown in comparison to existing 
with special attention to use of bigger volumes of data, machine learning and ontology  
",145,48,0.813798369218906,11,0.816569349982522
186,"Accomodation and food service activities","Information Retrieval",0.818235874176025,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","A review on use of big data in warehousing to enhance accessibility of food and outreach across the agricultural value- chain is ensured by Big- Data methods and practices.
This information is spread across input providers and produce buyers. This paper will analyze
the lacunas in data accessibility which render the efficacy of adequate supply of food  
",145,48,0.813798369218906,11,0.816569349982522
210,"Accomodation and food service activities","Information Retrieval",0.801103055477142,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","[HTML][HTML] Big-data-augmented approach to emerging technologies identification: case of agriculture and food sector <U+041F><U+043E><U+0445><U+043E><U+0436><U+0438><U+0435> <U+043F><U+0443><U+0431><U+043B><U+0438><U+043A><U+0430><U+0446><U+0438><U+0438>. Mapping the Radical Innovations in Food Industry: A Text Mining Study.
Kuzminov I., Bakhtin PD, Khabirova E. et al  I: Advances in Artificial Intelligence and Its Applications 
for gaining insight into the underlying conceptual structure of the data  
",145,48,0.813798369218906,11,0.816569349982522
216,"Accomodation and food service activities","Information Retrieval",0.802014589309692,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","[CITATION][C] Application of a business intelligence tool within the context of big data in a food industry companySmart ManufacturingPotential of New Digital Technologies and Big Data in the Food Industry",145,48,0.813798369218906,11,0.816569349982522
241,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.876822173595428,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Comparing rule-based and deep learning models for patient phenotyping modern methods use relevant concepts in a note as input to a machine learning algorithm to  As
we mentioned before, the goal with our data is to understand phenotypes that are  validation of
this approach in other types of clinical notes such as social work assessment to  
",195,35,0.819798954895565,11,0.824095173315568
247,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.842306435108185,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Predictive and prescriptive analytics, machine learning and child welfare risk assessment: The Broward County experience resonates well with both the American ethos of encouraging strong families and social work ideals
of  making and actuarial methods points to the need for computational and artificial intelligence
models that  are applied to an unseen sample set of data, the testing data set, to  
",195,35,0.819798954895565,11,0.824095173315568
251,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.841031908988953,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Can Predictive Algorithms Assist Decision-Making in Social Work with Children and Families? (1985) explored the potential of what they refer to as 'artificial intelligence' to develop 'expert  every
time' Decision Support Systems and Child and Family Social Work  In another instance where
a big data approach has been developed but has yet to be applied, Schwartz et al  
",195,35,0.819798954895565,11,0.824095173315568
263,"Human health and social work activities","Big Data",0.842475295066833,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Digital economics Strong artificial intelligence  Deep learning A machine learning technique that learns features
and tasks directly from data using an architecture of layers of neural networks. Big data Refers
to voluminous amounts of structured or unstructured data  
",195,99,0.823880515315316,11,0.83771993897178
265,"Human health and social work activities","Big Data",0.844025909900665,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[BOOK][B] Evidence-Based Decision-Making: How to Leverage Available Data and Avoid Cognitive Biases often felt like the proverbial salmon swimming upstream, especially when my data analytic findings 
a typically large set of possibilities) nature, chess naturally lends itself to machine learning in
the  systems roughly parallels the history of what is known as 'artificial intelligence' (AI  
",195,99,0.823880515315316,11,0.83771993897178
266,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.838338494300842,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","[BOOK][B] Evidence-Based Decision-Making: How to Leverage Available Data and Avoid Cognitive Biases often felt like the proverbial salmon swimming upstream, especially when my data analytic findings 
a typically large set of possibilities) nature, chess naturally lends itself to machine learning in
the  systems roughly parallels the history of what is known as 'artificial intelligence' (AI  
",195,35,0.819798954895565,11,0.824095173315568
278,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.826120793819427,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Singularity academics across the globe, gives a precise definition of artificial intelligence, its constituency 
large degree of integration and cross-fertilization among AI, machine learning, statistics, control 
of shared theoretical frameworks, combined with the availability of data and processing  
",195,35,0.819798954895565,11,0.824095173315568
301,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.809906005859375,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Do citations and readership identify seminal publications? deep learning paper which has caused a shift in the area of artificial intelligence/computer vision 
To do this, we use the threshold which achieves the best accuracy on the training data  The reason
why we chose the this simple model instead of a machine learning model such  
",195,35,0.819798954895565,11,0.824095173315568
305,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.806613564491272,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","The promise and the challenge of technology-facilitated methods for assessing behavioral and cognitive markers of risk for suicide among US Army National Guard  Salt Lake City, UT 84108, USA 4 Department of Social Work, University of  BSP also involves
additional data processing steps prior to generating behavioral markers  Recent developments
have extended these efforts by incorporating artificial intelligence techniques resulting in  
",195,35,0.819798954895565,11,0.824095173315568
312,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.797932922840118,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","A Cognitive Perspective For example, when thinking about artificial intelligence (AI) in healthcare, Christopher Khoury,
vice president of  was driven by the question of What if you could use data science to  around those
tendencies? Using more than 700 variables and machine learning, the company  
",195,35,0.819798954895565,11,0.824095173315568
323,"Human health and social work activities","Big Data",0.859790325164795,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Hands on the wheel: Navigating algorithmic management and Uber drivers' autonomy More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,99,0.823880515315316,11,0.83771993897178
326,"Human health and social work activities","Big Data",0.849044442176819,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Sentiment analysis for hate speech detection on social media: TF-IDF weighted N-Grams based approach As such, preprocessing unstructured data is a very important role in the text classification  The
number of features can therefore be quite big for a corpus that is average sized  problems and
poses a significant problem to many machine learning algorithms (Yang & Pedersen  
",195,99,0.823880515315316,11,0.83771993897178
332,"Human health and social work activities","Big Data",0.813762724399567,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Analysing large volumes of complex qualitative data-Reflections from a group of international experts Georgia Philip is a Research Fellow in the School of Social Work, at the University of East Anglia 
analysis?' In so doing, he considers the advantages and challenges of using Machine Learning
to assist with coding and help researchers handle large volumes of data in a  
",195,99,0.823880515315316,11,0.83771993897178
343,"Human health and social work activities","Big Data",0.863132476806641,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Hands on the wheel: Navigating algorithmic management and Uber drivers' More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,99,0.823880515315316,11,0.83771993897178
365,"Human health and social work activities","Big Data",0.79951137304306,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] STATE OF THE NORTH There are five big challenges which this generation will need to meet as they gradually 
Digitalisation, artificial intelligence, machine learning and advanced robotics are of particular
focus for economists, and are starting to  Source: HMRC, 'Summary data tables' (HMRC 2017)  
",195,99,0.823880515315316,11,0.83771993897178
376,"Human health and social work activities","Big Data",0.851979076862335,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Value co-creation and potential benefits through big data analytics: Health benefit analysis practice where the newer healthcare delivery models depend on user-friendly, real-time big data
analytics, artificial intelligence (AI) and machine learning (ML) tools, and that millions  regarding
the Finnish health data environment. However, they do not provide any  
",195,99,0.823880515315316,11,0.83771993897178
383,"Human health and social work activities","Big Data",0.840397357940674,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[BOOK][B] Toward information justice: Technology, politics, and policy for data in higher education administration 31 2.2 Big Data in Higher Education  scientists and users to accept current data practices and
outcomes as natural or inevitable, and to make data use the  Even in manual technolo- gies, the
technique reduces the human to machine, carrying out tasks as if human practitioners  
",195,99,0.823880515315316,11,0.83771993897178
389,"Human health and social work activities","Big Data",0.829444050788879,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[PDF][PDF] Coding Over the Cracks: Predictive Analytics and Child Protection families and families of color.3 In this age of automation and artificial intelligence, a tempting  are
building and deploying tools that pull together vast quantities of data stored by  analytics and
explains the fundamentally human process of developing a machine learning algorithm  
",195,99,0.823880515315316,11,0.83771993897178
398,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.78546416759491,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Efficiency, Correctness, and the Authority of Automation: Technology in College Basic Writing Instruction 101 DATA ANALYSIS  Yet there also are moments of authentic possibility for broader learning
and understanding through the use of the automated system  Virtually all remedial English at
the college level could be handled by automation, with the machine as an impartial judge  
",195,35,0.819798954895565,11,0.824095173315568
407,"Human health and social work activities","Big Data",0.821356296539307,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Re-examining and re-conceptualising Enterprise Search and Discovery capability: Towards a model for the factors and generative mechanisms for search task  225 5.6.2.3 Suboptimal Learning/Sharing culture  These include, the Statistical Machine (Goldberg
1927), Mundaneum (Otlet 1934), World Brain(Wells 1937), Universal  Organizations seek to exploit
'big data' volumes for differentiating insights supporting wealth creation  
",195,99,0.823880515315316,11,0.83771993897178
409,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.817738771438599,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Re-examining and re-conceptualising Enterprise Search and Discovery capability: Towards a model for the factors and generative mechanisms for search task  225 5.6.2.3 Suboptimal Learning/Sharing culture  These include, the Statistical Machine (Goldberg
1927), Mundaneum (Otlet 1934), World Brain(Wells 1937), Universal  Organizations seek to exploit
'big data' volumes for differentiating insights supporting wealth creation  
",195,35,0.819798954895565,11,0.824095173315568
418,"Human health and social work activities","Reliable and Interpretable Artificial Intelligence",0.822771668434143,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Future Implications of the Psychopathy Construct for Criminology and Criminal Justice Policy and Practice Torture, 6 For example, Rhodes (2002) provides participation observation data showing that 
Sensors and machine learning algorithms have been designed that can measure affective
information  In addition, artificial intelligence advances that can make robots feel so to speak  
",195,35,0.819798954895565,11,0.824095173315568
639,"Electricity, gas, steam and air conditioning supply","Big Data",0.833040773868561,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Big data analytics for discovering electricity consumption patterns in smart citiesNew technologies such as sensor networks have been incorporated into the management of buildings for organizations and cities. Sensor networks have led to an exponential increase in the volume of data available in recent years, which can be used to extract consumption ",105,99,0.823880515315316,11,0.827138721942902
656,"Electricity, gas, steam and air conditioning supply","Big Data",0.817728817462921,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Exploring big data for development: An electricity sector case study from IndiaThis paper presents exploratory research into data-intensive development that seeks to inductively identify issues and conceptual frameworks of relevance to big data in developing countries. It presents a case study of big data innovations in Stelcorp; a state electricity  ",105,99,0.823880515315316,11,0.827138721942902
661,"Electricity, gas, steam and air conditioning supply","Big Data",0.795249700546265,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Winning the Big Data Technologies Horizon Prize: Fast and reliable forecasting of electricity grid traffic by identification of recurrent fluctuationsThis paper provides a description of the approach and methodology I used in winning the European Union Big Data Technologies Horizon Prize on data-driven prediction of electricity grid traffic. The methodology relies on identifying typical short-term recurrent fluctuations ",105,99,0.823880515315316,11,0.827138721942902
667,"Electricity, gas, steam and air conditioning supply","Big Data",0.843502104282379,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Realization and Research of Intelligent system of client electricity information based on the Big Data Processing TechnologyBig data is the focus in power system currently. In order to analyze and classify the electricity model of clients, identify the avoiding peak space intelligently, extract value-added information of clients and control the electric load actively, it is extremely necessary to ",105,99,0.823880515315316,11,0.827138721942902
680,"Electricity, gas, steam and air conditioning supply","Big Data",0.834288597106934,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Sliding Time Window Electricity Consumption Optimization Algorithm for Communities in the Context of Big Data ProcessingBig data frameworks enable companies from various fields to build models that allow them to increase profit margins by improving decision making at different levels (middle management, senior management, and board) or by attempting to boost sales by ",105,99,0.823880515315316,11,0.827138721942902
690,"Electricity, gas, steam and air conditioning supply","Big Data",0.87535560131073,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","The Design of Distributed Power Big Data Analysis Framework and Its Application in Residential Electricity AnalysisWith the development of digital, information and intelligent process of power system, more and more data sources appear. The traditional standalone environment has been difficult to adapt to the need of the analysis of massive data. The power industry also needs to use real ",105,99,0.823880515315316,11,0.827138721942902
695,"Electricity, gas, steam and air conditioning supply","Big Data",0.836377501487732,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Visualization as a Mean of Big Data Management: Using Qatar's Electricity Consumption DataVisualization as a mean of big data management is the new century revolution. Managing data has become a great challenge today, as the amount of raw data size is increasing rapidly. For data like electricity consumption, a new data value is received every minute from ",105,99,0.823880515315316,11,0.827138721942902
717,"Electricity, gas, steam and air conditioning supply","Big Data",0.817379653453827,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[BOOK][B] Control and Automation Systems for Electricity Distribution Networks (EDN) of the futureThe CIGRÉ C6 Study Committee (Distribution Systems and Dispersed Generation) considers the different aspects of integration of distributed generation. In this context, the JWG C6. 25/B5 has worked to map current functionalities and to identify future needs for the ",105,99,0.823880515315316,11,0.827138721942902
721,"Electricity, gas, steam and air conditioning supply","Big Data",0.84548407793045,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","A Big Data Analytics Model for Household Electricity Consumption Tracking and MonitoringThe abundance of data nowadays can offer infinite opportunities and possibilities if being systematically explored. Exploration of the data can be achieved through the application of big data analytics (BDA). Consequently, a number of BDA models are seen developed in a ",105,99,0.823880515315316,11,0.827138721942902
729,"Electricity, gas, steam and air conditioning supply","Big Data",0.818228781223297,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","IArchitecture based on multivariate big data platform for analyzing electricity consumption behavior,""With the development of smart grid, more and more measuring devices extend to bottom layer. The development of advanced measurement system and distribution network inevitably leads to the geometric increase of user data. On the other hand, the power grid is ",105,99,0.823880515315316,11,0.827138721942902
738,"Electricity, gas, steam and air conditioning supply","Big Data",0.781890332698822,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Machine learning methods for the analysis of data of an Electricity Distribution Network OperatorOnce every few decades an invention changes the landscape of some aspects of our life. Industrial revolutions improved our everyday lives whilst medical revolutions expanded our lifespans. In the path we're leading, most of sciences will be reduced to computer science ",105,99,0.823880515315316,11,0.827138721942902
427,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.795071840286255,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Publicising food: big data, precision agriculture, and co-experimental techniques of addition A number of technological forms are thus investigated: eg, big data (big soil data, big climatedata, etc.), precision agriculture, and a variety of internet-based platforms utilised by
self-described activists and proponents of more local and regional based foodscapes  
",180,47,0.827168932620515,10,0.822573441267014
457,"Agriculture, forestry and fishing","Machine Learning",0.782967925071716,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","New trends in precision agriculture: a novel cloud-based system for enabling data storage and agricultural task planning and automationIt is well-known that information and communication technologies enable many tasks in the context of precision agriculture. In fact, more and more farmers and food and agriculture companies are using precision agriculture-based systems to enhance not only their products ",180,79,0.825409763975988,10,0.821002465486526
479,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.801326394081116,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","0245 Big data and occupational health vigilance: use of french medico-administrative databases for hypothesis generation regarding occupational risks in agriculture Poster Presentation. Methodology. 0245 Big data and occupational health vigilance: use of 
medico-administrative databases for hypothesis generation regarding occupational risks inagriculture  complementary methods relying on exploitation of already existing data, such as  
",180,47,0.827168932620515,10,0.822573441267014
481,"Agriculture, forestry and fishing","Machine Learning",0.803082048892975,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[PDF][PDF] Is big data driving a paradigm shift in precision agriculture Much of the discussion focuses on whether the arrival of big data signals the emergence of or
need for a new  Adoption, profitability, and making better use of precision farming data  The research
status on precision agriculture by use of bibliometric analysis from three databases  
",180,79,0.825409763975988,10,0.821002465486526
489,"Agriculture, forestry and fishing","Machine Learning",0.81128066778183,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM: Data to decisionsBig data are frequently used in many facets of business and agronomy to enhance knowledge needed to improve operational decisions. Livestock operations collect data of sufficient quantity to perform predictive analytics. Predictive analytics can be defined as a ",180,79,0.825409763975988,10,0.821002465486526
491,"Agriculture, forestry and fishing","Machine Learning",0.806378066539764,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Multi-sensor Data Fusion Algorithm of Wisdom Agriculture Based on Fusion SetIn wisdom agriculture, the advanced high-tech equipment is applied and human input is reduced to lower the operation and management costs and enhance agricultural management efficiency. In this thesis, a multi-sensor data fusion algorithm based on fusion ",180,79,0.825409763975988,10,0.821002465486526
495,"Agriculture, forestry and fishing","Machine Learning",0.825850427150726,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[PDF][PDF] BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM intelligence dedicated to the study of algorithms for prediction and inference. Learning  about
the data-generating mechanism in practical scenarios. Precision animal agriculture allows
farmers to formulate prompt management practices, and a predictive  
",180,79,0.825409763975988,10,0.821002465486526
504,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.780626237392426,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Application of big data in precision agriculture.Precision agriculture is the future way for agricultural modernization. The rapid expansion of
agricultural data and the development of big data technology provide a new method for the
development of precision agriculture, and become an important force leading to the development  
",180,47,0.827168932620515,10,0.822573441267014
507,"Agriculture, forestry and fishing","Machine Learning",0.817773222923279,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Machine learning based data processing and latency reduction in the internet of things for agricultureThe Internet of Things is best stated as a network of things that have the ability to generate and share information between themselves and interact with the environment according to the percepts from this environment. This network between these devices and humans ",180,79,0.825409763975988,10,0.821002465486526
536,"Agriculture, forestry and fishing","Machine Learning",0.834704339504242,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Artificial Intelligence on Remote Sensing Data for Precision Agriculture ApplicationsPrecision agriculture benefits greatly from information provided by high spatial resolution and high temporal frequency remotely sensed images. It requires effective methodologies and algorithms to exact information from the huge volume, dimension and variety of raw ",180,79,0.825409763975988,10,0.821002465486526
547,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.822238981723785,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector and shortcomings of currently available studies on emerging technologies in agriculture and
food  The opportunities of the new big-data-augmented methodology are shown in comparison
to  with special attention to use of bigger volumes of data, machine learning and ontology  
",180,47,0.827168932620515,10,0.822573441267014
548,"Agriculture, forestry and fishing","Machine Learning",0.820477783679962,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector and shortcomings of currently available studies on emerging technologies in agriculture and
food  The opportunities of the new big-data-augmented methodology are shown in comparison
to  with special attention to use of bigger volumes of data, machine learning and ontology  
",180,79,0.825409763975988,10,0.821002465486526
564,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.851788461208344,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Processing of Big Data in Internet of Things and Precision Agriculture.The main focus of the paper is the analysis of various types of agriculture data and open source
operational databases and platforms for data collection and data warehousing suitable for storing
data obtained from the Internet of Things and Precision Agriculture. The methodical approach  
",180,47,0.827168932620515,10,0.822573441267014
571,"Agriculture, forestry and fishing","Machine Learning",0.858769714832306,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry Predictive models developed using Big Data identify best management practices for achieving
the best  machine learning algorithms and rooted in comprehensive and reliable data- sets, provide 
recent decades, farmers have been intro- duced to precision agriculture, which is a  
",180,79,0.825409763975988,10,0.821002465486526
572,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.852974534034729,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry Predictive models developed using Big Data identify best management practices for achieving
the best  machine learning algorithms and rooted in comprehensive and reliable data- sets, provide 
recent decades, farmers have been intro- duced to precision agriculture, which is a  
",180,47,0.827168932620515,10,0.822573441267014
578,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.798387289047241,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","A Big Data Virtualization Role in Agriculture: A Comprehensive Review Page 11. A Big Data Virtualization Role in Agriculture  Fuzzy reasoning provides
uncertainty in both data and output.  Machine learning: It is another artificial intelligence
technique that allows both supervised and unsupervised methods  
",180,47,0.827168932620515,10,0.822573441267014
581,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.850886940956116,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[CITATION][C] Analysis of agriculture data using data mining techniques: application of big data in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",180,47,0.827168932620515,10,0.822573441267014
589,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.837716042995453,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics."," analyzes satellite data, market data, and weather data us- ing machine learning and big data
analytics to  is a relatively new company and evaluations of its program are forthcoming
(e-Agriculture, 2017  From Data to Decision All types of big data must go through a series of steps  
[PDF][PDF] How'Big Data'affects competition law analysis in Online Platforms and Agriculture: does one size fit all?",180,47,0.827168932620515,10,0.822573441267014
592,"Agriculture, forestry and fishing","Machine Learning",0.848740458488464,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[CITATION][C]  Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning Techniques with Retail Scanner DataBig data analytics framework for agriculture",180,79,0.825409763975988,10,0.821002465486526
594,"Agriculture, forestry and fishing","Artificial Intelligence and Data Science",0.834717690944672,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[CITATION][C]  Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning Techniques with Retail Scanner DataBig data analytics framework for agriculture",180,47,0.827168932620515,10,0.822573441267014
655,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.82706344127655,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Exploring big data for development: An electricity sector case study from IndiaThis paper presents exploratory research into data-intensive development that seeks to inductively identify issues and conceptual frameworks of relevance to big data in developing countries. It presents a case study of big data innovations in Stelcorp; a state electricity  ",105,120,0.829422693451246,10,0.828750336170197
662,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.795200347900391,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Winning the Big Data Technologies Horizon Prize: Fast and reliable forecasting of electricity grid traffic by identification of recurrent fluctuationsThis paper provides a description of the approach and methodology I used in winning the European Union Big Data Technologies Horizon Prize on data-driven prediction of electricity grid traffic. The methodology relies on identifying typical short-term recurrent fluctuations ",105,120,0.829422693451246,10,0.828750336170197
668,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.833682358264923,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Realization and Research of Intelligent system of client electricity information based on the Big Data Processing TechnologyBig data is the focus in power system currently. In order to analyze and classify the electricity model of clients, identify the avoiding peak space intelligently, extract value-added information of clients and control the electric load actively, it is extremely necessary to ",105,120,0.829422693451246,10,0.828750336170197
673,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.817668557167053,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Revealing Household Characteristics from Electricity Meter Data with Grade Analysis and Machine Learning AlgorithmsIn this article, the Grade Correspondence Analysis (GCA) with posterior clustering and visualization is introduced and applied to extract important features to reveal households' characteristics based on electricity usage data. The main goal of the analysis is to ",105,120,0.829422693451246,10,0.828750336170197
678,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.845076322555542,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Machine learning based electricity demand forecastingIn this empirical study we develop forecasting models for electricity demand using publicly available data and three models based on machine learning algorithms. It compares accuracy of these models using different evaluation metrics. The data consist of several ",105,120,0.829422693451246,10,0.828750336170197
681,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.824479103088379,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Sliding Time Window Electricity Consumption Optimization Algorithm for Communities in the Context of Big Data ProcessingBig data frameworks enable companies from various fields to build models that allow them to increase profit margins by improving decision making at different levels (middle management, senior management, and board) or by attempting to boost sales by ",105,120,0.829422693451246,10,0.828750336170197
692,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.860660314559937,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","The Design of Distributed Power Big Data Analysis Framework and Its Application in Residential Electricity AnalysisWith the development of digital, information and intelligent process of power system, more and more data sources appear. The traditional standalone environment has been difficult to adapt to the need of the analysis of massive data. The power industry also needs to use real ",105,120,0.829422693451246,10,0.828750336170197
696,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.824410796165466,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Visualization as a Mean of Big Data Management: Using Qatar's Electricity Consumption DataVisualization as a mean of big data management is the new century revolution. Managing data has become a great challenge today, as the amount of raw data size is increasing rapidly. For data like electricity consumption, a new data value is received every minute from ",105,120,0.829422693451246,10,0.828750336170197
703,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.815408945083618,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big Data Analytics for Electricity Price ForecastElectricity Price forecast is a major task in smart grid operation. There is a massive amount of data flowing in the power system including the data collection by control systems, sensors, etc. In addition, there are many data points which are not captured and processed by the ",105,120,0.829422693451246,10,0.828750336170197
722,"Electricity, gas, steam and air conditioning supply","Applied data analysis",0.843853175640106,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A Big Data Analytics Model for Household Electricity Consumption Tracking and MonitoringThe abundance of data nowadays can offer infinite opportunities and possibilities if being systematically explored. Exploration of the data can be achieved through the application of big data analytics (BDA). Consequently, a number of BDA models are seen developed in a ",105,120,0.829422693451246,10,0.828750336170197
877,"Public administration and defence, compulsory social security","Applied data analysis",0.79718691110611,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","To do more, better, faster and more cheaply: Using big data in public administrationBig data have become a game-changer for modern public administration in those areas in which they are used. Although their application is still limited in the public sector, their use develops dynamically in areas where they bring tangible results in terms of efficiency and ",110,120,0.829422693451246,10,0.822135627269745
885,"Public administration and defence, compulsory social security","Machine Learning",0.858972609043121,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Machine learning for public administration research, with application to organizational reputationAbstract Machine learning methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories ",110,79,0.825409763975988,10,0.821230959892273
893,"Public administration and defence, compulsory social security","Machine Learning",0.81195855140686,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","An information system for judicial and public administration using artificial intelligence and geospatial dataThe adoption of information technology in judicial and public administration has become a major need nowadays with the rapid growth of information regarding managerial issues. This paper presents an advanced methodology developed by using Information and ",110,79,0.825409763975988,10,0.821230959892273
898,"Public administration and defence, compulsory social security","Applied data analysis",0.828169822692871,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A Study on Conversational Public Administration Service of the Chatbot Based on Artificial IntelligenceArtificial intelligence-based services are expanding into a new industrial revolution. There is artificial intelligence technology applied in real life due to the development of big data and deep learning related technology. And data analysis and intelligent assistant services that ",110,120,0.829422693451246,10,0.822135627269745
901,"Public administration and defence, compulsory social security","Applied data analysis",0.80888158082962,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Public Administration Curriculum-Based Big Data Policy-Analytic Epistemology: Symbolic IoT Action-Learning Solution ModelThe equilibration that underscores the internet of things (IoT) and big data analytics (BDA) cannot be underestimated at the behest of real-life social challenges and significant policy data generated to redress the concerns of epistemic communities, such as political policy ",110,120,0.829422693451246,10,0.822135627269745
908,"Public administration and defence, compulsory social security","Machine Learning",0.788538038730621,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Digital and Intelligent Public Administration: transformations in the era of artificial intelligenceThis article addresses the impact of the digital era and it specifically refers to information and communication technologies (ICT) in Public Administration. It is based on the international approach and underscores the importance of incorporating new technologies established by ",110,79,0.825409763975988,10,0.821230959892273
912,"Public administration and defence, compulsory social security","Machine Learning",0.819602906703949,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","This paper describes a model of digital governance that reproduces within the system essential features of public administration while establishing logic for their utilization. The ultimate goal is to be able to confine all participants to their respective roles and Administration by Algorithm? Public Management Meets Public Sector Machine Learning",110,79,0.825409763975988,10,0.821230959892273
917,"Public administration and defence, compulsory social security","Machine Learning",0.865606784820557,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Machine Learning-Based Models for Big Data Analysis and Prediction: Social Security ApplicationsThe core technology of the Fourth Industrial Revolution is artificial intelligence and big data, and the continuous enhancement of algorithm performance through Machine Learning based on large scale accumulated data is an mportant source technology in all fields ",110,79,0.825409763975988,10,0.821230959892273
919,"Public administration and defence, compulsory social security","Applied data analysis",0.852837800979614,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Machine Learning-Based Models for Big Data Analysis and Prediction: Social Security ApplicationsThe core technology of the Fourth Industrial Revolution is artificial intelligence and big data, and the continuous enhancement of algorithm performance through Machine Learning based on large scale accumulated data is an mportant source technology in all fields ",110,120,0.829422693451246,10,0.822135627269745
934,"Public administration and defence, compulsory social security","Applied data analysis",0.82562667131424,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Research on the Optimization of Military Supplies under Big Data BackgroundOn the basis of analyzing the characteristics of big data technology and combining the actual demand of military demand industry in our army, this paper constructs a comprehensive analysis environment of big data for military demand, and on this basis ",110,120,0.829422693451246,10,0.822135627269745
936,"Public administration and defence, compulsory social security","Applied data analysis",0.7940713763237,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Analysis of Military Academy Smart Campus Based on Big DataThis paper compares the digital campus with the smart campus and analysis the framework of smart campus in the big data environment, based on the actual characteristics of military academies. The framework utilizes the information technologies such as Internet of Things ",110,120,0.829422693451246,10,0.822135627269745
937,"Public administration and defence, compulsory social security","Machine Learning",0.787513732910156,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Analysis of Military Academy Smart Campus Based on Big DataThis paper compares the digital campus with the smart campus and analysis the framework of smart campus in the big data environment, based on the actual characteristics of military academies. The framework utilizes the information technologies such as Internet of Things ",110,79,0.825409763975988,10,0.821230959892273
947,"Public administration and defence, compulsory social security","Machine Learning",0.789879143238068,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Artificial Intelligence Within the Military Domain and Cyber WarfareThe potential uses of machine learning and artificial intelligence in the cyber security domain have had a recent surge of interest. Much of the research and discussions in this area primarily focuses on reactive uses of the technology such as enhancing capabilities in ",110,79,0.825409763975988,10,0.821230959892273
950,"Public administration and defence, compulsory social security","Applied data analysis",0.822233855724335,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Application of Big Data Technology in Scientific Research Data Management of Military EnterprisesScientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology ",110,120,0.829422693451246,10,0.822135627269745
955,"Public administration and defence, compulsory social security","Applied data analysis",0.829391598701477,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","The Impact of a BIG DATA Decision Support Tool on Military Logistics: MEDICAL ANALYTICS MEETS THE MISSION.Using big data and predictive analytics, more segments of the US military will be able to create decision support tools that help them not only to carry out their missions more efficiently, but also to streamline their logistical requirements. Within the military's medical ",110,120,0.829422693451246,10,0.822135627269745
963,"Public administration and defence, compulsory social security","Machine Learning",0.83164644241333,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Machine learning techniques for autonomous agents in military simulationsMultum in parvoIn military simulations, software agents are used to represent individuals, weapon platforms or aggregates thereof. Modeling the behavioral capabilities and limitations of such agents may be time-consuming, requiring extensive interaction with subject matter experts and ",110,79,0.825409763975988,10,0.821230959892273
967,"Public administration and defence, compulsory social security","Applied data analysis",0.828915655612946,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Research on Military Logistics based on Big DataThis document gives formatting instructions for authors preparing papers for publication. With the arrival of the era of big data, the mature use of cloud computing and data mining technology, the big data mode have been widely applied in logistics. For military logistics, a ",110,120,0.829422693451246,10,0.822135627269745
975,"Public administration and defence, compulsory social security","Applied data analysis",0.834040999412537,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Are We Flooding Pilots with Data?Effects of Situational Awareness Automation Support Concepts on Decision-Making in Modern Military Air OperationsWithin highly dynamic situations, the amount of relevant information that a pilot needs to process to make an informed decision can be substantial. With an ever increasing amount of data available to the pilot there is a real risk that not all relevant data can be taken into ",110,120,0.829422693451246,10,0.822135627269745
976,"Public administration and defence, compulsory social security","Machine Learning",0.827995300292969,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Are We Flooding Pilots with Data?Effects of Situational Awareness Automation Support Concepts on Decision-Making in Modern Military Air OperationsWithin highly dynamic situations, the amount of relevant information that a pilot needs to process to make an informed decision can be substantial. With an ever increasing amount of data available to the pilot there is a real risk that not all relevant data can be taken into ",110,79,0.825409763975988,10,0.821230959892273
981,"Public administration and defence, compulsory social security","Machine Learning",0.830596089363098,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Article deals with a set of problems linked to a Engineer Force Protection Provision algorithm design and evaluation of input factors series. This algorithm is generally compatible with The NATO Force Protection Process Model adjusting it to a part of engineer forces' decision Intrusion Detection of Data Platform Based on Extreme Learning Machine in Civil and Military Integration",110,79,0.825409763975988,10,0.821230959892273
83,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.79375821352005,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Publicising food: big data, precision agriculture, and co-experimental techniques of addition interviews, involving 18 Iowa farmers, 14 individuals from big data industry (those involved in
the sale and promotion of large-scale data acquisition, predictive analytic software, and/or
precision agriculture technologies), and 19 interviews of regional food system entrepreneurs  
",145,47,0.827168932620515,9,0.8206078476376
84,"Accomodation and food service activities","Data Analystics for Smart Grids",0.790668547153473,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Publicising food: big data, precision agriculture, and co-experimental techniques of addition interviews, involving 18 Iowa farmers, 14 individuals from big data industry (those involved in
the sale and promotion of large-scale data acquisition, predictive analytic software, and/or
precision agriculture technologies), and 19 interviews of regional food system entrepreneurs  
",145,67,0.828709795403836,9,0.806300706333584
88,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.855027675628662,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Big data in food safety: An overview A list of the most used analysis methods for big data is shown in Table 3. These meth  These
systems are developed using data mining techniques (collaborative filtering, content based filtering
and hybrid  To the author's knowledge, these systems are not yet applied in food safety  
",145,47,0.827168932620515,9,0.8206078476376
92,"Accomodation and food service activities","Data Analystics for Smart Grids",0.808044493198395,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Facilitating knowledge management through filtered big data: SME competitiveness in an agri-food sector knowledge management (KM) process that utilises filtered big data within an agri-food supply
chain  The specific big data consumer analytics examined is those of the Tesco Clubcard data,
otherwise  17 million customers), with 10 per cent of this customer data being processed  
",145,67,0.828709795403836,9,0.806300706333584
108,"Accomodation and food service activities","Data Analystics for Smart Grids",0.805051743984222,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big Data and the transformation of food and beverage marketing: undermining efforts to reduce obesity? of digital marketing, harnessing the power of Big Data analytics, artificial intelligence, and powerful 
rich views of behavioral patterns that can be highly valuable to food and beverage  For example,
leading data company Neustar can identify people who display the following  
",145,67,0.828709795403836,9,0.806300706333584
114,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.782435894012451,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Satellite data and machine learning for weather risk management and food securityThe increase in frequency and severity of extreme weather events poses challenges for the agricultural sector in developing economies and for food security globally. In this article, we demonstrate how machine learning can be used to mine satellite data and identify pixel ",145,47,0.827168932620515,9,0.8206078476376
143,"Accomodation and food service activities","Data Analystics for Smart Grids",0.799345552921295,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] Can business generated big food data be used to understand food consumption behaviour and can a research infrastructure be generated around such data  platforms, (b) stakeholders along the food chain, and (c) policy actors in the agricultural-food
and nutrition-health  behaviour can be extracted from existing business generated data and on
which conditions these data might feed into a future RICHFIELDS big data platform  
",145,67,0.828709795403836,9,0.806300706333584
149,"Accomodation and food service activities","Data Analystics for Smart Grids",0.784600853919983,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big Data Analytics for Traceability in Food Supply Chain WAINA 2019: Web, Artificial Intelligence and Network Applications pp 880-884 | Cite as. Big Data
Analytics for Traceability in Food Supply Chain  The amount of socio-economic data generated
every day has grown dramatically in recent years thanks to the widespread use of the  
",145,67,0.828709795403836,9,0.806300706333584
157,"Accomodation and food service activities","Data Analystics for Smart Grids",0.796954095363617,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Design and Realization of Food Safety Monitoring and Pre-Control System Based on Multi-Source and Big Data efficiency,poor timeliness,and incomplete data. In order to realize the sharing of resources and
information in the process of food safety monitoring,this study designed and developed a food
safety monitoring and control system based on the multi-source and big data under the  
",145,67,0.828709795403836,9,0.806300706333584
164,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.834947168827057,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[PDF][PDF] Recommender System Based Tensor Candecomp Parafact Algorithm-ALS to Handle Sparse Data In Food Commerce Information ServicesRecommender systems have been widely researched in many applications especially in e-commerce services with the aim to make clear and easy communication between consumer and provider. Simple examples of Recommender systems would include personal and ",145,47,0.827168932620515,9,0.8206078476376
167,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.831271767616272,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Discussion on the Reform of Teaching Methods for Specialized Courses under the Background of Big Data-Taking Animal Food Technology as an Example E. Make full use of big data, help better feedback students' learning situation, and establish more 
feedback for the leaning situation of students using ""Xuexitong"" on the Animal Food Technology
Mooc  These detailed and specific data to be obtained, on the one hand, can help  
",145,47,0.827168932620515,9,0.8206078476376
173,"Accomodation and food service activities","Data Analystics for Smart Grids",0.8414186835289,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Systems Approach to Link Big Socio-ecological Geo-data to Food Systems Sustainability of (i) what information commonly needed by food system actors to response and adapt to
socio-ecological change and enhance the system performance, (ii) interoperability between
different types of data across scales, and (iii) sufficient guidance to utilize big data resources  
",145,67,0.828709795403836,9,0.806300706333584
176,"Accomodation and food service activities","Data Analystics for Smart Grids",0.832846939563751,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Emerging Data Governance Issues in Big Data Applications for Food SafetyThe food industry and food safety authorities show an increasing interest in Big Data applications. On the one hand, Big Data strengthens data storage, data mashup, and methodology of risk assessment; on the other hand, the presence of risks and challenges ",145,67,0.828709795403836,9,0.806300706333584
181,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.811061441898346,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector of currently available studies on emerging technologies in agriculture and food sector (A&F  The
opportunities of the new big-data-augmented methodology are shown in comparison to existing 
with special attention to use of bigger volumes of data, machine learning and ontology  
",145,47,0.827168932620515,9,0.8206078476376
194,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.798881828784943,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry make decisions that will increase yields and deliver safe, nutritious food to communities  Predictive
models developed using Big Data identify best management practices for achieving the  machinelearning algorithms and rooted in comprehensive and reliable data- sets, provide  
",145,47,0.827168932620515,9,0.8206078476376
198,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.840157091617584,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Food trend based on social media for big data analysis using K-mean clustering and SAW: A case study on yogyakarta culinary industry 1. Fig. 1. Big Data Processing Pipeline  From the data that has been done cleaning the data, then
convert in a bag of words matrix to be applied machine learning algorithm. Then perform feature
extraction to find relevant features in classifying trendy food data  
",145,47,0.827168932620515,9,0.8206078476376
201,"Accomodation and food service activities","Artificial Intelligence and Data Science",0.837929546833038,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[CITATION][C] Big data applications in food safety and quality in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",145,47,0.827168932620515,9,0.8206078476376
208,"Accomodation and food service activities","Data Analystics for Smart Grids",0.797775447368622,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[CITATION][C]  -land Algorithm Based on Neighborhood and Temporal Anomalies (FANTA) to Map Planted Versus Fallowed Croplands Using MODIS Data to Assist in [CITATION][C] Identify how big data enables forecasting and demand planning in food and beverages industry",145,67,0.828709795403836,9,0.806300706333584
637,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.844626247882843,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Big data analytics for discovering electricity consumption patterns in smart citiesNew technologies such as sensor networks have been incorporated into the management of buildings for organizations and cities. Sensor networks have led to an exponential increase in the volume of data available in recent years, which can be used to extract consumption ",105,53,0.824819772873285,9,0.822341038121117
649,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.79725307226181,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Forecasting Residential Electricity Demand Through Machine Learning and Model SynthesisThis paper aims to develop a predictive model of residential electricity demand using techniques from statistical science, data analysis and econometrics. Residential energy intensity is investigated as a critical component of demand and evaluated as a predictor of ",105,53,0.824819772873285,9,0.822341038121117
664,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.792356133460999,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Winning the Big Data Technologies Horizon Prize: Fast and reliable forecasting of electricity grid traffic by identification of recurrent fluctuationsThis paper provides a description of the approach and methodology I used in winning the European Union Big Data Technologies Horizon Prize on data-driven prediction of electricity grid traffic. The methodology relies on identifying typical short-term recurrent fluctuations ",105,53,0.824819772873285,9,0.822341038121117
670,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.832972943782806,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Revealing Household Characteristics from Electricity Meter Data with Grade Analysis and Machine Learning AlgorithmsIn this article, the Grade Correspondence Analysis (GCA) with posterior clustering and visualization is introduced and applied to extract important features to reveal households' characteristics based on electricity usage data. The main goal of the analysis is to ",105,53,0.824819772873285,9,0.822341038121117
688,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.832603394985199,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Design of big data analytics electricity collecting data analysis and intelligent monitoring systemsIn order to strengthen the power of users better characteristics, measurement device and distribution network equipment condition monitoring and analysis, based on electricity acquisition data analysis and intelligent monitoring system based on the integration of ",105,53,0.824819772873285,9,0.822341038121117
699,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.811464607715607,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Visualization as a Mean of Big Data Management: Using Qatar's Electricity Consumption DataVisualization as a mean of big data management is the new century revolution. Managing data has become a great challenge today, as the amount of raw data size is increasing rapidly. For data like electricity consumption, a new data value is received every minute from ",105,53,0.824819772873285,9,0.822341038121117
705,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.830939054489136,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] Artificial Intelligence and Nord Pool's intraday electricity market Elbas: a demonstration and pragmatic evaluation of employing deep learning for price prediction This thesis demonstrates the use of deep learning for automating hourly price forecasts in continuous intraday electricity markets, using various types of neural networks on comprehensive sequential market data and cutting-edge image processing networks on ",105,53,0.824819772873285,9,0.822341038121117
723,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.83129745721817,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","A Big Data Analytics Model for Household Electricity Consumption Tracking and MonitoringThe abundance of data nowadays can offer infinite opportunities and possibilities if being systematically explored. Exploration of the data can be achieved through the application of big data analytics (BDA). Consequently, a number of BDA models are seen developed in a ",105,53,0.824819772873285,9,0.822341038121117
728,"Electricity, gas, steam and air conditioning supply","A Network Tour of Data Science",0.827556431293488,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","IArchitecture based on multivariate big data platform for analyzing electricity consumption behavior,""With the development of smart grid, more and more measuring devices extend to bottom layer. The development of advanced measurement system and distribution network inevitably leads to the geometric increase of user data. On the other hand, the power grid is ",105,53,0.824819772873285,9,0.822341038121117
757,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.801539540290833,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","How Do the Global Stock Markets Influence One Another? Evidence from Finance Big Data and Granger Causality Directed NetworkThe recent financial network analysis approach reveals that the topologies of financial markets have an important influence on market dynamics. However, the majority of existing Finance Big Data networks are built as undirected networks without information on the ",135,67,0.828709795403836,9,0.818848106596205
764,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.784866034984589,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big Data Finance and Financial MarketsFinancial markets are always the most aggressive adopters of new information technologies. The recent boom in big data has enhanced the effect of information diffusion in financial markets since the physical cost of participation has been reduced and interactions among ",135,67,0.828709795403836,9,0.818848106596205
766,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.828255653381348,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","QuantCloud: big data infrastructure for quantitative finance on the cloudIn this paper, we present the QuantCloud infrastructure, designed for performing big data analytics in modern quantitative finance. Through analyzing market observations, quantitative finance (QF) utilizes mathematical models to search for subtle patterns and ",135,67,0.828709795403836,9,0.818848106596205
775,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.859416842460632,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big Data Analytics and Visualization: FinanceAll finance institutions have seen an explosion in their velocity, variety and volume of their internal 
datasets. New federal regulations requirement require leveraging internal and external data 
linking: [1] Customer service and transactional level data; [2] Social Media activity analysis (Sentimental ",135,67,0.828709795403836,9,0.818848106596205
784,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.821199297904968,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Signal Processing for Finance, Economics, and Marketing: Concepts, framework, and big data applicationsEconomic data and financial markets are intriguing to researchers working on data and quantitative models. With rapid growth of and increasing access to data in digital form, finance, economics, and marketing data are poised to become one of the most important ",135,67,0.828709795403836,9,0.818848106596205
804,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.784073352813721,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Compressed Sensing and its Applications in Risk Assessment for Internet Supply Chain Finance under Big DataPlenty of research focuses on supply chain finance and its risk, qualitatively or quantitatively. However, only a little literature studies on internet supply chain finance (ISCF), especially on its risk by quantitative analysis. After analyzing the information of partners' panorama data ",135,67,0.828709795403836,9,0.818848106596205
821,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.872052073478699,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big Data: Epistemological Reflections and Impacts in Finance and Capital Market Studies.Objective and method: Access to data series plays a central role in the area of Finance. The increasing availability of large volumes of data, in different formats and at high frequency, combined with the technological advances in data storage and processing tools, have ",135,67,0.828709795403836,9,0.818848106596205
840,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.819072723388672,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Reflection on Big Data Technology: Problems and Countermeasures in"" Big Data Credit Reporting"" of Internet Finance in ChinaWith the rapid development of Internet finance in China for the past few years, big data credit reporting agencies specifically for network credit information have been initially established. An analysis on the technology application, characteristics and operational difficulties of big  ",135,67,0.828709795403836,9,0.818848106596205
859,"Financial service activities, except insurance and pension funding","Data Analystics for Smart Grids",0.799157440662384,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Survey of Big Data applications: health, education, business & finance, and security & privacyNowadays, Big Data is experiencing an exponential growth in all domains of life. The total amount of data created in the world from the beginning of time up until 2005 is now created every 48 hours! Big Data represents large datasets that cannot be analyzed using traditional ",135,67,0.828709795403836,9,0.818848106596205
878,"Public administration and defence, compulsory social security","Information Retrieval",0.789689004421234,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","To do more, better, faster and more cheaply: Using big data in public administrationBig data have become a game-changer for modern public administration in those areas in which they are used. Although their application is still limited in the public sector, their use develops dynamically in areas where they bring tangible results in terms of efficiency and ",110,48,0.813798369218906,9,0.810743027263217
884,"Public administration and defence, compulsory social security","Information Retrieval",0.788117468357086,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","The application of artificial intelligence in public administration for forecasting high crime risk transportation areas in urban environmentPublic administration has adopted information and communication technology in order to construct new intelligent systems and design new risk prevention strategies in transportation management. The ultimate goal is to improve the quality of the transportation services and ",110,48,0.813798369218906,9,0.810743027263217
890,"Public administration and defence, compulsory social security","Information Retrieval",0.847516596317291,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","An information system for judicial and public administration using artificial intelligence and geospatial dataThe adoption of information technology in judicial and public administration has become a major need nowadays with the rapid growth of information regarding managerial issues. This paper presents an advanced methodology developed by using Information and ",110,48,0.813798369218906,9,0.810743027263217
895,"Public administration and defence, compulsory social security","Information Retrieval",0.853106617927551,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","A Study on Conversational Public Administration Service of the Chatbot Based on Artificial IntelligenceArtificial intelligence-based services are expanding into a new industrial revolution. There is artificial intelligence technology applied in real life due to the development of big data and deep learning related technology. And data analysis and intelligent assistant services that ",110,48,0.813798369218906,9,0.810743027263217
905,"Public administration and defence, compulsory social security","Information Retrieval",0.798630118370056,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Digital and Intelligent Public Administration: transformations in the era of artificial intelligenceThis article addresses the impact of the digital era and it specifically refers to information and communication technologies (ICT) in Public Administration. It is based on the international approach and underscores the importance of incorporating new technologies established by ",110,48,0.813798369218906,9,0.810743027263217
923,"Public administration and defence, compulsory social security","Information Retrieval",0.78792542219162,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Security and privacy of big data for social networking services in cloudBig Data (BD) is of great importance especially in wireless telecommunications field. Social Networking (SNg) is one more fast-growing technology that allows users to build their profile and could be described as web applications. Both of them face privacy and security issues ",110,48,0.813798369218906,9,0.810743027263217
945,"Public administration and defence, compulsory social security","Information Retrieval",0.812365353107452,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Artificial Intelligence Within the Military Domain and Cyber WarfareThe potential uses of machine learning and artificial intelligence in the cyber security domain have had a recent surge of interest. Much of the research and discussions in this area primarily focuses on reactive uses of the technology such as enhancing capabilities in ",110,48,0.813798369218906,9,0.810743027263217
954,"Public administration and defence, compulsory social security","Information Retrieval",0.802154958248138,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Application of Big Data Technology in Scientific Research Data Management of Military EnterprisesScientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology ",110,48,0.813798369218906,9,0.810743027263217
968,"Public administration and defence, compulsory social security","Information Retrieval",0.817181706428528,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Research on Military Logistics based on Big DataThis document gives formatting instructions for authors preparing papers for publication. With the arrival of the era of big data, the mature use of cloud computing and data mining technology, the big data mode have been widely applied in logistics. For military logistics, a ",110,48,0.813798369218906,9,0.810743027263217
13,"Legal and accounting activities","Big Data",0.804582178592682,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Influence of Artificial Intelligence on Activities and Competitiveness of an Organization view and edit this information), but also about the compliance with legal regulations (eg  Within
this group 85% of managers believe in the potential of artificial intelligence, 25% implement it 
The category of solutions integrating data from various sources (eg Twillio) and platforms  
",80,99,0.823880515315316,8,0.813418611884117
16,"Legal and accounting activities","Big Data",0.83898538351059,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Zero-Quartile Benchmarking How To Prioritize Digital Technologies In A Companys Transformation Journey Cognitive capabilities are used by applying artificial intelligence (AI) such as machine learningor natural language  It should help to handle the growing amount of data (Agarwal and Dhar 2014)and goes beyond traditional  descriptive  business intelligence (BI), because  
",80,99,0.823880515315316,8,0.813418611884117
20,"Legal and accounting activities","Big Data",0.804403066635132,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Toward blockchain-based accounting and assurance of an ecosystem of emerging technologies that includes artificial intelligence, the Internet of  rules
encoded could effectively control the recording of accounting activities and, therefore, provide intelligence to the accounting process by integrating Big Data and predictive analysis  
",80,99,0.823880515315316,8,0.813418611884117
34,"Legal and accounting activities","Big Data",0.788515448570251,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Bits and bolts Real estate 68 Low Low Legal and accounting activities, etc  online renting of data or computing
capacities), as well as back and front office integration systems such as customer relationship
management (CRM), and enterprise resource planning (ERP) software. Page 14  
",80,99,0.823880515315316,8,0.813418611884117
41,"Legal and accounting activities","Big Data",0.829229593276978,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Semantic Technologies for Business Decision Support processed and incorporated into the company structure, with the support of linguistic rules on
the intelligence phase  development, as it takes advantage of other disciplines as MachineLearning, Artificial Intelligence and Cognitive Science  It relies on data stored in a database  
",80,99,0.823880515315316,8,0.813418611884117
61,"Legal and accounting activities","Big Data",0.808561861515045,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","The compositional nature of productivity and innovation slowdown Indeed, our task is to understand if the 'data generating process' behind the productivity slowdown 
the socalled within, between, and covariance (or crosslevel) components (or effects  the following:
at the firm level, the within effect is interpreted as learning/innovation (change  
",80,99,0.823880515315316,8,0.813418611884117
72,"Legal and accounting activities","Big Data",0.808049917221069,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[BOOK][B] Decentralized Computing Using Blockchain Technologies and Smart Contracts: Emerging Research and Opportunities: Emerging Research and  With the advent of intelligent systems and self-learning machines, every device require freedom
of  chain, which is computationally very expensive and this makes the blockchain an immutabledata store  can be either an individual or a group of miners or a machine powered with  
",80,99,0.823880515315316,8,0.813418611884117
79,"Legal and accounting activities","Big Data",0.82502144575119,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Software robot-based automation of financial administration's processes Task type Support for human labourers Most advanced method Configuration (set rules/learning)
Analysing numbers Business Intelligence, data visualisation  The artificial intelligence is defined
as intelligent behaviour of machines (Ertel 2011, 1; Hutter & Legg 2007, 405)  
",80,99,0.823880515315316,8,0.813418611884117
421,"Agriculture, forestry and fishing","Data Analysis and Integration",0.866719245910645,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","A review on the practice of big data analysis in agriculture to the other research areas employing big data analysis, agriculture ranks at  stations, humans
as sensors, web-based data, GIS geospatial data, feeds from  web services, mobile applications,
statistical analysis, modeling, simulation, benchmarking, big data storage, message  
",180,20,0.825102084875107,8,0.838678166270256
439,"Agriculture, forestry and fishing","Data Analysis and Integration",0.79263824224472,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","IoT based agriculture as a cloud and big data service: the beginning of digital India Agriculture System Mechanism QoS-aware (Parameter) Domains Data Classification Resource
Management Big Data  thedifferent classlabelsofusers.K-NNissupervisedmachinelearning
techniquewhich  Thefinalstepistointerprettheagriculturedatasubmittedbydifferentusersof  
",180,20,0.825102084875107,8,0.838678166270256
443,"Agriculture, forestry and fishing","Data Analysis and Integration",0.856197774410248,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","IoT, big data science & analytics, cloud computing and mobile app based hybrid system for smart agriculture Aadhar linked agricultural information network can be easily conceptualized using mobile
communication and big data analytics operating on geo-spatial data already available with Ministry
of agriculture, ISRO, Survey of India to optimize resource availability, spread and  
",180,20,0.825102084875107,8,0.838678166270256
454,"Agriculture, forestry and fishing","Data Analysis and Integration",0.833492696285248,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","[PDF][PDF] Effective use of Big Data Analytics in Crop planning to increase Agriculture Production in India use of multi-sensor data such as satellites, IoT, and drones, and artificial intelligence algorithms
to  that collect data from internet and store it to an open Agriculture database with  The data clusteringalgorithm will run inside of Hadoop platform that offers parallel processing and  
",180,20,0.825102084875107,8,0.838678166270256
527,"Agriculture, forestry and fishing","Data Analysis and Integration",0.836910903453827,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","[PDF][PDF] A review on big data analytics in the field of agriculture Processing OLTP Analytical Big data Processing  We plan to work on precisionagriculture techniques. DISTRIBUTED NOSQL DATABASE FILE SYSTEM
PROGRAMMING COLUMN-DATA MODEL DOCUMENT-DATA MODEL  
",180,20,0.825102084875107,8,0.838678166270256
530,"Agriculture, forestry and fishing","Data Analysis and Integration",0.839772462844849,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Research on the Multi-agent Synergic Mechanism for the Opening and Sharing of Big Data in Chinese Agriculture The American agricultural big data system has the characteristics of taking the official data of
the ministry of agriculture as the core and rich data content [6] [7]. In order to integrate public data
of member states, the EU has built a normalized and standardized data sharing  
",180,20,0.825102084875107,8,0.838678166270256
558,"Agriculture, forestry and fishing","Data Analysis and Integration",0.815074503421783,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Productivity improvement in agriculture sector using big data tools Data of various benefits and polices provided by government from ministry of agriculture  second
part we implement prediction function for establish forecast data through kYmeans  2017
International Conference On Big Data Analytics and computational Intelligence (ICBDACI)  
",180,20,0.825102084875107,8,0.838678166270256
595,"Agriculture, forestry and fishing","Data Analysis and Integration",0.868619501590729,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. "," for Big Data scholarship in the realm of food and agriculture  Data inconsistency: Data is misplaced
during capturing and filing hence the information is prone to errors  Big data analytics framework
for agricultural services system is a solution that enables farmers  
[CITATION][C] Big Data: Managing the Future's Agriculture and Natural Resource Systems",180,20,0.825102084875107,8,0.838678166270256
635,"Electricity, gas, steam and air conditioning supply","Bid Data Measuring Systems",0.853884398937225,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Big data analytics for discovering electricity consumption patterns in smart citiesNew technologies such as sensor networks have been incorporated into the management of buildings for organizations and cities. Sensor networks have led to an exponential increase in the volume of data available in recent years, which can be used to extract consumption ",105,24,0.826420515775681,8,0.83820053935051
658,"Electricity, gas, steam and air conditioning supply","Lab in data science",0.801314830780029,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Exploring big data for development: An electricity sector case study from IndiaThis paper presents exploratory research into data-intensive development that seeks to inductively identify issues and conceptual frameworks of relevance to big data in developing countries. It presents a case study of big data innovations in Stelcorp; a state electricity  ",105,78,0.825876659307724,8,0.817321442067623
663,"Electricity, gas, steam and air conditioning supply","Lab in data science",0.794911921024323,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Winning the Big Data Technologies Horizon Prize: Fast and reliable forecasting of electricity grid traffic by identification of recurrent fluctuationsThis paper provides a description of the approach and methodology I used in winning the European Union Big Data Technologies Horizon Prize on data-driven prediction of electricity grid traffic. The methodology relies on identifying typical short-term recurrent fluctuations ",105,78,0.825876659307724,8,0.817321442067623
669,"Electricity, gas, steam and air conditioning supply","Bid Data Measuring Systems",0.829882740974426,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Realization and Research of Intelligent system of client electricity information based on the Big Data Processing TechnologyBig data is the focus in power system currently. In order to analyze and classify the electricity model of clients, identify the avoiding peak space intelligently, extract value-added information of clients and control the electric load actively, it is extremely necessary to ",105,24,0.826420515775681,8,0.83820053935051
684,"Electricity, gas, steam and air conditioning supply","Lab in data science",0.803814888000488,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Sliding Time Window Electricity Consumption Optimization Algorithm for Communities in the Context of Big Data ProcessingBig data frameworks enable companies from various fields to build models that allow them to increase profit margins by improving decision making at different levels (middle management, senior management, and board) or by attempting to boost sales by ",105,78,0.825876659307724,8,0.817321442067623
686,"Electricity, gas, steam and air conditioning supply","Bid Data Measuring Systems",0.879390239715576,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Design of big data analytics electricity collecting data analysis and intelligent monitoring systemsIn order to strengthen the power of users better characteristics, measurement device and distribution network equipment condition monitoring and analysis, based on electricity acquisition data analysis and intelligent monitoring system based on the integration of ",105,24,0.826420515775681,8,0.83820053935051
693,"Electricity, gas, steam and air conditioning supply","Bid Data Measuring Systems",0.851951718330383,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","The Design of Distributed Power Big Data Analysis Framework and Its Application in Residential Electricity AnalysisWith the development of digital, information and intelligent process of power system, more and more data sources appear. The traditional standalone environment has been difficult to adapt to the need of the analysis of massive data. The power industry also needs to use real ",105,24,0.826420515775681,8,0.83820053935051
694,"Electricity, gas, steam and air conditioning supply","Lab in data science",0.842125833034515,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","The Design of Distributed Power Big Data Analysis Framework and Its Application in Residential Electricity AnalysisWith the development of digital, information and intelligent process of power system, more and more data sources appear. The traditional standalone environment has been difficult to adapt to the need of the analysis of massive data. The power industry also needs to use real ",105,78,0.825876659307724,8,0.817321442067623
698,"Electricity, gas, steam and air conditioning supply","Lab in data science",0.822125971317291,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Visualization as a Mean of Big Data Management: Using Qatar's Electricity Consumption DataVisualization as a mean of big data management is the new century revolution. Managing data has become a great challenge today, as the amount of raw data size is increasing rapidly. For data like electricity consumption, a new data value is received every minute from ",105,78,0.825876659307724,8,0.817321442067623
701,"Electricity, gas, steam and air conditioning supply","Lab in data science",0.830215632915497,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big Data Analytics for Electricity Price ForecastElectricity Price forecast is a major task in smart grid operation. There is a massive amount of data flowing in the power system including the data collection by control systems, sensors, etc. In addition, there are many data points which are not captured and processed by the ",105,78,0.825876659307724,8,0.817321442067623
702,"Electricity, gas, steam and air conditioning supply","Bid Data Measuring Systems",0.822702765464783,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Big Data Analytics for Electricity Price ForecastElectricity Price forecast is a major task in smart grid operation. There is a massive amount of data flowing in the power system including the data collection by control systems, sensors, etc. In addition, there are many data points which are not captured and processed by the ",105,24,0.826420515775681,8,0.83820053935051
710,"Electricity, gas, steam and air conditioning supply","Bid Data Measuring Systems",0.799430727958679,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Development of Automation Algorithm for Step of Designing Technology of Static Electricity Protection ClothingThe article presents the research results, which aim at providing an automation algorithm for the step of the designing technology of static electricity protection clothing. Designing the protective clothing is intended to the creation of such a structure, the properties of which ",105,24,0.826420515775681,8,0.83820053935051
716,"Electricity, gas, steam and air conditioning supply","Bid Data Measuring Systems",0.826710939407349,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","[BOOK][B] Control and Automation Systems for Electricity Distribution Networks (EDN) of the futureThe CIGRÉ C6 Study Committee (Distribution Systems and Dispersed Generation) considers the different aspects of integration of distributed generation. In this context, the JWG C6. 25/B5 has worked to map current functionalities and to identify future needs for the ",105,24,0.826420515775681,8,0.83820053935051
724,"Electricity, gas, steam and air conditioning supply","Lab in data science",0.827997386455536,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","A Big Data Analytics Model for Household Electricity Consumption Tracking and MonitoringThe abundance of data nowadays can offer infinite opportunities and possibilities if being systematically explored. Exploration of the data can be achieved through the application of big data analytics (BDA). Consequently, a number of BDA models are seen developed in a ",105,78,0.825876659307724,8,0.817321442067623
726,"Electricity, gas, steam and air conditioning supply","Bid Data Measuring Systems",0.841650784015656,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","IArchitecture based on multivariate big data platform for analyzing electricity consumption behavior,""With the development of smart grid, more and more measuring devices extend to bottom layer. The development of advanced measurement system and distribution network inevitably leads to the geometric increase of user data. On the other hand, the power grid is ",105,24,0.826420515775681,8,0.83820053935051
730,"Electricity, gas, steam and air conditioning supply","Lab in data science",0.816065073013306,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report."," In many developed countries, such as America, England, Japan and so on those used the
technology of Artificial Intelligence (AI), such as Neural Network, Data Mining, Machine Learning
and so on to apply to use in electricity energy forecasting for gain the best performance  
Study of electricity load forecasting based on multiple kernels learning and weighted support vector regression machine",105,78,0.825876659307724,8,0.817321442067623
755,"Financial service activities, except insurance and pension funding","A Network Tour of Data Science",0.816608309745789,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","How Do the Global Stock Markets Influence One Another? Evidence from Finance Big Data and Granger Causality Directed NetworkThe recent financial network analysis approach reveals that the topologies of financial markets have an important influence on market dynamics. However, the majority of existing Finance Big Data networks are built as undirected networks without information on the ",135,53,0.824819772873285,8,0.818801701068878
762,"Financial service activities, except insurance and pension funding","A Network Tour of Data Science",0.790284037590027,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Big Data Finance and Financial MarketsFinancial markets are always the most aggressive adopters of new information technologies. The recent boom in big data has enhanced the effect of information diffusion in financial markets since the physical cost of participation has been reduced and interactions among ",135,53,0.824819772873285,8,0.818801701068878
765,"Financial service activities, except insurance and pension funding","A Network Tour of Data Science",0.836006164550781,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","QuantCloud: big data infrastructure for quantitative finance on the cloudIn this paper, we present the QuantCloud infrastructure, designed for performing big data analytics in modern quantitative finance. Through analyzing market observations, quantitative finance (QF) utilizes mathematical models to search for subtle patterns and ",135,53,0.824819772873285,8,0.818801701068878
778,"Financial service activities, except insurance and pension funding","A Network Tour of Data Science",0.829204082489014,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Big Data Analytics and Visualization: FinanceAll finance institutions have seen an explosion in their velocity, variety and volume of their internal 
datasets. New federal regulations requirement require leveraging internal and external data 
linking: [1] Customer service and transactional level data; [2] Social Media activity analysis (Sentimental ",135,53,0.824819772873285,8,0.818801701068878
781,"Financial service activities, except insurance and pension funding","A Network Tour of Data Science",0.826008439064026,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Signal Processing for Finance, Economics, and Marketing: Concepts, framework, and big data applicationsEconomic data and financial markets are intriguing to researchers working on data and quantitative models. With rapid growth of and increasing access to data in digital form, finance, economics, and marketing data are poised to become one of the most important ",135,53,0.824819772873285,8,0.818801701068878
785,"Financial service activities, except insurance and pension funding","A Network Tour of Data Science",0.836568713188171,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Quantcloud: Enabling big data complex event processing for quantitative finance through a data-driven executionQuantitative Finance (QF) utilizes increasingly sophisticated mathematic models and advanced computer techniques to predict the movement of global markets and price the derivatives. Today, the rise of QF requires an integrated toolchain of enabling technologies ",135,53,0.824819772873285,8,0.818801701068878
800,"Financial service activities, except insurance and pension funding","A Network Tour of Data Science",0.8038050532341,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Compressed Sensing and its Applications in Risk Assessment for Internet Supply Chain Finance under Big DataPlenty of research focuses on supply chain finance and its risk, qualitatively or quantitatively. However, only a little literature studies on internet supply chain finance (ISCF), especially on its risk by quantitative analysis. After analyzing the information of partners' panorama data ",135,53,0.824819772873285,8,0.818801701068878
858,"Financial service activities, except insurance and pension funding","A Network Tour of Data Science",0.811928808689117,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Survey of Big Data applications: health, education, business & finance, and security & privacyNowadays, Big Data is experiencing an exponential growth in all domains of life. The total amount of data created in the world from the beginning of time up until 2005 is now created every 48 hours! Big Data represents large datasets that cannot be analyzed using traditional ",135,53,0.824819772873285,8,0.818801701068878
0,"Legal and accounting activities","Data Analystics for Smart Grids",0.829418122768402,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Business intelligence & analytics in management accounting research: Status and future focus that although there is potential for studying business intelligence solutions in  is linked to other
emerging technologies such as big data, machine learning and the Internet  Judgment and
decision-making, Databases, and Expert systems, artificial intelligence and decision-aids  
",80,67,0.828709795403836,7,0.819192273276193
8,"Legal and accounting activities","Data Analystics for Smart Grids",0.828594744205475,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[PDF][PDF] Bankruptcy risk prediction models based on artificial neural networks As part of the Artificial Intelligence, the neural networks are systems that use approximation
methods based  31 December 2015 in Romania there were a number of 773781 active legal
entities  978-0-9742114-1-1. Walczak, S.(2001)An empirical analysis of data requirements for  
",80,67,0.828709795403836,7,0.819192273276193
12,"Legal and accounting activities","Applied data analysis",0.807732343673706,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Influence of Artificial Intelligence on Activities and Competitiveness of an Organization view and edit this information), but also about the compliance with legal regulations (eg  Within
this group 85% of managers believe in the potential of artificial intelligence, 25% implement it 
The category of solutions integrating data from various sources (eg Twillio) and platforms  
",80,120,0.829422693451246,7,0.810311675071716
21,"Legal and accounting activities","Applied data analysis",0.799693405628204,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Toward blockchain-based accounting and assurance of an ecosystem of emerging technologies that includes artificial intelligence, the Internet of  rules
encoded could effectively control the recording of accounting activities and, therefore, provide intelligence to the accounting process by integrating Big Data and predictive analysis  
",80,120,0.829422693451246,7,0.810311675071716
24,"Legal and accounting activities","Data Analystics for Smart Grids",0.793908834457397,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Toward blockchain-based accounting and assurance of an ecosystem of emerging technologies that includes artificial intelligence, the Internet of  rules
encoded could effectively control the recording of accounting activities and, therefore, provide intelligence to the accounting process by integrating Big Data and predictive analysis  
",80,67,0.828709795403836,7,0.819192273276193
25,"Legal and accounting activities","Applied data analysis",0.824421584606171,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Big data in accounting Ernst &Young Limited (EY) is the third case study. Its scope of accounting activities includes:
advisory, assurance and tax services and as a result it demonstrates great scientific interest  BigData can be very  visual data analytics (Gepp et. al, 2018). Another suggestion is meta  
",80,120,0.829422693451246,7,0.810311675071716
29,"Legal and accounting activities","Data Analystics for Smart Grids",0.79653412103653,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Big data in accounting Ernst &Young Limited (EY) is the third case study. Its scope of accounting activities includes:
advisory, assurance and tax services and as a result it demonstrates great scientific interest  BigData can be very  visual data analytics (Gepp et. al, 2018). Another suggestion is meta  
",80,67,0.828709795403836,7,0.819192273276193
36,"Legal and accounting activities","Data Analystics for Smart Grids",0.829451978206635,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","[BOOK][B] The Routledge Companion to Accounting Information Systems interests include continuous auditing and monitoring, audit data analytics and artificial intelligence
in auditing  process management for AIS in the context of new legal requirements for  constituting
different decision-making environments and what they call data environments  
",80,67,0.828709795403836,7,0.819192273276193
38,"Legal and accounting activities","Applied data analysis",0.815936923027039,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[BOOK][B] The Routledge Companion to Accounting Information Systems interests include continuous auditing and monitoring, audit data analytics and artificial intelligence
in auditing  process management for AIS in the context of new legal requirements for  constituting
different decision-making environments and what they call data environments  
",80,120,0.829422693451246,7,0.810311675071716
54,"Legal and accounting activities","Applied data analysis",0.786774098873138,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Tech human resources: a study on the impact of digital technologies on international HR startups The role of technology is spreading in all processes which composed Human Resource function:
form Recruiting to Learning and  resource tech industry are Social, Mobile, Artificial Intelligence,Big Data and Analytics and Cloud  oriented or effectiveness oriented  
",80,120,0.829422693451246,7,0.810311675071716
66,"Legal and accounting activities","Data Analystics for Smart Grids",0.819998621940613,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Measuring Performance of Adaptive Supply Chains The data source for such a system is the reporting of individual units under accounting activities 
does not solely result from the will to satisfy specific customer requirements, but also the necessity
to adjust products to the legal requirements of  5.2 Sample and Data Collection  
",80,67,0.828709795403836,7,0.819192273276193
68,"Legal and accounting activities","Applied data analysis",0.807090044021606,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Measuring Performance of Adaptive Supply Chains The data source for such a system is the reporting of individual units under accounting activities 
does not solely result from the will to satisfy specific customer requirements, but also the necessity
to adjust products to the legal requirements of  5.2 Sample and Data Collection  
",80,120,0.829422693451246,7,0.810311675071716
75,"Legal and accounting activities","Data Analystics for Smart Grids",0.836439490318298,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Software robot-based automation of financial administration's processes Task type Support for human labourers Most advanced method Configuration (set rules/learning)
Analysing numbers Business Intelligence, data visualisation  The artificial intelligence is defined
as intelligent behaviour of machines (Ertel 2011, 1; Hutter & Legg 2007, 405)  
",80,67,0.828709795403836,7,0.819192273276193
76,"Legal and accounting activities","Applied data analysis",0.83053332567215,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Software robot-based automation of financial administration's processes Task type Support for human labourers Most advanced method Configuration (set rules/learning)
Analysing numbers Business Intelligence, data visualisation  The artificial intelligence is defined
as intelligent behaviour of machines (Ertel 2011, 1; Hutter & Legg 2007, 405)  
",80,120,0.829422693451246,7,0.810311675071716
95,"Accomodation and food service activities","Machine Learning",0.857511401176453,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Alternative data mining/machine learning methods for the analytical evaluation of food quality and authenticityA reviewIn recent years, the variety and volume of data acquired by modern analytical instruments in order to conduct a better authentication of food has dramatically increased. Several pattern recognition tools have been developed to deal with the large volume and complexity of ",145,79,0.825409763975988,7,0.832977337496621
97,"Accomodation and food service activities","A Network Tour of Data Science",0.850186169147491,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Alternative data mining/machine learning methods for the analytical evaluation of food quality and authenticityA reviewIn recent years, the variety and volume of data acquired by modern analytical instruments in order to conduct a better authentication of food has dramatically increased. Several pattern recognition tools have been developed to deal with the large volume and complexity of ",145,53,0.824819772873285,7,0.817785084247589
99,"Accomodation and food service activities","Machine Learning",0.84393036365509,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Alternative data mining/machine learning methods for the analytical evaluation of food quality and authenticityA reviewIn recent years, the variety and volume of data acquired by modern analytical instruments in order to conduct a better authentication of food has dramatically increased. Several pattern recognition tools have been developed to deal with the large volume and complexity of ",145,79,0.825409763975988,7,0.832977337496621
110,"Accomodation and food service activities","Machine Learning",0.790869116783142,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Satellite data and machine learning for weather risk management and food securityThe increase in frequency and severity of extreme weather events poses challenges for the agricultural sector in developing economies and for food security globally. In this article, we demonstrate how machine learning can be used to mine satellite data and identify pixel ",145,79,0.825409763975988,7,0.832977337496621
121,"Accomodation and food service activities","A Network Tour of Data Science",0.831008315086365,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning Taking advantage of a geo-big data approach and multilevel regression model, we make a
contribution to current literature by  should offer deeper spatial insights into intra-urban foodscape
and provide more nuanced understanding of food deserts  2. Methodology and data. 2.1  
",145,53,0.824819772873285,7,0.817785084247589
125,"Accomodation and food service activities","A Network Tour of Data Science",0.822258412837982,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Nutritional Culturomics and Big Data: Macroscopic Patterns of Change in Food, Nutrition and Diet Choices and diet, their knowledge, awareness and understanding of the interdepend- ence of food
consumption, health  [15]), and mathematical tools to handle complex data sets (eg  application
of Artificial Intelligence for large-scale content analysis [18, 19] or random fractal theory to  
",145,53,0.824819772873285,7,0.817785084247589
144,"Accomodation and food service activities","A Network Tour of Data Science",0.780157506465912,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] Can business generated big food data be used to understand food consumption behaviour and can a research infrastructure be generated around such data  platforms, (b) stakeholders along the food chain, and (c) policy actors in the agricultural-food
and nutrition-health  behaviour can be extracted from existing business generated data and on
which conditions these data might feed into a future RICHFIELDS big data platform  
",145,53,0.824819772873285,7,0.817785084247589
165,"Accomodation and food service activities","Machine Learning",0.846430599689484,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Discussion on the Reform of Teaching Methods for Specialized Courses under the Background of Big Data-Taking Animal Food Technology as an Example E. Make full use of big data, help better feedback students' learning situation, and establish more 
feedback for the leaning situation of students using ""Xuexitong"" on the Animal Food Technology
Mooc  These detailed and specific data to be obtained, on the one hand, can help  
",145,79,0.825409763975988,7,0.832977337496621
184,"Accomodation and food service activities","Machine Learning",0.800331652164459,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector of currently available studies on emerging technologies in agriculture and food sector (A&F  The
opportunities of the new big-data-augmented methodology are shown in comparison to existing 
with special attention to use of bigger volumes of data, machine learning and ontology  
",145,79,0.825409763975988,7,0.832977337496621
193,"Accomodation and food service activities","A Network Tour of Data Science",0.799285590648651,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] Big Data and Opportunities for Agriculture and Food Industry make decisions that will increase yields and deliver safe, nutritious food to communities  Predictive
models developed using Big Data identify best management practices for achieving the  machinelearning algorithms and rooted in comprehensive and reliable data- sets, provide  
",145,53,0.824819772873285,7,0.817785084247589
199,"Accomodation and food service activities","A Network Tour of Data Science",0.839459359645844,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Food trend based on social media for big data analysis using K-mean clustering and SAW: A case study on yogyakarta culinary industry 1. Fig. 1. Big Data Processing Pipeline  From the data that has been done cleaning the data, then
convert in a bag of words matrix to be applied machine learning algorithm. Then perform feature
extraction to find relevant features in classifying trendy food data  
",145,53,0.824819772873285,7,0.817785084247589
203,"Accomodation and food service activities","Machine Learning",0.816693305969238,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[CITATION][C] Big data applications in food safety and quality in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",145,79,0.825409763975988,7,0.832977337496621
206,"Accomodation and food service activities","A Network Tour of Data Science",0.802140235900879,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[CITATION][C]  -land Algorithm Based on Neighborhood and Temporal Anomalies (FANTA) to Map Planted Versus Fallowed Croplands Using MODIS Data to Assist in [CITATION][C] Identify how big data enables forecasting and demand planning in food and beverages industry",145,53,0.824819772873285,7,0.817785084247589
220,"Accomodation and food service activities","Machine Learning",0.875074923038483,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. "," presents exemplary applications of the smart manufacturing concept in food industry enterprises 
methods and solutions, in- cluding machine learning and artificial intelligence algorithms [Tao 
Traditional analyses use conventional algorithms and data that has been previ- ously  
[CITATION][C]  Demand Models in the Food and Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning ",145,79,0.825409763975988,7,0.832977337496621
428,"Agriculture, forestry and fishing","Information Retrieval",0.791641712188721,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Publicising food: big data, precision agriculture, and co-experimental techniques of addition A number of technological forms are thus investigated: eg, big data (big soil data, big climatedata, etc.), precision agriculture, and a variety of internet-based platforms utilised by
self-described activists and proponents of more local and regional based foodscapes  
",180,48,0.813798369218906,7,0.814693408352988
455,"Agriculture, forestry and fishing","Information Retrieval",0.813328623771667,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","New trends in precision agriculture: a novel cloud-based system for enabling data storage and agricultural task planning and automationIt is well-known that information and communication technologies enable many tasks in the context of precision agriculture. In fact, more and more farmers and food and agriculture companies are using precision agriculture-based systems to enhance not only their products ",180,48,0.813798369218906,7,0.814693408352988
467,"Agriculture, forestry and fishing","Information Retrieval",0.79967212677002,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","[PDF][PDF] Machine learning and data mining advance predictive big data analysis in precision animal agriculturePrecision animal agriculture is poised to rise to prominence in the livestock enterprise in the domains of management, production, welfare, sustainability, health surveillance, and environmental footprint. Considerable progress has been made in the use of tools to ",180,48,0.813798369218906,7,0.814693408352988
501,"Agriculture, forestry and fishing","Information Retrieval",0.783823430538177,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Application of big data in precision agriculture.Precision agriculture is the future way for agricultural modernization. The rapid expansion of
agricultural data and the development of big data technology provide a new method for the
development of precision agriculture, and become an important force leading to the development  
",180,48,0.813798369218906,7,0.814693408352988
506,"Agriculture, forestry and fishing","Information Retrieval",0.820422649383545,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Machine learning based data processing and latency reduction in the internet of things for agricultureThe Internet of Things is best stated as a network of things that have the ability to generate and share information between themselves and interact with the environment according to the percepts from this environment. This network between these devices and humans ",180,48,0.813798369218906,7,0.814693408352988
545,"Agriculture, forestry and fishing","Information Retrieval",0.841536521911621,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector and shortcomings of currently available studies on emerging technologies in agriculture and
food  The opportunities of the new big-data-augmented methodology are shown in comparison
to  with special attention to use of bigger volumes of data, machine learning and ontology  
",180,48,0.813798369218906,7,0.814693408352988
563,"Agriculture, forestry and fishing","Information Retrieval",0.852428793907166,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Processing of Big Data in Internet of Things and Precision Agriculture.The main focus of the paper is the analysis of various types of agriculture data and open source
operational databases and platforms for data collection and data warehousing suitable for storing
data obtained from the Internet of Things and Precision Agriculture. The methodical approach  
",180,48,0.813798369218906,7,0.814693408352988
1016,"Real estate activities","Applied data analysis",0.812171101570129,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Automation of the technical due diligence with artificial intelligence in the real estate industryOver the real estate lifecycle numerous documents and data are generated. The majority of building-related data is collected in day-to-day operations, such as maintenance protocols, contracts or energy consumptions. Previous successes in the classification already help to ",60,120,0.829422693451246,7,0.837225360529763
1018,"Real estate activities","Big Data",0.797066152095795,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Automation of the technical due diligence with artificial intelligence in the real estate industryOver the real estate lifecycle numerous documents and data are generated. The majority of building-related data is collected in day-to-day operations, such as maintenance protocols, contracts or energy consumptions. Previous successes in the classification already help to ",60,99,0.823880515315316,7,0.823123565741948
1020,"Real estate activities","Applied data analysis",0.882924020290375,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[BOOK][B] Real Estate Analysis in the Information Age: Techniques for Big Data and Statistical ModelingThe creation, accumulation, and use of copious amounts of data are driving rapid change across a wide variety of industries and academic disciplines. This 'Big Data'phenomenon is the result of recent developments in computational technology and improved data gathering ",60,120,0.829422693451246,7,0.837225360529763
1023,"Real estate activities","Big Data",0.863697111606598,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[BOOK][B] Real Estate Analysis in the Information Age: Techniques for Big Data and Statistical ModelingThe creation, accumulation, and use of copious amounts of data are driving rapid change across a wide variety of industries and academic disciplines. This 'Big Data'phenomenon is the result of recent developments in computational technology and improved data gathering ",60,99,0.823880515315316,7,0.823123565741948
1029,"Real estate activities","Big Data",0.784008860588074,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Real estate bubble prediction based on big dataDisclosed herein are a computer apparatus, non-transitory computer readable medium, and method for predicting real estate bubbles based on big data analysis. Historical variable data associated with real estate assets are obtained from remote data sources. Portions of ",60,99,0.823880515315316,7,0.823123565741948
1035,"Real estate activities","Applied data analysis",0.888077676296234,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Urban data streams and machine learning: a case of swiss real estate marketIn this paper, we show how using publicly available data streams and machine learning algorithms one can develop practical data driven services with no input from domain experts as a form of prior knowledge. We report the initial steps toward development of a real estate  ",60,120,0.829422693451246,7,0.837225360529763
1037,"Real estate activities","Big Data",0.868341386318207,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Urban data streams and machine learning: a case of swiss real estate marketIn this paper, we show how using publicly available data streams and machine learning algorithms one can develop practical data driven services with no input from domain experts as a form of prior knowledge. We report the initial steps toward development of a real estate  ",60,99,0.823880515315316,7,0.823123565741948
1045,"Real estate activities","Applied data analysis",0.861330628395081,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","News-based sentiment analysis in real estate: A supervised machine learning approach with support vector networksWith the rapid growth of news, information and opinionated data available in digital form, accompanied by a swift progress of textual analysis techniques, the field of sentiment analysis became a hotspot in the area of natural language processing. Additionally ",60,120,0.829422693451246,7,0.837225360529763
1048,"Real estate activities","Big Data",0.833521246910095,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","News-based sentiment analysis in real estate: A supervised machine learning approach with support vector networksWith the rapid growth of news, information and opinionated data available in digital form, accompanied by a swift progress of textual analysis techniques, the field of sentiment analysis became a hotspot in the area of natural language processing. Additionally ",60,99,0.823880515315316,7,0.823123565741948
1057,"Real estate activities","Applied data analysis",0.818760454654694,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Broad application of artificial intelligence for document classification, information extraction and predictive analytics in real estateReal estate represents a major share of economic activities and wealth in all economies. Due to the lack of widely acknowledged standards, however, the structuring, providing and managing of a life cycle-comprehensive building documentation yet remain challenging ",60,120,0.829422693451246,7,0.837225360529763
1059,"Real estate activities","Big Data",0.814840137958527,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Broad application of artificial intelligence for document classification, information extraction and predictive analytics in real estateReal estate represents a major share of economic activities and wealth in all economies. Due to the lack of widely acknowledged standards, however, the structuring, providing and managing of a life cycle-comprehensive building documentation yet remain challenging ",60,99,0.823880515315316,7,0.823123565741948
1061,"Real estate activities","Big Data",0.800390064716339,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","[HTML][HTML] Urban Tech on the Rise: Machine Learning Disrupts the Real Estate Industry. Featuring interviews of: Marc Rutzen and Jasjeet Thind by Stanislas Chaillou The practice of AI-powered Urban Analytics is taking off within the real estate industry. Data science and algorithmic logic are close to the forefront of new urban development practices. How close? is the questionexperts predict that digitization will go far beyond intelligent ",60,99,0.823880515315316,7,0.823123565741948
1062,"Real estate activities","Applied data analysis",0.796018064022064,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","[HTML][HTML] Urban Tech on the Rise: Machine Learning Disrupts the Real Estate Industry. Featuring interviews of: Marc Rutzen and Jasjeet Thind by Stanislas Chaillou The practice of AI-powered Urban Analytics is taking off within the real estate industry. Data science and algorithmic logic are close to the forefront of new urban development practices. How close? is the questionexperts predict that digitization will go far beyond intelligent ",60,120,0.829422693451246,7,0.837225360529763
1073,"Real estate activities","Applied data analysis",0.801295578479767,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Comparing three machine learning algorithms in the task of appraising commercial real estateIn a unique opportunity to examine rare appraisal data from the commercial real estate sector, the accuracy of three machine learning algorithms is compared in the task of appraising commercial real estate. The algorithms; random forests, support vector ",60,120,0.829422693451246,7,0.837225360529763
2,"Legal and accounting activities","Machine Learning",0.818037569522858,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Business intelligence & analytics in management accounting research: Status and future focus that although there is potential for studying business intelligence solutions in  is linked to other
emerging technologies such as big data, machine learning and the Internet  Judgment and
decision-making, Databases, and Expert systems, artificial intelligence and decision-aids  
",80,79,0.825409763975988,6,0.810662031173706
14,"Legal and accounting activities","Machine Learning",0.801497280597687,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Influence of Artificial Intelligence on Activities and Competitiveness of an Organization view and edit this information), but also about the compliance with legal regulations (eg  Within
this group 85% of managers believe in the potential of artificial intelligence, 25% implement it 
The category of solutions integrating data from various sources (eg Twillio) and platforms  
",80,79,0.825409763975988,6,0.810662031173706
19,"Legal and accounting activities","Machine Learning",0.824621975421906,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Zero-Quartile Benchmarking How To Prioritize Digital Technologies In A Companys Transformation Journey Cognitive capabilities are used by applying artificial intelligence (AI) such as machine learningor natural language  It should help to handle the growing amount of data (Agarwal and Dhar 2014)and goes beyond traditional  descriptive  business intelligence (BI), because  
",80,79,0.825409763975988,6,0.810662031173706
44,"Legal and accounting activities","Machine Learning",0.817577540874481,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Semantic Technologies for Business Decision Support processed and incorporated into the company structure, with the support of linguistic rules on
the intelligence phase  development, as it takes advantage of other disciplines as MachineLearning, Artificial Intelligence and Cognitive Science  It relies on data stored in a database  
",80,79,0.825409763975988,6,0.810662031173706
53,"Legal and accounting activities","Machine Learning",0.796975910663605,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Tech human resources: a study on the impact of digital technologies on international HR startups The role of technology is spreading in all processes which composed Human Resource function:
form Recruiting to Learning and  resource tech industry are Social, Mobile, Artificial Intelligence,Big Data and Analytics and Cloud  oriented or effectiveness oriented  
",80,79,0.825409763975988,6,0.810662031173706
59,"Legal and accounting activities","Machine Learning",0.8052619099617,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Adapting business framework conditions to deal with disruptive technologies in Denmark Participation in life-long learning is high but decreasing  Second, the ability to combine new
advanced technologies (such as sensors, advanced robotics and 3D printing), new processes
(such as data-driven production and artificial intelligence) and new business models  
",80,79,0.825409763975988,6,0.810662031173706
112,"Accomodation and food service activities","Bid Data Measuring Systems",0.786377727985382,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Satellite data and machine learning for weather risk management and food securityThe increase in frequency and severity of extreme weather events poses challenges for the agricultural sector in developing economies and for food security globally. In this article, we demonstrate how machine learning can be used to mine satellite data and identify pixel ",145,24,0.826420515775681,6,0.822160005569458
139,"Accomodation and food service activities","Bid Data Measuring Systems",0.800877213478088,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Combining open data and machine learning to predict food security in EthiopiaFood security is commonly measured by means of surveys, requiring substantial time and budget. Open data can possibly serve as a cost-effective alternative to predict food security. In this paper a method is proposed that uses open data related to food insecurity drivers to ",145,24,0.826420515775681,6,0.822160005569458
155,"Accomodation and food service activities","Bid Data Measuring Systems",0.837855577468872,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Design and Realization of Food Safety Monitoring and Pre-Control System Based on Multi-Source and Big Data efficiency,poor timeliness,and incomplete data. In order to realize the sharing of resources and
information in the process of food safety monitoring,this study designed and developed a food
safety monitoring and control system based on the multi-source and big data under the  
",145,24,0.826420515775681,6,0.822160005569458
161,"Accomodation and food service activities","Bid Data Measuring Systems",0.842268466949463,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","[PDF][PDF] Recommender System Based Tensor Candecomp Parafact Algorithm-ALS to Handle Sparse Data In Food Commerce Information ServicesRecommender systems have been widely researched in many applications especially in e-commerce services with the aim to make clear and easy communication between consumer and provider. Simple examples of Recommender systems would include personal and ",145,24,0.826420515775681,6,0.822160005569458
170,"Accomodation and food service activities","Bid Data Measuring Systems",0.863789737224579,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Systems Approach to Link Big Socio-ecological Geo-data to Food Systems Sustainability of (i) what information commonly needed by food system actors to response and adapt to
socio-ecological change and enhance the system performance, (ii) interoperability between
different types of data across scales, and (iii) sufficient guidance to utilize big data resources  
",145,24,0.826420515775681,6,0.822160005569458
189,"Accomodation and food service activities","Bid Data Measuring Systems",0.801791310310364,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","A review on use of big data in warehousing to enhance accessibility of food and outreach across the agricultural value- chain is ensured by Big- Data methods and practices.
This information is spread across input providers and produce buyers. This paper will analyze
the lacunas in data accessibility which render the efficacy of adequate supply of food  
",145,24,0.826420515775681,6,0.822160005569458
227,"Human health and social work activities","Lab in data science",0.789335370063782,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[PDF][PDF] Where is technology taking the economy the Internet, the cloud, big data, robotics, machine learning, and now artificial intelligence
together powerful  And data can't easily be owned either, it can be garnered from nonproprietary 
will still have jobs, especially those like kindergarten teaching or social work that require  
",195,78,0.825876659307724,6,0.823953568935394
234,"Human health and social work activities","A Network Tour of Data Science",0.825062215328217,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Identifying child abuse through text mining and machine learning is related to work in the area of data exploration and supervised classification based  risk modeling
(PRM) tools coupled with data mining and machine-learning algorithms should  a linear prediction
model (45.2% sensitivity, 82.4% specificity) using administrative data from 716  
",195,53,0.824819772873285,6,0.848983814318975
249,"Human health and social work activities","A Network Tour of Data Science",0.838248372077942,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Predictive and prescriptive analytics, machine learning and child welfare risk assessment: The Broward County experience resonates well with both the American ethos of encouraging strong families and social work ideals
of  making and actuarial methods points to the need for computational and artificial intelligence
models that  are applied to an unseen sample set of data, the testing data set, to  
",195,53,0.824819772873285,6,0.848983814318975
256,"Human health and social work activities","A Network Tour of Data Science",0.87413477897644,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","A review of existing applications and techniques for narrative text analysis in electronic medical records ARC (Automated Retrieval Console): An algorithm based on an artificial intelligence program,
which  That result suggested that the major variety and contexts for the PHI in the social work notes
is more difficult to model  Data quality is an important barrier to NLP and text mining  
",195,53,0.824819772873285,6,0.848983814318975
281,"Human health and social work activities","Information Retrieval",0.844909608364105,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","[BOOK][B] The future of work in Australia: anticipating how new technologies will reshape labour markets, occupations and skill requirements He holds qualifications in political science, social policy, and social work  recent developments
in information and communication technology (ICT), computer-based technologies (CBT) andartificial intelligence (AI) have  Big data has helped to facilitate significant advances in AI  
",195,48,0.813798369218906,6,0.821868936220805
290,"Human health and social work activities","Information Retrieval",0.846901535987854,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Web analytics enhancing Project Planning: the case of Digital Marketing campaigns: Qualitative study of structured Web analytics data in Project Management enable the process of trace and read virtual traffic, by learning how the user interacts  time and
date at which it occurred, and the characteristics of the machine from which  research on project
planning with Web mining, especially looking at Web analytics data: this combination  
",195,48,0.813798369218906,6,0.821868936220805
291,"Human health and social work activities","Lab in data science",0.83749920129776,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Web analytics enhancing Project Planning: the case of Digital Marketing campaigns: Qualitative study of structured Web analytics data in Project Management enable the process of trace and read virtual traffic, by learning how the user interacts  time and
date at which it occurred, and the characteristics of the machine from which  research on project
planning with Web mining, especially looking at Web analytics data: this combination  
",195,78,0.825876659307724,6,0.823953568935394
324,"Human health and social work activities","A Network Tour of Data Science",0.853774964809418,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Hands on the wheel: Navigating algorithmic management and Uber drivers' autonomy More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,53,0.824819772873285,6,0.848983814318975
329,"Human health and social work activities","A Network Tour of Data Science",0.84189510345459,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Sentiment analysis for hate speech detection on social media: TF-IDF weighted N-Grams based approach As such, preprocessing unstructured data is a very important role in the text classification  The
number of features can therefore be quite big for a corpus that is average sized  problems and
poses a significant problem to many machine learning algorithms (Yang & Pedersen  
",195,53,0.824819772873285,6,0.848983814318975
334,"Human health and social work activities","Lab in data science",0.806936144828796,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Analysing large volumes of complex qualitative data-Reflections from a group of international experts Georgia Philip is a Research Fellow in the School of Social Work, at the University of East Anglia 
analysis?' In so doing, he considers the advantages and challenges of using Machine Learning
to assist with coding and help researchers handle large volumes of data in a  
",195,78,0.825876659307724,6,0.823953568935394
344,"Human health and social work activities","A Network Tour of Data Science",0.860787451267242,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] Hands on the wheel: Navigating algorithmic management and Uber drivers' More recently, with the rise of big data collection and machine learning techniques, algorithms
have  Moreover, algorithms based on big data and statistics are often too complex to understand,
and since  referring to theory and the academic literature to inform our data analysis  
",195,53,0.824819772873285,6,0.848983814318975
349,"Human health and social work activities","Information Retrieval",0.786134481430054,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","[PDF][PDF] The future of well-being in a tech-saturated world Dangers: Tiziana Dearing, a professor at the Boston College School of Social Work, said, People's
well  lives, technology companies will find new, invasive ways to exploit data generated on  As
we enter the Artificial Intelligence era we must examine and make transparent how  
",195,48,0.813798369218906,6,0.821868936220805
361,"Human health and social work activities","Lab in data science",0.829260051250458,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Tracking and Controlling Inter-System Processing Events Using Event Tokens or tourism; G06Q50/10Services; G06Q50/22Health care, eg hospitals; Social work;  disclosure
relate to data processing, artificial intelligence, and using artificial intelligence-enabled data  event
token management computing platform 110 may receive adjudication data from a  
",195,78,0.825876659307724,6,0.823953568935394
362,"Human health and social work activities","Information Retrieval",0.825659811496735,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Tracking and Controlling Inter-System Processing Events Using Event Tokens or tourism; G06Q50/10Services; G06Q50/22Health care, eg hospitals; Social work;  disclosure
relate to data processing, artificial intelligence, and using artificial intelligence-enabled data  event
token management computing platform 110 may receive adjudication data from a  
",195,48,0.813798369218906,6,0.821868936220805
373,"Human health and social work activities","Information Retrieval",0.809214949607849,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","The Future of MarketingAn Investigation into Disruption and Innovation Hoanca, 2015, p. 45) for market research, which will itself become automated. Privacy may
fall by the wayside as marketing applications use deep learning to  expected innovations around
analytics, Big Data, machine learning, and artificial intelligence  
",195,48,0.813798369218906,6,0.821868936220805
378,"Human health and social work activities","Lab in data science",0.836522877216339,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Value co-creation and potential benefits through big data analytics: Health benefit analysis practice where the newer healthcare delivery models depend on user-friendly, real-time big data
analytics, artificial intelligence (AI) and machine learning (ML) tools, and that millions  regarding
the Finnish health data environment. However, they do not provide any  
",195,78,0.825876659307724,6,0.823953568935394
382,"Human health and social work activities","Lab in data science",0.844167768955231,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[BOOK][B] Toward information justice: Technology, politics, and policy for data in higher education administration 31 2.2 Big Data in Higher Education  scientists and users to accept current data practices and
outcomes as natural or inevitable, and to make data use the  Even in manual technolo- gies, the
technique reduces the human to machine, carrying out tasks as if human practitioners  
",195,78,0.825876659307724,6,0.823953568935394
408,"Human health and social work activities","Information Retrieval",0.818393230438232,"Understand and use techniques for efficient keyword-based information retrieval, such as inverted indices, top-k query answering, resolving wildcard queries.
Explain and use ranking techniques, including the vector-space model, the Boolean retrieval model, and language-based models.
Understand the advantages and disadvantages of the different score models and be able to decide in which situation to use what model.
Apply data preprocessing techniques to improve retrieval, such as stemming, lemmatization, stop-word removal, latent semantic indexing.
Assess the importance of web-based resources based on link analysis such as pagerank and hubs-and-authorities.
Efficiently implement high-dimensional search via latent-semantic indexing.
Apply the retrieval techniques covered in thee course to develop a retrieval engine that addresses an information retrieval need for a given data collection.The course covers the foundations of current ranking techniques (which are often based on statistical models), index structures, and current implementation issues for the design of effective and scalable information-system architectures. The course covers a wide range of ranking principles, starting from the Boolean retrieval model, over to statistical ranking models such as TF-IDF, and on to probabilistic ranking techniques such as Okapi-BM25. The course will cover classical IR topics, including link-analysis methods such as PageRank, the user-specific personalization of queries, and relevance feedback. 

The course topics are:

 

Boolean retrieval
Vector Space Model
Language Based Model
Boolean Independence Model
Top-K querying and Index construction
Index construction and compression
Feedback-Expansion
Evaluating IR
Link analysis
Dimensionality Reduction
Neighbor search in high dimensional data
","Re-examining and re-conceptualising Enterprise Search and Discovery capability: Towards a model for the factors and generative mechanisms for search task  225 5.6.2.3 Suboptimal Learning/Sharing culture  These include, the Statistical Machine (Goldberg
1927), Mundaneum (Otlet 1934), World Brain(Wells 1937), Universal  Organizations seek to exploit
'big data' volumes for differentiating insights supporting wealth creation  
",195,48,0.813798369218906,6,0.821868936220805
760,"Financial service activities, except insurance and pension funding","Information Retrieval",0.793152153491974,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Big Data Finance and Financial MarketsFinancial markets are always the most aggressive adopters of new information technologies. The recent boom in big data has enhanced the effect of information diffusion in financial markets since the physical cost of participation has been reduced and interactions among ",135,48,0.813798369218906,6,0.806733916203181
801,"Financial service activities, except insurance and pension funding","Information Retrieval",0.792005658149719,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Compressed Sensing and its Applications in Risk Assessment for Internet Supply Chain Finance under Big DataPlenty of research focuses on supply chain finance and its risk, qualitatively or quantitatively. However, only a little literature studies on internet supply chain finance (ISCF), especially on its risk by quantitative analysis. After analyzing the information of partners' panorama data ",135,48,0.813798369218906,6,0.806733916203181
812,"Financial service activities, except insurance and pension funding","Information Retrieval",0.817867636680603,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Introduction to Machine Learning and Network Analytics in Finance MinitrackWe are experiencing enormous growth in the interest of application of various computational methods in finance, which is the consequence of various developments in the last 15 years. As a result, the number and importance of contributions utilizing various machine learning  ",135,48,0.813798369218906,6,0.806733916203181
842,"Financial service activities, except insurance and pension funding","Information Retrieval",0.813815593719482,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Reflection on Big Data Technology: Problems and Countermeasures in"" Big Data Credit Reporting"" of Internet Finance in ChinaWith the rapid development of Internet finance in China for the past few years, big data credit reporting agencies specifically for network credit information have been initially established. An analysis on the technology application, characteristics and operational difficulties of big  ",135,48,0.813798369218906,6,0.806733916203181
861,"Financial service activities, except insurance and pension funding","Information Retrieval",0.809747576713562,"Understand and use techniques for efficient keyword-based information retrieval, such as inverted indices, top-k query answering, resolving wildcard queries.
Explain and use ranking techniques, including the vector-space model, the Boolean retrieval model, and language-based models.
Understand the advantages and disadvantages of the different score models and be able to decide in which situation to use what model.
Apply data preprocessing techniques to improve retrieval, such as stemming, lemmatization, stop-word removal, latent semantic indexing.
Assess the importance of web-based resources based on link analysis such as pagerank and hubs-and-authorities.
Efficiently implement high-dimensional search via latent-semantic indexing.
Apply the retrieval techniques covered in thee course to develop a retrieval engine that addresses an information retrieval need for a given data collection.The course covers the foundations of current ranking techniques (which are often based on statistical models), index structures, and current implementation issues for the design of effective and scalable information-system architectures. The course covers a wide range of ranking principles, starting from the Boolean retrieval model, over to statistical ranking models such as TF-IDF, and on to probabilistic ranking techniques such as Okapi-BM25. The course will cover classical IR topics, including link-analysis methods such as PageRank, the user-specific personalization of queries, and relevance feedback. 

The course topics are:

 

Boolean retrieval
Vector Space Model
Language Based Model
Boolean Independence Model
Top-K querying and Index construction
Index construction and compression
Feedback-Expansion
Evaluating IR
Link analysis
Dimensionality Reduction
Neighbor search in high dimensional data
","[PDF][PDF] Applications of Big Data methods in Finance: Index TrackingRESEARCH OBJECTIVES Although the curse of dimensionality does not relate to most financial settings, high-dimensional methods gained some relevance in the recent finance literature. Index tracking aims at finding an optimal sample of stocks able to mimic the ",135,48,0.813798369218906,6,0.806733916203181
869,"Financial service activities, except insurance and pension funding","Information Retrieval",0.813814878463745,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Using Robotic Process Automation in Finance organizations: Case studySoftware robotics has emerged as a new technological development over the last few years, offering a lot of potential for optimizing and improving processes. Software robotics offers a new tool also for finance organizations, where there are still number of manual tasks being ",135,48,0.813798369218906,6,0.806733916203181
875,"Public administration and defence, compulsory social security","Big Data",0.82181191444397,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","To do more, better, faster and more cheaply: Using big data in public administrationBig data have become a game-changer for modern public administration in those areas in which they are used. Although their application is still limited in the public sector, their use develops dynamically in areas where they bring tangible results in terms of efficiency and ",110,99,0.823880515315316,6,0.817420492569605
879,"Public administration and defence, compulsory social security","Reliable and Interpretable Artificial Intelligence",0.786823987960815,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","To do more, better, faster and more cheaply: Using big data in public administrationBig data have become a game-changer for modern public administration in those areas in which they are used. Although their application is still limited in the public sector, their use develops dynamically in areas where they bring tangible results in terms of efficiency and ",110,35,0.819798954895565,6,0.809020042419434
903,"Public administration and defence, compulsory social security","Big Data",0.804739236831665,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Public Administration Curriculum-Based Big Data Policy-Analytic Epistemology: Symbolic IoT Action-Learning Solution ModelThe equilibration that underscores the internet of things (IoT) and big data analytics (BDA) cannot be underestimated at the behest of real-life social challenges and significant policy data generated to redress the concerns of epistemic communities, such as political policy ",110,99,0.823880515315316,6,0.817420492569605
909,"Public administration and defence, compulsory social security","Reliable and Interpretable Artificial Intelligence",0.787194728851318,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Digital and Intelligent Public Administration: transformations in the era of artificial intelligenceThis article addresses the impact of the digital era and it specifically refers to information and communication technologies (ICT) in Public Administration. It is based on the international approach and underscores the importance of incorporating new technologies established by ",110,35,0.819798954895565,6,0.809020042419434
910,"Public administration and defence, compulsory social security","Reliable and Interpretable Artificial Intelligence",0.828967154026031,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","This paper describes a model of digital governance that reproduces within the system essential features of public administration while establishing logic for their utilization. The ultimate goal is to be able to confine all participants to their respective roles and Administration by Algorithm? Public Management Meets Public Sector Machine Learning",110,35,0.819798954895565,6,0.809020042419434
931,"Public administration and defence, compulsory social security","Big Data",0.830275177955627,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Research on the Optimization of Military Supplies under Big Data BackgroundOn the basis of analyzing the characteristics of big data technology and combining the actual demand of military demand industry in our army, this paper constructs a comprehensive analysis environment of big data for military demand, and on this basis ",110,99,0.823880515315316,6,0.817420492569605
935,"Public administration and defence, compulsory social security","Big Data",0.794776439666748,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Analysis of Military Academy Smart Campus Based on Big DataThis paper compares the digital campus with the smart campus and analysis the framework of smart campus in the big data environment, based on the actual characteristics of military academies. The framework utilizes the information technologies such as Internet of Things ",110,99,0.823880515315316,6,0.817420492569605
942,"Public administration and defence, compulsory social security","Reliable and Interpretable Artificial Intelligence",0.820969939231873,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Facilitation of Trust in Automation: A Qualitative Study of Behaviour and Attitudes Towards Emerging Technology in Military CultureThe research in this field is limited due to the inherent technological limitations of existing systems, of which has saturated the literature at this point (Barnes, et al., 2014). The core of existing research centres along assessment of emerging and novel interfaces for the pursuit ",110,35,0.819798954895565,6,0.809020042419434
956,"Public administration and defence, compulsory social security","Big Data",0.817136704921722,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","The Impact of a BIG DATA Decision Support Tool on Military Logistics: MEDICAL ANALYTICS MEETS THE MISSION.Using big data and predictive analytics, more segments of the US military will be able to create decision support tools that help them not only to carry out their missions more efficiently, but also to streamline their logistical requirements. Within the military's medical ",110,99,0.823880515315316,6,0.817420492569605
966,"Public administration and defence, compulsory social security","Big Data",0.8357834815979,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Research on Military Logistics based on Big DataThis document gives formatting instructions for authors preparing papers for publication. With the arrival of the era of big data, the mature use of cloud computing and data mining technology, the big data mode have been widely applied in logistics. For military logistics, a ",110,99,0.823880515315316,6,0.817420492569605
970,"Public administration and defence, compulsory social security","Reliable and Interpretable Artificial Intelligence",0.804283380508423,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","[PDF][PDF] Developments in Artificial IntelligenceOpportunities and Challenges for Military Modeling and SimulationOne of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is"" Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"". Simulation might play a significant role to play in these developments as it ",110,35,0.819798954895565,6,0.809020042419434
983,"Public administration and defence, compulsory social security","Reliable and Interpretable Artificial Intelligence",0.825881063938141,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Article deals with a set of problems linked to a Engineer Force Protection Provision algorithm design and evaluation of input factors series. This algorithm is generally compatible with The NATO Force Protection Process Model adjusting it to a part of engineer forces' decision Intrusion Detection of Data Platform Based on Extreme Learning Machine in Civil and Military Integration",110,35,0.819798954895565,6,0.809020042419434
1019,"Real estate activities","Lab in data science",0.793766438961029,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Automation of the technical due diligence with artificial intelligence in the real estate industryOver the real estate lifecycle numerous documents and data are generated. The majority of building-related data is collected in day-to-day operations, such as maintenance protocols, contracts or energy consumptions. Previous successes in the classification already help to ",60,78,0.825876659307724,6,0.821711113055547
1024,"Real estate activities","Lab in data science",0.859490752220154,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[BOOK][B] Real Estate Analysis in the Information Age: Techniques for Big Data and Statistical ModelingThe creation, accumulation, and use of copious amounts of data are driving rapid change across a wide variety of industries and academic disciplines. This 'Big Data'phenomenon is the result of recent developments in computational technology and improved data gathering ",60,78,0.825876659307724,6,0.821711113055547
1025,"Real estate activities","Lab in data science",0.792148768901825,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Real estate bubble prediction based on big dataDisclosed herein are a computer apparatus, non-transitory computer readable medium, and method for predicting real estate bubbles based on big data analysis. Historical variable data associated with real estate assets are obtained from remote data sources. Portions of ",60,78,0.825876659307724,6,0.821711113055547
1038,"Real estate activities","Lab in data science",0.860062181949615,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Urban data streams and machine learning: a case of swiss real estate marketIn this paper, we show how using publicly available data streams and machine learning algorithms one can develop practical data driven services with no input from domain experts as a form of prior knowledge. We report the initial steps toward development of a real estate  ",60,78,0.825876659307724,6,0.821711113055547
1049,"Real estate activities","Lab in data science",0.829237759113312,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","News-based sentiment analysis in real estate: A supervised machine learning approach with support vector networksWith the rapid growth of news, information and opinionated data available in digital form, accompanied by a swift progress of textual analysis techniques, the field of sentiment analysis became a hotspot in the area of natural language processing. Additionally ",60,78,0.825876659307724,6,0.821711113055547
1063,"Real estate activities","Lab in data science",0.795560777187347,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","[HTML][HTML] Urban Tech on the Rise: Machine Learning Disrupts the Real Estate Industry. Featuring interviews of: Marc Rutzen and Jasjeet Thind by Stanislas Chaillou The practice of AI-powered Urban Analytics is taking off within the real estate industry. Data science and algorithmic logic are close to the forefront of new urban development practices. How close? is the questionexperts predict that digitization will go far beyond intelligent ",60,78,0.825876659307724,6,0.821711113055547
4,"Legal and accounting activities","Information Retrieval",0.812439918518066,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Business intelligence & analytics in management accounting research: Status and future focus that although there is potential for studying business intelligence solutions in  is linked to other
emerging technologies such as big data, machine learning and the Internet  Judgment and
decision-making, Databases, and Expert systems, artificial intelligence and decision-aids  
",80,48,0.813798369218906,5,0.816890025138855
10,"Legal and accounting activities","Information Retrieval",0.813255250453949,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Influence of Artificial Intelligence on Activities and Competitiveness of an Organization view and edit this information), but also about the compliance with legal regulations (eg  Within
this group 85% of managers believe in the potential of artificial intelligence, 25% implement it 
The category of solutions integrating data from various sources (eg Twillio) and platforms  
",80,48,0.813798369218906,5,0.816890025138855
22,"Legal and accounting activities","Information Retrieval",0.799102127552032,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Toward blockchain-based accounting and assurance of an ecosystem of emerging technologies that includes artificial intelligence, the Internet of  rules
encoded could effectively control the recording of accounting activities and, therefore, provide intelligence to the accounting process by integrating Big Data and predictive analysis  
",80,48,0.813798369218906,5,0.816890025138855
46,"Legal and accounting activities","Information Retrieval",0.824065208435059,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","What role for social sciences in innovation? related fields (including computer science, information systems, software engineering and artificialintelligence) are found  Availability of comprehensive, long-term and internationally comparabledata allows for  cite NPL; citations are frequently given by examiners or by patent  
",80,48,0.813798369218906,5,0.816890025138855
50,"Legal and accounting activities","Information Retrieval",0.835587620735168,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Tech human resources: a study on the impact of digital technologies on international HR startups The role of technology is spreading in all processes which composed Human Resource function:
form Recruiting to Learning and  resource tech industry are Social, Mobile, Artificial Intelligence,Big Data and Analytics and Cloud  oriented or effectiveness oriented  
",80,48,0.813798369218906,5,0.816890025138855
225,"Human health and social work activities","Deep Learning",0.801013946533203,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","[PDF][PDF] Where is technology taking the economy the Internet, the cloud, big data, robotics, machine learning, and now artificial intelligence
together powerful  And data can't easily be owned either, it can be garnered from nonproprietary 
will still have jobs, especially those like kindergarten teaching or social work that require  
",195,13,0.819467407006484,5,0.803144860267639
300,"Human health and social work activities","Deep Learning",0.836821854114532,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Do citations and readership identify seminal publications? deep learning paper which has caused a shift in the area of artificial intelligence/computer vision 
To do this, we use the threshold which achieves the best accuracy on the training data  The reason
why we chose the this simple model instead of a machine learning model such  
",195,13,0.819467407006484,5,0.803144860267639
355,"Human health and social work activities","Deep Learning",0.809291124343872,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Evaluation of support and training sign language services at Setotolwane Secondary School DATA PRESENTATION, ANALYSIS AND INTERPRTATION ..... 27  Starner, Weaver and
Pentland (1998) present two real-time machine systems for  learners with hearing impairments
in using an educational game for learning the Sign Language  
",195,13,0.819467407006484,5,0.803144860267639
369,"Human health and social work activities","Deep Learning",0.781019449234009,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","[PDF][PDF] STATE OF THE NORTH There are five big challenges which this generation will need to meet as they gradually 
Digitalisation, artificial intelligence, machine learning and advanced robotics are of particular
focus for economists, and are starting to  Source: HMRC, 'Summary data tables' (HMRC 2017)  
",195,13,0.819467407006484,5,0.803144860267639
412,"Human health and social work activities","Deep Learning",0.787577927112579,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Suicide attempts of community adolescents and young adults: an explanatory and predictive epidemiological approach My first association with (co-)authorship is intensive learning (again). That's not the worst first
association  the focus away from risk factors towards risk algorithms, ie Machine Learning (ML).
ML is deemed to be well suited to investigate complex associations in data  
",195,13,0.819467407006484,5,0.803144860267639
431,"Agriculture, forestry and fishing","Data Mining",0.802482485771179,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","[HTML][HTML] Analysis of agriculture data using data mining techniques: application of big dataIn agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estimation for the ",180,9,0.830869707796309,5,0.81921603679657
469,"Agriculture, forestry and fishing","Data Mining",0.796967744827271,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","[PDF][PDF] Machine learning and data mining advance predictive big data analysis in precision animal agriculturePrecision animal agriculture is poised to rise to prominence in the livestock enterprise in the domains of management, production, welfare, sustainability, health surveillance, and environmental footprint. Considerable progress has been made in the use of tools to ",180,9,0.830869707796309,5,0.81921603679657
475,"Agriculture, forestry and fishing","Data Mining",0.813122510910034,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","0245 Big data and occupational health vigilance: use of french medico-administrative databases for hypothesis generation regarding occupational risks in agriculture Poster Presentation. Methodology. 0245 Big data and occupational health vigilance: use of 
medico-administrative databases for hypothesis generation regarding occupational risks inagriculture  complementary methods relying on exploitation of already existing data, such as  
",180,9,0.830869707796309,5,0.81921603679657
553,"Agriculture, forestry and fishing","Data Mining",0.829012513160706,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","What is cyber-physical system driven agriculture?-Redesign of big data for outstanding farmer management Theoretically, precision agriculture tools coupled with innovative data- mining procedures and
predictive models based on artificial intelligence, will be able to deliver personalized
recommendations at an appropriate spatial scale, so that agricultural productivity  
",180,9,0.830869707796309,5,0.81921603679657
580,"Agriculture, forestry and fishing","Data Mining",0.85449492931366,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","[CITATION][C] Analysis of agriculture data using data mining techniques: application of big data in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",180,9,0.830869707796309,5,0.81921603679657
653,"Electricity, gas, steam and air conditioning supply","Systems for data science",0.81829309463501,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Clustering Electricity Big Data for Consumption Modeling Using Comparative Strainer Method for High Accuracy Attainment and Dimensionality ReductionIn smart grid, the relation between grid and customer is bidirectional. Therefore, analyzing load consumption patterns is essential for optimal and efficient operation and planning of smart grid in addition to precise load forecasting. However, emergence of the advanced ",105,21,0.821209547065553,5,0.822704148292541
689,"Electricity, gas, steam and air conditioning supply","Systems for data science",0.831022083759308,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Design of big data analytics electricity collecting data analysis and intelligent monitoring systemsIn order to strengthen the power of users better characteristics, measurement device and distribution network equipment condition monitoring and analysis, based on electricity acquisition data analysis and intelligent monitoring system based on the integration of ",105,21,0.821209547065553,5,0.822704148292541
704,"Electricity, gas, steam and air conditioning supply","Systems for data science",0.8152015209198,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Big Data Analytics for Electricity Price ForecastElectricity Price forecast is a major task in smart grid operation. There is a massive amount of data flowing in the power system including the data collection by control systems, sensors, etc. In addition, there are many data points which are not captured and processed by the ",105,21,0.821209547065553,5,0.822704148292541
708,"Electricity, gas, steam and air conditioning supply","Systems for data science",0.813468813896179,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","[PDF][PDF] Artificial Intelligence and Nord Pool's intraday electricity market Elbas: a demonstration and pragmatic evaluation of employing deep learning for price prediction This thesis demonstrates the use of deep learning for automating hourly price forecasts in continuous intraday electricity markets, using various types of neural networks on comprehensive sequential market data and cutting-edge image processing networks on ",105,21,0.821209547065553,5,0.822704148292541
727,"Electricity, gas, steam and air conditioning supply","Systems for data science",0.835535228252411,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","IArchitecture based on multivariate big data platform for analyzing electricity consumption behavior,""With the development of smart grid, more and more measuring devices extend to bottom layer. The development of advanced measurement system and distribution network inevitably leads to the geometric increase of user data. On the other hand, the power grid is ",105,21,0.821209547065553,5,0.822704148292541
742,"Financial service activities, except insurance and pension funding","Artificial Intelligence and Data Science",0.82464325428009,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Machine learning in finance: A topic modeling approachWe provide a first comprehensive structuring of the literature applying machine learning to finance. We use a probabilistic topic modeling approach to make sense of this diverse body of research spanning across the disciplines of finance, economics, computer sciences, and ",135,47,0.827168932620515,5,0.833380305767059
753,"Financial service activities, except insurance and pension funding","Deep Learning ",0.816372752189636,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Machine learning for quantitative finance: fast derivative pricing, hedging and fittingIn this paper, we show how we can deploy machine learning techniques in the context of traditional quant problems. We illustrate that for many classical problems, we can arrive at speed-ups of several orders of magnitude by deploying machine learning techniques based ",135,16,0.825420185923576,5,0.830093097686768
772,"Financial service activities, except insurance and pension funding","Artificial Intelligence and Data Science",0.837588727474213,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[PDF][PDF] Machine learning in finance: the case of deep learning for option pricingModern advancements in mathematical analysis, computational hardware and software, and availability of big data have made possible commoditized machines that can learn to operate as investment managers, financial analysts, and traders. We briefly survey how and ",135,47,0.827168932620515,5,0.833380305767059
774,"Financial service activities, except insurance and pension funding","Deep Learning ",0.824011087417603,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","[PDF][PDF] Machine learning in finance: the case of deep learning for option pricingModern advancements in mathematical analysis, computational hardware and software, and availability of big data have made possible commoditized machines that can learn to operate as investment managers, financial analysts, and traders. We briefly survey how and ",135,16,0.825420185923576,5,0.830093097686768
792,"Financial service activities, except insurance and pension funding","Deep Learning ",0.843639731407166,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","[PDF][PDF] Machine Learning Projection Methods for Macro-Finance ModelsThis paper develops a global solution method to solve large state space macro-finance models using machine learning. Our new method, an artificial neural network expectation algorithm, is not only considerably faster but also as precise and more scalable than the ",135,16,0.825420185923576,5,0.830093097686768
809,"Financial service activities, except insurance and pension funding","Deep Learning ",0.869974911212921,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Introduction to the Minitrack on Machine Learning and Network Analytics in FinanceRecent years have seen a rapid evolution of methodologies in artificial intelligence and machine learning, and as a result, increasingly widespread use of these techniques in different domains. One of the most important application areas is finance, offering ",135,16,0.825420185923576,5,0.830093097686768
826,"Financial service activities, except insurance and pension funding","Artificial Intelligence and Data Science",0.849189937114716,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Research on the Core Competence and Training System of Computer Speciality for Big Data Processing in Colleges and Universities of Finance and EconomicsIn big data era, the computer professionals should be equipped with the ability of big data processing and analysis. And multi-disciplinary collaborative innovation and interdisciplinary cross-learning ability is becoming more and more important. According to ",135,47,0.827168932620515,5,0.833380305767059
832,"Financial service activities, except insurance and pension funding","Artificial Intelligence and Data Science",0.807186841964722,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Appropriate machine learning techniques for credit scoring and bankruptcy prediction in banking and finance: a comparative studyAbstract Machine learning techniques have been used successfully in several areas such as banking and finance. These techniques are used mainly for prediction, classification and partitioning data into different groups according to a certain common characteristic. In this ",135,47,0.827168932620515,5,0.833380305767059
849,"Financial service activities, except insurance and pension funding","Deep Learning ",0.796467006206512,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Machine Learning for Structured FinanceMachine learning and artificial intelligence have evolved beyond simple hype and have integrated themselves in business and in popular conversation as an increasing number of smart applications profoundly transform the way we work and live. This article defines ",135,16,0.825420185923576,5,0.830093097686768
873,"Financial service activities, except insurance and pension funding","Artificial Intelligence and Data Science",0.848292768001556,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Machine learning with applications to financeThe impact of data driven, machine learning technologies across a wide variety of fields is undeniable. The financial industry, which relies heavily on predictive modeling being no exception. In this work we summarize two widely used machine learning models: support ",135,47,0.827168932620515,5,0.833380305767059
880,"Public administration and defence, compulsory social security","Biomedical Informatics",0.817059636116028,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","The application of artificial intelligence in public administration for forecasting high crime risk transportation areas in urban environmentPublic administration has adopted information and communication technology in order to construct new intelligent systems and design new risk prevention strategies in transportation management. The ultimate goal is to improve the quality of the transportation services and ",110,18,0.817355437411202,5,0.812551724910736
882,"Public administration and defence, compulsory social security","Information security and privacy",0.797451853752136,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","The application of artificial intelligence in public administration for forecasting high crime risk transportation areas in urban environmentPublic administration has adopted information and communication technology in order to construct new intelligent systems and design new risk prevention strategies in transportation management. The ultimate goal is to improve the quality of the transportation services and ",110,10,0.806313174962997,5,0.806759071350098
891,"Public administration and defence, compulsory social security","Biomedical Informatics",0.825757682323456,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","An information system for judicial and public administration using artificial intelligence and geospatial dataThe adoption of information technology in judicial and public administration has become a major need nowadays with the rapid growth of information regarding managerial issues. This paper presents an advanced methodology developed by using Information and ",110,18,0.817355437411202,5,0.812551724910736
892,"Public administration and defence, compulsory social security","Information security and privacy",0.814504384994507,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","An information system for judicial and public administration using artificial intelligence and geospatial dataThe adoption of information technology in judicial and public administration has become a major need nowadays with the rapid growth of information regarding managerial issues. This paper presents an advanced methodology developed by using Information and ",110,10,0.806313174962997,5,0.806759071350098
896,"Public administration and defence, compulsory social security","Biomedical Informatics",0.837538361549377,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","A Study on Conversational Public Administration Service of the Chatbot Based on Artificial IntelligenceArtificial intelligence-based services are expanding into a new industrial revolution. There is artificial intelligence technology applied in real life due to the development of big data and deep learning related technology. And data analysis and intelligent assistant services that ",110,18,0.817355437411202,5,0.812551724910736
906,"Public administration and defence, compulsory social security","Biomedical Informatics",0.794585645198822,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Digital and Intelligent Public Administration: transformations in the era of artificial intelligenceThis article addresses the impact of the digital era and it specifically refers to information and communication technologies (ICT) in Public Administration. It is based on the international approach and underscores the importance of incorporating new technologies established by ",110,18,0.817355437411202,5,0.812551724910736
918,"Public administration and defence, compulsory social security","Lab in data science",0.8614781498909,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Machine Learning-Based Models for Big Data Analysis and Prediction: Social Security ApplicationsThe core technology of the Fourth Industrial Revolution is artificial intelligence and big data, and the continuous enhancement of algorithm performance through Machine Learning based on large scale accumulated data is an mportant source technology in all fields ",110,78,0.825876659307724,5,0.823179686069489
920,"Public administration and defence, compulsory social security","Information security and privacy",0.802215754985809,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","Security and privacy of big data for social networking services in cloudBig Data (BD) is of great importance especially in wireless telecommunications field. Social Networking (SNg) is one more fast-growing technology that allows users to build their profile and could be described as web applications. Both of them face privacy and security issues ",110,10,0.806313174962997,5,0.806759071350098
933,"Public administration and defence, compulsory social security","Lab in data science",0.826572954654694,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Research on the Optimization of Military Supplies under Big Data BackgroundOn the basis of analyzing the characteristics of big data technology and combining the actual demand of military demand industry in our army, this paper constructs a comprehensive analysis environment of big data for military demand, and on this basis ",110,78,0.825876659307724,5,0.823179686069489
939,"Public administration and defence, compulsory social security","Lab in data science",0.785073459148407,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Analysis of Military Academy Smart Campus Based on Big DataThis paper compares the digital campus with the smart campus and analysis the framework of smart campus in the big data environment, based on the actual characteristics of military academies. The framework utilizes the information technologies such as Internet of Things ",110,78,0.825876659307724,5,0.823179686069489
949,"Public administration and defence, compulsory social security","Biomedical Informatics",0.787817299365997,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Artificial Intelligence Within the Military Domain and Cyber WarfareThe potential uses of machine learning and artificial intelligence in the cyber security domain have had a recent surge of interest. Much of the research and discussions in this area primarily focuses on reactive uses of the technology such as enhancing capabilities in ",110,18,0.817355437411202,5,0.812551724910736
958,"Public administration and defence, compulsory social security","Lab in data science",0.805845975875854,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","The Impact of a BIG DATA Decision Support Tool on Military Logistics: MEDICAL ANALYTICS MEETS THE MISSION.Using big data and predictive analytics, more segments of the US military will be able to create decision support tools that help them not only to carry out their missions more efficiently, but also to streamline their logistical requirements. Within the military's medical ",110,78,0.825876659307724,5,0.823179686069489
959,"Public administration and defence, compulsory social security","Information security and privacy",0.798214614391327,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","The Impact of a BIG DATA Decision Support Tool on Military Logistics: MEDICAL ANALYTICS MEETS THE MISSION.Using big data and predictive analytics, more segments of the US military will be able to create decision support tools that help them not only to carry out their missions more efficiently, but also to streamline their logistical requirements. Within the military's medical ",110,10,0.806313174962997,5,0.806759071350098
965,"Public administration and defence, compulsory social security","Lab in data science",0.836927890777588,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Research on Military Logistics based on Big DataThis document gives formatting instructions for authors preparing papers for publication. With the arrival of the era of big data, the mature use of cloud computing and data mining technology, the big data mode have been widely applied in logistics. For military logistics, a ",110,78,0.825876659307724,5,0.823179686069489
978,"Public administration and defence, compulsory social security","Information security and privacy",0.821408748626709,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","Are We Flooding Pilots with Data?Effects of Situational Awareness Automation Support Concepts on Decision-Making in Modern Military Air OperationsWithin highly dynamic situations, the amount of relevant information that a pilot needs to process to make an informed decision can be substantial. With an ever increasing amount of data available to the pilot there is a real risk that not all relevant data can be taken into ",110,10,0.806313174962997,5,0.806759071350098
985,"Water supply, sewerage, waste management and remediation activities","Applied data analysis",0.847949802875519,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Machine learning in coupled wildfire-water supply risk assessment: Data science toolkitThe frontier of wildfire-related risk assessment is moving into data science territory, and with good reason. Computational statistics, built on a foundation of high resolution remote sensing data, ground data, and theory, forms the basis of powerful risk assessment tools ",30,120,0.829422693451246,5,0.83244115114212
990,"Water supply, sewerage, waste management and remediation activities","Applied data analysis",0.808433055877686,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Preparing for the Use of Big Data in Denmark's Waste Management SectorThis project explored the challenges and opportunities associated with prospective implementations of big data analytics in Denmark's waste industry. We found that while some waste management companies collect detailed data, they do not use or share their ",30,120,0.829422693451246,5,0.83244115114212
996,"Water supply, sewerage, waste management and remediation activities","Applied data analysis",0.811764299869537,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","A framework of developing a big data platform for construction waste management: a Hong Kong studyBig data has shown great potentials in improving management discretion in many areas. The applications of big data in areas such as finance, computer science, health care and medical science have made continued success. Despite of big data's potentials, its ",30,120,0.829422693451246,5,0.83244115114212
1001,"Water supply, sewerage, waste management and remediation activities","Applied data analysis",0.835487186908722,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Remediation, convergence, and big data: Conceptual limits of cross-platform social mediaThe era of multiplatform media and big data provide new opportunities to reconsider data access by media companies. Outlined here is the discussion surrounding data access from media institutional logic and user-centric perspectives in the contexts of digitalization and ",30,120,0.829422693451246,5,0.83244115114212
1011,"Water supply, sewerage, waste management and remediation activities","Applied data analysis",0.858571410179138,"This course teaches the basic techniques and practical skills required to make sense out of a variety of data, with the help of the most acclaimed software tools in the data science world: pandas, scikit-learn, Spark, etc.Thanks to a new breed of software tools that allows to easily process and analyze data at scale, we are now able to extract invaluable insights from the vast amount of data generated daily. As a result, both the business and scientific world are undergoing a revolution which is fueled by one of the most sought after job profiles: the data scientist.

This course covers the fundamental steps of the data science pipeline:

Data Acquisition

Variety as one of the main challenges in big data: structured, semi-structured, unstructured
Data sources: open, public (scraping, parsing, and down-sampling)
Dataset fusion, filtering, slicing & dicing
Data granularities and aggregations
Data Wrangling

Data manipulation, array programming, dataframes
The many sources of data problems (and how to fix them): missing data, incorrect data, inconsistent representations
Schema alignment, data reconciliation
Data quality testing with crowdsourcing
Data Interpretation

Stats in practice (distribution fitting, statistical significance, etc.)
Co-occurrence grouping (market-basket analysis)
Machine learning in practice (supervised and unsupervised, feature engineering, more data vs. advanced algorithms, curse of dimensionality, etc.)
Text mining: vector space model, topic models, word embedding
Social network analysis (influencers, community detection, etc.)
 Data Visualization

Introduction to different plot types (1, 2, and 3 variables), layout best practices, network and geographical data
Visualization to diagnose data problems, scaling visualization to large datasets, visualizing uncertain data
Reporting

Results reporting, infographics
How to publish reproducible results
Anonymiziation, ethical concerns
 

The students will learn the techniques during the ex-cathedra lectures, and will then get familiar with the software tools to complete the homework assignments (which will be in part executed under the supervision of the teacher and the assistants, during the lab hours).

In parallel, the students will embark in a semester-long project, split in agile teams of 3. The outcome of such team efforts will be unified towards the end of the course, to build a project portfolio that will be made public (and available as open-source).

At the end of the semester, students will also take a 3-hour final exam in a classroom with computers, where they will be asked to complete a data analysis pipeline (both with code and extensive comments) on a dataset they have never worked with before.By the end of the course, the student must be able to:
Construct a coherent understanding of the techniques and software tools required to perform the fundamental steps of the Data Science pipeline
Perform data acquisition (data formats, dataset fusion, Web scrapers, Rest APIs, Open Data, Big Data platforms, etc.)
Perform data wrangling (fixing missing and incorrect data, data reconciliation, data quality assessments, etc.)
Perform data interpretation (statistics, knowledge extraction, critical thinking, team discussions, ad-hoc visualizations, etc.)
Perform result dissemination (reporting, visualizations, publishing reproducible results, ethical concerns, etc.)
Perform data acquisition (data formats, dataset fusion, Web scrapers, REST APIs, open data, big data platforms, etc.)Transversal skills
Give feedback (critique) in an appropriate fashion.
Demonstrate the capacity for critical thinking
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Apparatus and method of adjusting a sensitivity buffer of semi-supervised machine learning principals for remediation of issues in a computer environmentIn a host device, a method for performing an anomaly analysis of a computer environment includes applying a learned behavior function to a data training set and to a set of data elements received from at least one computer environment resource to define at least one ",30,120,0.829422693451246,5,0.83244115114212
1,"Legal and accounting activities","Biomedical Informatics",0.818983674049377,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Business intelligence & analytics in management accounting research: Status and future focus that although there is potential for studying business intelligence solutions in  is linked to other
emerging technologies such as big data, machine learning and the Internet  Judgment and
decision-making, Databases, and Expert systems, artificial intelligence and decision-aids  
",80,18,0.817355437411202,4,0.822807759046555
18,"Legal and accounting activities","Reliable and Interpretable Artificial Intelligence",0.828640878200531,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Zero-Quartile Benchmarking How To Prioritize Digital Technologies In A Companys Transformation Journey Cognitive capabilities are used by applying artificial intelligence (AI) such as machine learningor natural language  It should help to handle the growing amount of data (Agarwal and Dhar 2014)and goes beyond traditional  descriptive  business intelligence (BI), because  
",80,35,0.819798954895565,4,0.816315025091171
23,"Legal and accounting activities","Reliable and Interpretable Artificial Intelligence",0.79407924413681,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Toward blockchain-based accounting and assurance of an ecosystem of emerging technologies that includes artificial intelligence, the Internet of  rules
encoded could effectively control the recording of accounting activities and, therefore, provide intelligence to the accounting process by integrating Big Data and predictive analysis  
",80,35,0.819798954895565,4,0.816315025091171
32,"Legal and accounting activities","Systems for data science",0.798514187335968,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Bits and bolts Real estate 68 Low Low Legal and accounting activities, etc  online renting of data or computing
capacities), as well as back and front office integration systems such as customer relationship
management (CRM), and enterprise resource planning (ERP) software. Page 14  
",80,21,0.821209547065553,4,0.808902338147163
39,"Legal and accounting activities","Systems for data science",0.800468802452087,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","[BOOK][B] The Routledge Companion to Accounting Information Systems interests include continuous auditing and monitoring, audit data analytics and artificial intelligence
in auditing  process management for AIS in the context of new legal requirements for  constituting
different decision-making environments and what they call data environments  
",80,21,0.821209547065553,4,0.808902338147163
43,"Legal and accounting activities","Reliable and Interpretable Artificial Intelligence",0.819946527481079,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Semantic Technologies for Business Decision Support processed and incorporated into the company structure, with the support of linguistic rules on
the intelligence phase  development, as it takes advantage of other disciplines as MachineLearning, Artificial Intelligence and Cognitive Science  It relies on data stored in a database  
",80,35,0.819798954895565,4,0.816315025091171
45,"Legal and accounting activities","Biomedical Informatics",0.840905725955963,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","What role for social sciences in innovation? related fields (including computer science, information systems, software engineering and artificialintelligence) are found  Availability of comprehensive, long-term and internationally comparabledata allows for  cite NPL; citations are frequently given by examiners or by patent  
",80,18,0.817355437411202,4,0.822807759046555
51,"Legal and accounting activities","Biomedical Informatics",0.814083158969879,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Tech human resources: a study on the impact of digital technologies on international HR startups The role of technology is spreading in all processes which composed Human Resource function:
form Recruiting to Learning and  resource tech industry are Social, Mobile, Artificial Intelligence,Big Data and Analytics and Cloud  oriented or effectiveness oriented  
",80,18,0.817355437411202,4,0.822807759046555
56,"Legal and accounting activities","Biomedical Informatics",0.817258477210999,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Adapting business framework conditions to deal with disruptive technologies in Denmark Participation in life-long learning is high but decreasing  Second, the ability to combine new
advanced technologies (such as sensors, advanced robotics and 3D printing), new processes
(such as data-driven production and artificial intelligence) and new business models  
",80,18,0.817355437411202,4,0.822807759046555
60,"Legal and accounting activities","Reliable and Interpretable Artificial Intelligence",0.822593450546265,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","The compositional nature of productivity and innovation slowdown Indeed, our task is to understand if the 'data generating process' behind the productivity slowdown 
the socalled within, between, and covariance (or crosslevel) components (or effects  the following:
at the firm level, the within effect is interpreted as learning/innovation (change  
",80,35,0.819798954895565,4,0.816315025091171
67,"Legal and accounting activities","Systems for data science",0.809062600135803,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Measuring Performance of Adaptive Supply Chains The data source for such a system is the reporting of individual units under accounting activities 
does not solely result from the will to satisfy specific customer requirements, but also the necessity
to adjust products to the legal requirements of  5.2 Sample and Data Collection  
",80,21,0.821209547065553,4,0.808902338147163
77,"Legal and accounting activities","Systems for data science",0.827563762664795,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Software robot-based automation of financial administration's processes Task type Support for human labourers Most advanced method Configuration (set rules/learning)
Analysing numbers Business Intelligence, data visualisation  The artificial intelligence is defined
as intelligent behaviour of machines (Ertel 2011, 1; Hutter & Legg 2007, 405)  
",80,21,0.821209547065553,4,0.808902338147163
123,"Accomodation and food service activities","Optimization for Data Science",0.823827087879181,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning Taking advantage of a geo-big data approach and multilevel regression model, we make a
contribution to current literature by  should offer deeper spatial insights into intra-urban foodscape
and provide more nuanced understanding of food deserts  2. Methodology and data. 2.1  
",145,15,0.81723126967748,4,0.817874237895012
127,"Accomodation and food service activities","Optimization for Data Science",0.804906785488129,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","Nutritional Culturomics and Big Data: Macroscopic Patterns of Change in Food, Nutrition and Diet Choices and diet, their knowledge, awareness and understanding of the interdepend- ence of food
consumption, health  [15]), and mathematical tools to handle complex data sets (eg  application
of Artificial Intelligence for large-scale content analysis [18, 19] or random fractal theory to  
",145,15,0.81723126967748,4,0.817874237895012
214,"Accomodation and food service activities","Optimization for Data Science",0.784131586551666,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","[HTML][HTML] Big-data-augmented approach to emerging technologies identification: case of agriculture and food sector <U+041F><U+043E><U+0445><U+043E><U+0436><U+0438><U+0435> <U+043F><U+0443><U+0431><U+043B><U+0438><U+043A><U+0430><U+0446><U+0438><U+0438>. Mapping the Radical Innovations in Food Industry: A Text Mining Study.
Kuzminov I., Bakhtin PD, Khabirova E. et al  I: Advances in Artificial Intelligence and Its Applications 
for gaining insight into the underlying conceptual structure of the data  
",145,15,0.81723126967748,4,0.817874237895012
223,"Accomodation and food service activities","Optimization for Data Science",0.858631491661072,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation."," presents exemplary applications of the smart manufacturing concept in food industry enterprises 
methods and solutions, in- cluding machine learning and artificial intelligence algorithms [Tao 
Traditional analyses use conventional algorithms and data that has been previ- ously  
[CITATION][C]  Demand Models in the Food and Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning ",145,15,0.81723126967748,4,0.817874237895012
235,"Human health and social work activities","Computational Intelligence Lab",0.858721852302551,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","[HTML][HTML] Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives The deep learning based approach we evaluate in this paper does not require any hand 
MIMIC-III contains de-identified clinical data of over 53,000 hospital admissions for adult patients 
approaches then use relevant concepts in a note as input to machine learning algorithms to  
",195,11,0.82310460914265,4,0.813534304499626
304,"Human health and social work activities","Computational Intelligence Lab",0.800876080989838,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","Do citations and readership identify seminal publications? deep learning paper which has caused a shift in the area of artificial intelligence/computer vision 
To do this, we use the threshold which achieves the best accuracy on the training data  The reason
why we chose the this simple model instead of a machine learning model such  
",195,11,0.82310460914265,4,0.813534304499626
309,"Human health and social work activities","Computational Intelligence Lab",0.800435066223145,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","The promise and the challenge of technology-facilitated methods for assessing behavioral and cognitive markers of risk for suicide among US Army National Guard  Salt Lake City, UT 84108, USA 4 Department of Social Work, University of  BSP also involves
additional data processing steps prior to generating behavioral markers  Recent developments
have extended these efforts by incorporating artificial intelligence techniques resulting in  
",195,11,0.82310460914265,4,0.813534304499626
396,"Human health and social work activities","Computational Intelligence Lab",0.794104218482971,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","Efficiency, Correctness, and the Authority of Automation: Technology in College Basic Writing Instruction 101 DATA ANALYSIS  Yet there also are moments of authentic possibility for broader learning
and understanding through the use of the automated system  Virtually all remedial English at
the college level could be handled by automation, with the machine as an impartial judge  
",195,11,0.82310460914265,4,0.813534304499626
458,"Agriculture, forestry and fishing","Bid Data Measuring Systems",0.78180593252182,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","New trends in precision agriculture: a novel cloud-based system for enabling data storage and agricultural task planning and automationIt is well-known that information and communication technologies enable many tasks in the context of precision agriculture. In fact, more and more farmers and food and agriculture companies are using precision agriculture-based systems to enhance not only their products ",180,24,0.826420515775681,4,0.808309257030487
494,"Agriculture, forestry and fishing","Bid Data Measuring Systems",0.798281848430634,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Multi-sensor Data Fusion Algorithm of Wisdom Agriculture Based on Fusion SetIn wisdom agriculture, the advanced high-tech equipment is applied and human input is reduced to lower the operation and management costs and enhance agricultural management efficiency. In this thesis, a multi-sensor data fusion algorithm based on fusion ",180,24,0.826420515775681,4,0.808309257030487
505,"Agriculture, forestry and fishing","Bid Data Measuring Systems",0.823536574840546,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Machine learning based data processing and latency reduction in the internet of things for agricultureThe Internet of Things is best stated as a network of things that have the ability to generate and share information between themselves and interact with the environment according to the percepts from this environment. This network between these devices and humans ",180,24,0.826420515775681,4,0.808309257030487
511,"Agriculture, forestry and fishing","Bid Data Measuring Systems",0.829612672328949,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","[PDF][PDF] Application of Data Warehouse and Big Data Technology in Agriculture in IndiaIn the recent years, it is observed that the scientist, planners, executives across the globe are using data collected from traditional record keeping by government agencies, data collected using sensors and satellite imagery technologies and combining it with predictive weather ",180,24,0.826420515775681,4,0.808309257030487
534,"Agriculture, forestry and fishing","Data visualization",0.818118453025818,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","Research on the Multi-agent Synergic Mechanism for the Opening and Sharing of Big Data in Chinese Agriculture The American agricultural big data system has the characteristics of taking the official data of
the ministry of agriculture as the core and rich data content [6] [7]. In order to integrate public data
of member states, the EU has built a normalized and standardized data sharing  
",180,10,0.81925385594368,4,0.826244339346886
543,"Agriculture, forestry and fishing","Data visualization",0.852119445800781,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","Big data analytics in agriculture and distribution channel with map reduce using ct image analysis provide best result [3] Precision agriculture presents
great  My proposed solution for this all type of problem use big data for distributed computing  as
use bossiness analytical application like pentaho BI that give 3d data visualization with  
",180,10,0.81925385594368,4,0.826244339346886
567,"Agriculture, forestry and fishing","Data visualization",0.842004895210266,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","Visualisation of Big Data in Agriculture and Rural Development There is also plan to implement a graphical user interface to allow user to add his/her
own data without a need of coding. 6.2 Developed applications 3D visualisation is bringing
new potential into analysis of Big data in the field of agriculture  
",180,10,0.81925385594368,4,0.826244339346886
579,"Agriculture, forestry and fishing","Data visualization",0.792734563350677,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","A Big Data Virtualization Role in Agriculture: A Comprehensive Review Page 11. A Big Data Virtualization Role in Agriculture  Fuzzy reasoning provides
uncertainty in both data and output.  Machine learning: It is another artificial intelligence
technique that allows both supervised and unsupervised methods  
",180,10,0.81925385594368,4,0.826244339346886
601,"Arts, entertainment and recreation","Machine Perception",0.845684051513672,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","[PDF][PDF] Augmented Imagination: Machine Learning Art as AutomatismIn one corner, there are designers focusing on applying the strengths of neural networks to the design field. They dream up new,intelligent, generative tools that, for example, help analyze data or produce a thousand variations of a design in an effort to select the best one ",35,24,0.819486998021603,4,0.817227825522423
607,"Arts, entertainment and recreation","Machine Perception",0.783742904663086,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Computer art design based on artificial intelligenceThis article studies the design of computer art based on artificial intelligence in the digital media environment, and mainly discusses the re-creation of the classic animated images. By analyzing and summarizing the animated images, the components needed to form the ",35,24,0.819486998021603,4,0.817227825522423
627,"Arts, entertainment and recreation","Machine Perception",0.839992523193359,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Machine Learning in Supply Chain Planning--When Art & Science Converge.This article discusses the important AI subset of Machine Learning and its application to the area of supply chain planning and optimization. It defines machine learning and how it relates to other advanced analytic methods including AI; predictive, prescriptive and ",35,24,0.819486998021603,4,0.817227825522423
633,"Arts, entertainment and recreation","Machine Perception",0.799491822719574,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","[PDF][PDF] AIA. Artificial intelligence for artWe observe the success of artificial neural networks in simulating human performance on a number of tasks: such as image recognition, natural language processing, etc. However, there are limits to state of-the-art AI that separate it from human-like intelligence. Humans ",35,24,0.819486998021603,4,0.817227825522423
642,"Electricity, gas, steam and air conditioning supply","Statistical Learning Theory",0.805159032344818,"The course covers advanced methods of statistical learning :Statistical learning theory;variational methods and optimization, e.g., maximum entropy techniques, information bottleneck, deterministic and simulated annealing; clustering for vectorial, histogram and relational data; model selection; graphical models.The course surveys recent methods of statistical learning. The fundamentals of machine learning as presented in the course ""Introduction to Machine Learning"" are expanded and in particular, the theory of statistical learning is discussed.        # Theory of estimators: How can we measure the quality of a statistical estimator? We already discussed bias and variance of estimators very briefly, but the interesting part is yet to come.

# Variational methods and optimization: We consider optimization approaches for problems where the optimizer is a probability distribution. Concepts we will discuss in this context include:

* Maximum Entropy
* Information Bottleneck
* Deterministic Annealing

# Clustering: The problem of sorting data into groups without using training samples. This requires a definition of ``similarity'' between data points and adequate optimization procedures.

# Model selection: We have already discussed how to fit a model to a data set in ML I, which usually involved adjusting model parameters for a given type of model. Model selection refers to the question of how complex the chosen model should be. As we already know, simple and complex models both have advantages and drawbacks alike.

# Statistical physics models: approaches for large systems approximate optimization, which originate in the statistical physics (free energy minimization applied to spin glasses and other models); sampling methods based on these models","[PDF][PDF] Machine Learning Based Prediction of Wind Power Electricity Generation from Seasonal Climate Forecasts Institute for Sustainable Economic Development I Johann Baumgartner Status Quo and Aim <U+25AA>
Mainly conceptual models used for this purpose: <U+25AA> High model set up effort incl. bias correction <U+25AA>
Spatially separated interdependencies hard to model <U+25AA> Machine learning models so far only ",105,8,0.840308278799057,4,0.822518065571785
648,"Electricity, gas, steam and air conditioning supply","Statistical Learning Theory",0.797369480133057,"The course covers advanced methods of statistical learning :Statistical learning theory;variational methods and optimization, e.g., maximum entropy techniques, information bottleneck, deterministic and simulated annealing; clustering for vectorial, histogram and relational data; model selection; graphical models.The course surveys recent methods of statistical learning. The fundamentals of machine learning as presented in the course ""Introduction to Machine Learning"" are expanded and in particular, the theory of statistical learning is discussed.        # Theory of estimators: How can we measure the quality of a statistical estimator? We already discussed bias and variance of estimators very briefly, but the interesting part is yet to come.

# Variational methods and optimization: We consider optimization approaches for problems where the optimizer is a probability distribution. Concepts we will discuss in this context include:

* Maximum Entropy
* Information Bottleneck
* Deterministic Annealing

# Clustering: The problem of sorting data into groups without using training samples. This requires a definition of ``similarity'' between data points and adequate optimization procedures.

# Model selection: We have already discussed how to fit a model to a data set in ML I, which usually involved adjusting model parameters for a given type of model. Model selection refers to the question of how complex the chosen model should be. As we already know, simple and complex models both have advantages and drawbacks alike.

# Statistical physics models: approaches for large systems approximate optimization, which originate in the statistical physics (free energy minimization applied to spin glasses and other models); sampling methods based on these models","Forecasting Residential Electricity Demand Through Machine Learning and Model SynthesisThis paper aims to develop a predictive model of residential electricity demand using techniques from statistical science, data analysis and econometrics. Residential energy intensity is investigated as a critical component of demand and evaluated as a predictor of ",105,8,0.840308278799057,4,0.822518065571785
651,"Electricity, gas, steam and air conditioning supply","Statistical Learning Theory",0.826069831848145,"The course covers advanced methods of statistical learning :Statistical learning theory;variational methods and optimization, e.g., maximum entropy techniques, information bottleneck, deterministic and simulated annealing; clustering for vectorial, histogram and relational data; model selection; graphical models.The course surveys recent methods of statistical learning. The fundamentals of machine learning as presented in the course ""Introduction to Machine Learning"" are expanded and in particular, the theory of statistical learning is discussed.        # Theory of estimators: How can we measure the quality of a statistical estimator? We already discussed bias and variance of estimators very briefly, but the interesting part is yet to come.

# Variational methods and optimization: We consider optimization approaches for problems where the optimizer is a probability distribution. Concepts we will discuss in this context include:

* Maximum Entropy
* Information Bottleneck
* Deterministic Annealing

# Clustering: The problem of sorting data into groups without using training samples. This requires a definition of ``similarity'' between data points and adequate optimization procedures.

# Model selection: We have already discussed how to fit a model to a data set in ML I, which usually involved adjusting model parameters for a given type of model. Model selection refers to the question of how complex the chosen model should be. As we already know, simple and complex models both have advantages and drawbacks alike.

# Statistical physics models: approaches for large systems approximate optimization, which originate in the statistical physics (free energy minimization applied to spin glasses and other models); sampling methods based on these models","Clustering Electricity Big Data for Consumption Modeling Using Comparative Strainer Method for High Accuracy Attainment and Dimensionality ReductionIn smart grid, the relation between grid and customer is bidirectional. Therefore, analyzing load consumption patterns is essential for optimal and efficient operation and planning of smart grid in addition to precise load forecasting. However, emergence of the advanced ",105,8,0.840308278799057,4,0.822518065571785
654,"Electricity, gas, steam and air conditioning supply","DD2437 Artificial Neural Networks and Deep Architectures",0.810832798480988,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Clustering Electricity Big Data for Consumption Modeling Using Comparative Strainer Method for High Accuracy Attainment and Dimensionality ReductionIn smart grid, the relation between grid and customer is bidirectional. Therefore, analyzing load consumption patterns is essential for optimal and efficient operation and planning of smart grid in addition to precise load forecasting. However, emergence of the advanced ",105,33,0.829086807641116,4,0.814399033784866
676,"Electricity, gas, steam and air conditioning supply","Statistical Learning Theory",0.861473917961121,"The course covers advanced methods of statistical learning :Statistical learning theory;variational methods and optimization, e.g., maximum entropy techniques, information bottleneck, deterministic and simulated annealing; clustering for vectorial, histogram and relational data; model selection; graphical models.The course surveys recent methods of statistical learning. The fundamentals of machine learning as presented in the course ""Introduction to Machine Learning"" are expanded and in particular, the theory of statistical learning is discussed.        # Theory of estimators: How can we measure the quality of a statistical estimator? We already discussed bias and variance of estimators very briefly, but the interesting part is yet to come.

# Variational methods and optimization: We consider optimization approaches for problems where the optimizer is a probability distribution. Concepts we will discuss in this context include:

* Maximum Entropy
* Information Bottleneck
* Deterministic Annealing

# Clustering: The problem of sorting data into groups without using training samples. This requires a definition of ``similarity'' between data points and adequate optimization procedures.

# Model selection: We have already discussed how to fit a model to a data set in ML I, which usually involved adjusting model parameters for a given type of model. Model selection refers to the question of how complex the chosen model should be. As we already know, simple and complex models both have advantages and drawbacks alike.

# Statistical physics models: approaches for large systems approximate optimization, which originate in the statistical physics (free energy minimization applied to spin glasses and other models); sampling methods based on these models","Machine learning based electricity demand forecastingIn this empirical study we develop forecasting models for electricity demand using publicly available data and three models based on machine learning algorithms. It compares accuracy of these models using different evaluation metrics. The data consist of several ",105,8,0.840308278799057,4,0.822518065571785
677,"Electricity, gas, steam and air conditioning supply","Machine Learning",0.853026926517487,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Machine learning based electricity demand forecastingIn this empirical study we develop forecasting models for electricity demand using publicly available data and three models based on machine learning algorithms. It compares accuracy of these models using different evaluation metrics. The data consist of several ",105,79,0.825409763975988,4,0.80831216275692
683,"Electricity, gas, steam and air conditioning supply","Machine Learning",0.804777264595032,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Sliding Time Window Electricity Consumption Optimization Algorithm for Communities in the Context of Big Data ProcessingBig data frameworks enable companies from various fields to build models that allow them to increase profit margins by improving decision making at different levels (middle management, senior management, and board) or by attempting to boost sales by ",105,79,0.825409763975988,4,0.80831216275692
706,"Electricity, gas, steam and air conditioning supply","DD2437 Artificial Neural Networks and Deep Architectures",0.817347407341003,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[PDF][PDF] Artificial Intelligence and Nord Pool's intraday electricity market Elbas: a demonstration and pragmatic evaluation of employing deep learning for price prediction This thesis demonstrates the use of deep learning for automating hourly price forecasts in continuous intraday electricity markets, using various types of neural networks on comprehensive sequential market data and cutting-edge image processing networks on ",105,33,0.829086807641116,4,0.814399033784866
711,"Electricity, gas, steam and air conditioning supply","Machine Learning",0.79014265537262,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Development of Automation Algorithm for Step of Designing Technology of Static Electricity Protection ClothingThe article presents the research results, which aim at providing an automation algorithm for the step of the designing technology of static electricity protection clothing. Designing the protective clothing is intended to the creation of such a structure, the properties of which ",105,79,0.825409763975988,4,0.80831216275692
719,"Electricity, gas, steam and air conditioning supply","DD2437 Artificial Neural Networks and Deep Architectures",0.813968062400818,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[BOOK][B] Control and Automation Systems for Electricity Distribution Networks (EDN) of the futureThe CIGRÉ C6 Study Committee (Distribution Systems and Dispersed Generation) considers the different aspects of integration of distributed generation. In this context, the JWG C6. 25/B5 has worked to map current functionalities and to identify future needs for the ",105,33,0.829086807641116,4,0.814399033784866
731,"Electricity, gas, steam and air conditioning supply","DD2437 Artificial Neural Networks and Deep Architectures",0.815447866916656,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
"," In many developed countries, such as America, England, Japan and so on those used the
technology of Artificial Intelligence (AI), such as Neural Network, Data Mining, Machine Learning
and so on to apply to use in electricity energy forecasting for gain the best performance  
Study of electricity load forecasting based on multiple kernels learning and weighted support vector regression machine",105,33,0.829086807641116,4,0.814399033784866
736,"Electricity, gas, steam and air conditioning supply","Machine Learning",0.785301804542542,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Machine learning methods for the analysis of data of an Electricity Distribution Network OperatorOnce every few decades an invention changes the landscape of some aspects of our life. Industrial revolutions improved our everyday lives whilst medical revolutions expanded our lifespans. In the path we're leading, most of sciences will be reduced to computer science ",105,79,0.825409763975988,4,0.80831216275692
782,"Financial service activities, except insurance and pension funding","Lab in data science",0.823341071605682,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Signal Processing for Finance, Economics, and Marketing: Concepts, framework, and big data applicationsEconomic data and financial markets are intriguing to researchers working on data and quantitative models. With rapid growth of and increasing access to data in digital form, finance, economics, and marketing data are poised to become one of the most important ",135,78,0.825876659307724,4,0.835735261440277
789,"Financial service activities, except insurance and pension funding","DD2437 Artificial Neural Networks and Deep Architectures",0.826888144016266,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Quantcloud: Enabling big data complex event processing for quantitative finance through a data-driven executionQuantitative Finance (QF) utilizes increasingly sophisticated mathematic models and advanced computer techniques to predict the movement of global markets and price the derivatives. Today, the rise of QF requires an integrated toolchain of enabling technologies ",135,33,0.829086807641116,4,0.837456047534943
793,"Financial service activities, except insurance and pension funding","DD2437 Artificial Neural Networks and Deep Architectures",0.841891229152679,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[PDF][PDF] Machine Learning Projection Methods for Macro-Finance ModelsThis paper develops a global solution method to solve large state space macro-finance models using machine learning. Our new method, an artificial neural network expectation algorithm, is not only considerably faster but also as precise and more scalable than the ",135,33,0.829086807641116,4,0.837456047534943
806,"Financial service activities, except insurance and pension funding","DD2437 Artificial Neural Networks and Deep Architectures",0.872420966625214,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Introduction to the Minitrack on Machine Learning and Network Analytics in FinanceRecent years have seen a rapid evolution of methodologies in artificial intelligence and machine learning, and as a result, increasingly widespread use of these techniques in different domains. One of the most important application areas is finance, offering ",135,33,0.829086807641116,4,0.837456047534943
813,"Financial service activities, except insurance and pension funding","Deep Learning",0.810833990573883,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Introduction to Machine Learning and Network Analytics in Finance MinitrackWe are experiencing enormous growth in the interest of application of various computational methods in finance, which is the consequence of various developments in the last 15 years. As a result, the number and importance of contributions utilizing various machine learning  ",135,13,0.819467407006484,4,0.827626794576645
814,"Financial service activities, except insurance and pension funding","DD2437 Artificial Neural Networks and Deep Architectures",0.808623850345612,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Introduction to Machine Learning and Network Analytics in Finance MinitrackWe are experiencing enormous growth in the interest of application of various computational methods in finance, which is the consequence of various developments in the last 15 years. As a result, the number and importance of contributions utilizing various machine learning  ",135,33,0.829086807641116,4,0.837456047534943
823,"Financial service activities, except insurance and pension funding","Lab in data science",0.861967802047729,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big Data: Epistemological Reflections and Impacts in Finance and Capital Market Studies.Objective and method: Access to data series plays a central role in the area of Finance. The increasing availability of large volumes of data, in different formats and at high frequency, combined with the technological advances in data storage and processing tools, have ",135,78,0.825876659307724,4,0.835735261440277
827,"Financial service activities, except insurance and pension funding","Lab in data science",0.831462919712067,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Research on the Core Competence and Training System of Computer Speciality for Big Data Processing in Colleges and Universities of Finance and EconomicsIn big data era, the computer professionals should be equipped with the ability of big data processing and analysis. And multi-disciplinary collaborative innovation and interdisciplinary cross-learning ability is becoming more and more important. According to ",135,78,0.825876659307724,4,0.835735261440277
845,"Financial service activities, except insurance and pension funding","Deep Learning",0.821755766868591,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Machine Learning for Structured FinanceMachine learning and artificial intelligence have evolved beyond simple hype and have integrated themselves in business and in popular conversation as an increasing number of smart applications profoundly transform the way we work and live. This article defines ",135,13,0.819467407006484,4,0.827626794576645
853,"Financial service activities, except insurance and pension funding","Deep Learning",0.829922199249268,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Essays on machine learning for economics and financeEconometrics and machine learning are quite close and related concepts. Nowadays, it is always more important to extract value from raw data, and distilling actionable insights from quantitative values as well as qualitative features. In order to deal with these topics, the first ",135,13,0.819467407006484,4,0.827626794576645
857,"Financial service activities, except insurance and pension funding","Lab in data science",0.82616925239563,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Survey of Big Data applications: health, education, business & finance, and security & privacyNowadays, Big Data is experiencing an exponential growth in all domains of life. The total amount of data created in the world from the beginning of time up until 2005 is now created every 48 hours! Big Data represents large datasets that cannot be analyzed using traditional ",135,78,0.825876659307724,4,0.835735261440277
874,"Financial service activities, except insurance and pension funding","Deep Learning",0.847995221614838,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Machine learning with applications to financeThe impact of data driven, machine learning technologies across a wide variety of fields is undeniable. The financial industry, which relies heavily on predictive modeling being no exception. In this work we summarize two widely used machine learning models: support ",135,13,0.819467407006484,4,0.827626794576645
904,"Public administration and defence, compulsory social security","A Network Tour of Data Science",0.797984004020691,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Public Administration Curriculum-Based Big Data Policy-Analytic Epistemology: Symbolic IoT Action-Learning Solution ModelThe equilibration that underscores the internet of things (IoT) and big data analytics (BDA) cannot be underestimated at the behest of real-life social challenges and significant policy data generated to redress the concerns of epistemic communities, such as political policy ",110,53,0.824819772873285,4,0.826730042695999
915,"Public administration and defence, compulsory social security","A Network Tour of Data Science",0.868337273597717,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Machine Learning-Based Models for Big Data Analysis and Prediction: Social Security ApplicationsThe core technology of the Fourth Industrial Revolution is artificial intelligence and big data, and the continuous enhancement of algorithm performance through Machine Learning based on large scale accumulated data is an mportant source technology in all fields ",110,53,0.824819772873285,4,0.826730042695999
916,"Public administration and defence, compulsory social security","Data Analystics for Smart Grids",0.867559909820557,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Machine Learning-Based Models for Big Data Analysis and Prediction: Social Security ApplicationsThe core technology of the Fourth Industrial Revolution is artificial intelligence and big data, and the continuous enhancement of algorithm performance through Machine Learning based on large scale accumulated data is an mportant source technology in all fields ",110,67,0.828709795403836,4,0.833848729729652
930,"Public administration and defence, compulsory social security","Data Analystics for Smart Grids",0.833279490470886,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Research on the Optimization of Military Supplies under Big Data BackgroundOn the basis of analyzing the characteristics of big data technology and combining the actual demand of military demand industry in our army, this paper constructs a comprehensive analysis environment of big data for military demand, and on this basis ",110,67,0.828709795403836,4,0.833848729729652
932,"Public administration and defence, compulsory social security","A Network Tour of Data Science",0.830249190330505,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Research on the Optimization of Military Supplies under Big Data BackgroundOn the basis of analyzing the characteristics of big data technology and combining the actual demand of military demand industry in our army, this paper constructs a comprehensive analysis environment of big data for military demand, and on this basis ",110,53,0.824819772873285,4,0.826730042695999
951,"Public administration and defence, compulsory social security","Data Analystics for Smart Grids",0.817866265773773,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Application of Big Data Technology in Scientific Research Data Management of Military EnterprisesScientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology ",110,67,0.828709795403836,4,0.833848729729652
952,"Public administration and defence, compulsory social security","A Network Tour of Data Science",0.810349702835083,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Application of Big Data Technology in Scientific Research Data Management of Military EnterprisesScientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology ",110,53,0.824819772873285,4,0.826730042695999
957,"Public administration and defence, compulsory social security","Data Analystics for Smart Grids",0.816689252853394,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","The Impact of a BIG DATA Decision Support Tool on Military Logistics: MEDICAL ANALYTICS MEETS THE MISSION.Using big data and predictive analytics, more segments of the US military will be able to create decision support tools that help them not only to carry out their missions more efficiently, but also to streamline their logistical requirements. Within the military's medical ",110,67,0.828709795403836,4,0.833848729729652
1030,"Real estate activities","Machine Learning",0.812893927097321,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Artificial intelligence and machine learning: current applications in real estateReal estate meets machine learning: real contribution or just hype? Creating and managing the built environment is a complicated task fraught with difficult decisions, challenging relationships, and a multitude of variables. Today's technology experts are building ",60,79,0.825409763975988,4,0.824364006519318
1033,"Real estate activities","Reliable and Interpretable Artificial Intelligence",0.792808294296265,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Artificial intelligence and machine learning: current applications in real estateReal estate meets machine learning: real contribution or just hype? Creating and managing the built environment is a complicated task fraught with difficult decisions, challenging relationships, and a multitude of variables. Today's technology experts are building ",60,35,0.819798954895565,4,0.824618443846703
1039,"Real estate activities","Reliable and Interpretable Artificial Intelligence",0.851688981056213,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Urban data streams and machine learning: a case of swiss real estate marketIn this paper, we show how using publicly available data streams and machine learning algorithms one can develop practical data driven services with no input from domain experts as a form of prior knowledge. We report the initial steps toward development of a real estate  ",60,35,0.819798954895565,4,0.824618443846703
1040,"Real estate activities","Machine Learning",0.848622679710388,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Comparison of expert algorithms with machine learning models for real estate appraisalMachine learning models require numerous training examples to provide reliable predictions of real estate prices. Expert algorithms could be applied wherever only several training samples are available. The accuracy of two expert algorithms based on the sales ",60,79,0.825409763975988,4,0.824364006519318
1056,"Real estate activities","Machine Learning",0.819772362709045,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Broad application of artificial intelligence for document classification, information extraction and predictive analytics in real estateReal estate represents a major share of economic activities and wealth in all economies. Due to the lack of widely acknowledged standards, however, the structuring, providing and managing of a life cycle-comprehensive building documentation yet remain challenging ",60,79,0.825409763975988,4,0.824364006519318
1064,"Real estate activities","Reliable and Interpretable Artificial Intelligence",0.793657243251801,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","[HTML][HTML] Urban Tech on the Rise: Machine Learning Disrupts the Real Estate Industry. Featuring interviews of: Marc Rutzen and Jasjeet Thind by Stanislas Chaillou The practice of AI-powered Urban Analytics is taking off within the real estate industry. Data science and algorithmic logic are close to the forefront of new urban development practices. How close? is the questionexperts predict that digitization will go far beyond intelligent ",60,35,0.819798954895565,4,0.824618443846703
1068,"Real estate activities","Reliable and Interpretable Artificial Intelligence",0.860319256782532,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","A machine learning approach to big data regression analysis of real estate prices for inferential and predictive purposesThe hedonic price regressions have mainly been used for inference. In contrast, machine learning employed on big data has a great potential for prediction. To contribute to the integration of these two strategies, this article proposes a machine learning approach to the ",60,35,0.819798954895565,4,0.824618443846703
1070,"Real estate activities","Machine Learning",0.816167056560516,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Comparing three machine learning algorithms in the task of appraising commercial real estateIn a unique opportunity to examine rare appraisal data from the commercial real estate sector, the accuracy of three machine learning algorithms is compared in the task of appraising commercial real estate. The algorithms; random forests, support vector ",60,79,0.825409763975988,4,0.824364006519318
3,"Legal and accounting activities","Artificial Intelligence and Data Science",0.814576864242554,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Business intelligence & analytics in management accounting research: Status and future focus that although there is potential for studying business intelligence solutions in  is linked to other
emerging technologies such as big data, machine learning and the Internet  Judgment and
decision-making, Databases, and Expert systems, artificial intelligence and decision-aids  
",80,47,0.827168932620515,3,0.813659866650899
7,"Legal and accounting activities","DD2437 Artificial Neural Networks and Deep Architectures",0.831636726856232,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","[PDF][PDF] Bankruptcy risk prediction models based on artificial neural networks As part of the Artificial Intelligence, the neural networks are systems that use approximation
methods based  31 December 2015 in Romania there were a number of 773781 active legal
entities  978-0-9742114-1-1. Walczak, S.(2001)An empirical analysis of data requirements for  
",80,33,0.829086807641116,3,0.819927374521891
15,"Legal and accounting activities","DD2437 Artificial Neural Networks and Deep Architectures",0.842320263385773,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Zero-Quartile Benchmarking How To Prioritize Digital Technologies In A Companys Transformation Journey Cognitive capabilities are used by applying artificial intelligence (AI) such as machine learningor natural language  It should help to handle the growing amount of data (Agarwal and Dhar 2014)and goes beyond traditional  descriptive  business intelligence (BI), because  
",80,33,0.829086807641116,3,0.819927374521891
17,"Legal and accounting activities","Deep Learning ",0.836007595062256,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Zero-Quartile Benchmarking How To Prioritize Digital Technologies In A Companys Transformation Journey Cognitive capabilities are used by applying artificial intelligence (AI) such as machine learningor natural language  It should help to handle the growing amount of data (Agarwal and Dhar 2014)and goes beyond traditional  descriptive  business intelligence (BI), because  
",80,16,0.825420185923576,3,0.816036661465963
31,"Legal and accounting activities","Data Analysis and Integration",0.801291227340698,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Bits and bolts Real estate 68 Low Low Legal and accounting activities, etc  online renting of data or computing
capacities), as well as back and front office integration systems such as customer relationship
management (CRM), and enterprise resource planning (ERP) software. Page 14  
",80,20,0.825102084875107,3,0.817505717277527
35,"Legal and accounting activities","Data Analysis and Integration",0.850033283233643,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","[BOOK][B] The Routledge Companion to Accounting Information Systems interests include continuous auditing and monitoring, audit data analytics and artificial intelligence
in auditing  process management for AIS in the context of new legal requirements for  constituting
different decision-making environments and what they call data environments  
",80,20,0.825102084875107,3,0.817505717277527
52,"Legal and accounting activities","Artificial Intelligence and Data Science",0.800481677055359,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Tech human resources: a study on the impact of digital technologies on international HR startups The role of technology is spreading in all processes which composed Human Resource function:
form Recruiting to Learning and  resource tech industry are Social, Mobile, Artificial Intelligence,Big Data and Analytics and Cloud  oriented or effectiveness oriented  
",80,47,0.827168932620515,3,0.813659866650899
58,"Legal and accounting activities","Deep Learning ",0.80836945772171,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Adapting business framework conditions to deal with disruptive technologies in Denmark Participation in life-long learning is high but decreasing  Second, the ability to combine new
advanced technologies (such as sensors, advanced robotics and 3D printing), new processes
(such as data-driven production and artificial intelligence) and new business models  
",80,16,0.825420185923576,3,0.816036661465963
64,"Legal and accounting activities","DD2437 Artificial Neural Networks and Deep Architectures",0.785825133323669,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","The compositional nature of productivity and innovation slowdown Indeed, our task is to understand if the 'data generating process' behind the productivity slowdown 
the socalled within, between, and covariance (or crosslevel) components (or effects  the following:
at the firm level, the within effect is interpreted as learning/innovation (change  
",80,33,0.829086807641116,3,0.819927374521891
69,"Legal and accounting activities","Data Analysis and Integration",0.80119264125824,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Measuring Performance of Adaptive Supply Chains The data source for such a system is the reporting of individual units under accounting activities 
does not solely result from the will to satisfy specific customer requirements, but also the necessity
to adjust products to the legal requirements of  5.2 Sample and Data Collection  
",80,20,0.825102084875107,3,0.817505717277527
73,"Legal and accounting activities","Deep Learning ",0.803732931613922,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","[BOOK][B] Decentralized Computing Using Blockchain Technologies and Smart Contracts: Emerging Research and Opportunities: Emerging Research and  With the advent of intelligent systems and self-learning machines, every device require freedom
of  chain, which is computationally very expensive and this makes the blockchain an immutabledata store  can be either an individual or a group of miners or a machine powered with  
",80,16,0.825420185923576,3,0.816036661465963
78,"Legal and accounting activities","Artificial Intelligence and Data Science",0.825921058654785,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Software robot-based automation of financial administration's processes Task type Support for human labourers Most advanced method Configuration (set rules/learning)
Analysing numbers Business Intelligence, data visualisation  The artificial intelligence is defined
as intelligent behaviour of machines (Ertel 2011, 1; Hutter & Legg 2007, 405)  
",80,47,0.827168932620515,3,0.813659866650899
91,"Accomodation and food service activities","Data Analysis and Integration",0.809919536113739,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Facilitating knowledge management through filtered big data: SME competitiveness in an agri-food sector knowledge management (KM) process that utilises filtered big data within an agri-food supply
chain  The specific big data consumer analytics examined is those of the Tesco Clubcard data,
otherwise  17 million customers), with 10 per cent of this customer data being processed  
",145,20,0.825102084875107,3,0.809170802434286
158,"Accomodation and food service activities","Data Analysis and Integration",0.792675793170929,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Design and Realization of Food Safety Monitoring and Pre-Control System Based on Multi-Source and Big Data efficiency,poor timeliness,and incomplete data. In order to realize the sharing of resources and
information in the process of food safety monitoring,this study designed and developed a food
safety monitoring and control system based on the multi-source and big data under the  
",145,20,0.825102084875107,3,0.809170802434286
179,"Accomodation and food service activities","Data Analysis and Integration",0.824917078018188,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Emerging Data Governance Issues in Big Data Applications for Food SafetyThe food industry and food safety authorities show an increasing interest in Big Data applications. On the one hand, Big Data strengthens data storage, data mashup, and methodology of risk assessment; on the other hand, the presence of risks and challenges ",145,20,0.825102084875107,3,0.809170802434286
243,"Human health and social work activities","Optimization for Data Science",0.871821582317352,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","Comparing rule-based and deep learning models for patient phenotyping modern methods use relevant concepts in a note as input to a machine learning algorithm to  As
we mentioned before, the goal with our data is to understand phenotypes that are  validation of
this approach in other types of clinical notes such as social work assessment to  
",195,15,0.81723126967748,3,0.813406884670258
252,"Human health and social work activities","Bayesian Networks",0.835737586021423,"This module presents Bayesian Networks as graphic tools which are well consolidated and of wide use nowadays to model uncertainty and reason with in intelligent systems. Uncertainty is modelled with probabilities and reasoning is based on Bayes’ rule. It begins by explaining the meaning of the networks to model reasoning with uncertainty, both casual and non-casual, and both from a structural (qualitative) point of view and parametric (quantitative). The next step is to pose questions to the network, in other words, to infer knowledge from observations or data that is being collected. Thus, we can ask, for instance, for the diagnosis of a disease or the most likely explanation for the observed evidence. The algorithms can obtain the exact or an approximate answer, in the latter case probably using Monte Carlo simulation. The network is built by analysing the problem with an expert, but can also be induced from a database. This is a current issue: how to obtain a structure and parameters for the network and for that machine learning methods will be discussed. Finally, by knowing how to build the network and how to use it to perform queries, it will be possible to see its application on decision making and other applications of great interest within Artificial Intelligence: computer vision, automatic classification, filtering of email, etc.","Can Predictive Algorithms Assist Decision-Making in Social Work with Children and Families? (1985) explored the potential of what they refer to as 'artificial intelligence' to develop 'expert  every
time' Decision Support Systems and Child and Family Social Work  In another instance where
a big data approach has been developed but has yet to be applied, Schwartz et al  
",195,5,0.810826969146728,3,0.815676748752594
260,"Human health and social work activities","Deep Learning ",0.869164407253265,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","[PDF][PDF] Digital economics Strong artificial intelligence  Deep learning A machine learning technique that learns features
and tasks directly from data using an architecture of layers of neural networks. Big data Refers
to voluminous amounts of structured or unstructured data  
",195,16,0.825420185923576,3,0.836465895175934
267,"Human health and social work activities","Deep Learning ",0.835228979587555,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","[BOOK][B] Evidence-Based Decision-Making: How to Leverage Available Data and Avoid Cognitive Biases often felt like the proverbial salmon swimming upstream, especially when my data analytic findings 
a typically large set of possibilities) nature, chess naturally lends itself to machine learning in
the  systems roughly parallels the history of what is known as 'artificial intelligence' (AI  
",195,16,0.825420185923576,3,0.836465895175934
270,"Human health and social work activities","Web Science",0.845393776893616,"Web Science studies the phenomena related to the analysis and design of sociotechnical systems. Sociology plays an important role in the design of the web of the future. This course introduces the principles of web science. The design systems used in web science are presented, including information retrieval mechanisms, recommender systems and sentiment analysis systems. Then, the terms Social Computing and Citizen Science are defined, paying special attention to artificial societies and trust and reputation mechanisms. Finally, social decision-making mechanisms based on preference aggregation are revised.","[BOOK][B] Handbook of Research Methods in Complexity Science: Theory and Applications in complexity science Emily S. Ihara is an Associate Professor of Social Work at George  on brain
dynamics and structure by analys- ing fMRI and EEG data and he is  His research includes
applications in Social and economic systems, business, artificial intelligence and robotics  
",195,7,0.813184056963239,3,0.821716646353404
273,"Human health and social work activities","Optimization for Data Science",0.783450126647949,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","[BOOK][B] Handbook of Research Methods in Complexity Science: Theory and Applications in complexity science Emily S. Ihara is an Associate Professor of Social Work at George  on brain
dynamics and structure by analys- ing fMRI and EEG data and he is  His research includes
applications in Social and economic systems, business, artificial intelligence and robotics  
",195,15,0.81723126967748,3,0.813406884670258
282,"Human health and social work activities","Big Data, Law, and Policy",0.824451386928558,"        This course introduces students to societal perspectives on the big data revolution. Discussing important contributions from machine learning and data science, the course explores their legal, economic, ethical, and political implications in the past, present, and future.        This course is intended both for students of machine learning and data science who want to reflect on the societal implications of their field, and for students from other disciplines who want to explore the societal impact of data sciences. The course will first discuss some of the methodological foundations of machine learning, followed by a discussion of research papers and real-world applications where big data and societal values may clash. Potential topics include the implications of big data for privacy, liability, insurance, health systems, voting, and democratic institutions, as well as the use of predictive algorithms for price discrimination and the criminal justice system. Guest speakers, weekly readings and reaction papers ensure a lively debate among participants from various backgrounds.","[BOOK][B] The future of work in Australia: anticipating how new technologies will reshape labour markets, occupations and skill requirements He holds qualifications in political science, social policy, and social work  recent developments
in information and communication technology (ICT), computer-based technologies (CBT) andartificial intelligence (AI) have  Big data has helped to facilitate significant advances in AI  
",195,6,0.817278911670049,3,0.815951367219289
283,"Human health and social work activities","Web Science",0.82333505153656,"Web Science studies the phenomena related to the analysis and design of sociotechnical systems. Sociology plays an important role in the design of the web of the future. This course introduces the principles of web science. The design systems used in web science are presented, including information retrieval mechanisms, recommender systems and sentiment analysis systems. Then, the terms Social Computing and Citizen Science are defined, paying special attention to artificial societies and trust and reputation mechanisms. Finally, social decision-making mechanisms based on preference aggregation are revised.","[BOOK][B] The future of work in Australia: anticipating how new technologies will reshape labour markets, occupations and skill requirements He holds qualifications in political science, social policy, and social work  recent developments
in information and communication technology (ICT), computer-based technologies (CBT) andartificial intelligence (AI) have  Big data has helped to facilitate significant advances in AI  
",195,7,0.813184056963239,3,0.821716646353404
299,"Human health and social work activities","Systems for data science",0.781096935272217,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Health maintenance advisory technology for specific business sectors, eg utilities or tourism; G06Q50/10Services; G06Q50/22Socialwork  be passed to the query engine 72 as a confirmed result for machine learning algorithms
that  The user condition may be sensed by accessing such data from the server system, at  
",195,21,0.821209547065553,3,0.812609076499939
302,"Human health and social work activities","Model Driven Engineering",0.809337794780731,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","Do citations and readership identify seminal publications? deep learning paper which has caused a shift in the area of artificial intelligence/computer vision 
To do this, we use the threshold which achieves the best accuracy on the training data  The reason
why we chose the this simple model instead of a machine learning model such  
",195,9,0.815234687593248,3,0.8056813677152
310,"Human health and social work activities","Deep Learning ",0.805004298686981,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","A Cognitive Perspective For example, when thinking about artificial intelligence (AI) in healthcare, Christopher Khoury,
vice president of  was driven by the question of What if you could use data science to  around those
tendencies? Using more than 700 variables and machine learning, the company  
",195,16,0.825420185923576,3,0.836465895175934
314,"Human health and social work activities","Bayesian Networks",0.794820964336395,"This module presents Bayesian Networks as graphic tools which are well consolidated and of wide use nowadays to model uncertainty and reason with in intelligent systems. Uncertainty is modelled with probabilities and reasoning is based on Bayes’ rule. It begins by explaining the meaning of the networks to model reasoning with uncertainty, both casual and non-casual, and both from a structural (qualitative) point of view and parametric (quantitative). The next step is to pose questions to the network, in other words, to infer knowledge from observations or data that is being collected. Thus, we can ask, for instance, for the diagnosis of a disease or the most likely explanation for the observed evidence. The algorithms can obtain the exact or an approximate answer, in the latter case probably using Monte Carlo simulation. The network is built by analysing the problem with an expert, but can also be induced from a database. This is a current issue: how to obtain a structure and parameters for the network and for that machine learning methods will be discussed. Finally, by knowing how to build the network and how to use it to perform queries, it will be possible to see its application on decision making and other applications of great interest within Artificial Intelligence: computer vision, automatic classification, filtering of email, etc.","A Cognitive Perspective For example, when thinking about artificial intelligence (AI) in healthcare, Christopher Khoury,
vice president of  was driven by the question of What if you could use data science to  around those
tendencies? Using more than 700 variables and machine learning, the company  
",195,5,0.810826969146728,3,0.815676748752594
317,"Human health and social work activities","Model Driven Engineering",0.82610672712326,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","Methods and systems for growing and retaining the value of brand drugs by computer predictive model methods specially adapted for specific business sectors, eg utilities or tourism; G06Q50/10
Services; G06Q50/22Social work  [0042]. Learning Machinerefers to a  or semi-automated
process of generating a prediction based on a model, typically combining software and data  
",195,9,0.815234687593248,3,0.8056813677152
345,"Human health and social work activities","Big Data, Law, and Policy",0.808859705924988,"        This course introduces students to societal perspectives on the big data revolution. Discussing important contributions from machine learning and data science, the course explores their legal, economic, ethical, and political implications in the past, present, and future.        This course is intended both for students of machine learning and data science who want to reflect on the societal implications of their field, and for students from other disciplines who want to explore the societal impact of data sciences. The course will first discuss some of the methodological foundations of machine learning, followed by a discussion of research papers and real-world applications where big data and societal values may clash. Potential topics include the implications of big data for privacy, liability, insurance, health systems, voting, and democratic institutions, as well as the use of predictive algorithms for price discrimination and the criminal justice system. Guest speakers, weekly readings and reaction papers ensure a lively debate among participants from various backgrounds.","[PDF][PDF] The future of well-being in a tech-saturated world Dangers: Tiziana Dearing, a professor at the Boston College School of Social Work, said, People's
well  lives, technology companies will find new, invasive ways to exploit data generated on  As
we enter the Artificial Intelligence era we must examine and make transparent how  
",195,6,0.817278911670049,3,0.815951367219289
363,"Human health and social work activities","Systems for data science",0.825657784938812,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Tracking and Controlling Inter-System Processing Events Using Event Tokens or tourism; G06Q50/10Services; G06Q50/22Health care, eg hospitals; Social work;  disclosure
relate to data processing, artificial intelligence, and using artificial intelligence-enabled data  event
token management computing platform 110 may receive adjudication data from a  
",195,21,0.821209547065553,3,0.812609076499939
388,"Human health and social work activities","Systems for data science",0.831072509288788,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","[PDF][PDF] Coding Over the Cracks: Predictive Analytics and Child Protection families and families of color.3 In this age of automation and artificial intelligence, a tempting  are
building and deploying tools that pull together vast quantities of data stored by  analytics and
explains the fundamentally human process of developing a machine learning algorithm  
",195,21,0.821209547065553,3,0.812609076499939
390,"Human health and social work activities","Big Data, Law, and Policy",0.814543008804321,"        This course introduces students to societal perspectives on the big data revolution. Discussing important contributions from machine learning and data science, the course explores their legal, economic, ethical, and political implications in the past, present, and future.        This course is intended both for students of machine learning and data science who want to reflect on the societal implications of their field, and for students from other disciplines who want to explore the societal impact of data sciences. The course will first discuss some of the methodological foundations of machine learning, followed by a discussion of research papers and real-world applications where big data and societal values may clash. Potential topics include the implications of big data for privacy, liability, insurance, health systems, voting, and democratic institutions, as well as the use of predictive algorithms for price discrimination and the criminal justice system. Guest speakers, weekly readings and reaction papers ensure a lively debate among participants from various backgrounds.","[BOOK][B] Trends and Issues in Interdisciplinary Behavior and Social Science: Proceedings of the 5th International Congress on Interdisciplinary Behavior and Social  light on researches about depression, especially in the Malaysian context as there is a big lack
of  Besides, the systematic data collection and analysis method are suggested to be replicated
for other phenomenological studies  Journal of Social Work Practice, 13(2), 135145  
",195,6,0.817278911670049,3,0.815951367219289
391,"Human health and social work activities","Web Science",0.796421110630035,"Web Science studies the phenomena related to the analysis and design of sociotechnical systems. Sociology plays an important role in the design of the web of the future. This course introduces the principles of web science. The design systems used in web science are presented, including information retrieval mechanisms, recommender systems and sentiment analysis systems. Then, the terms Social Computing and Citizen Science are defined, paying special attention to artificial societies and trust and reputation mechanisms. Finally, social decision-making mechanisms based on preference aggregation are revised.","[BOOK][B] Trends and Issues in Interdisciplinary Behavior and Social Science: Proceedings of the 5th International Congress on Interdisciplinary Behavior and Social  light on researches about depression, especially in the Malaysian context as there is a big lack
of  Besides, the systematic data collection and analysis method are suggested to be replicated
for other phenomenological studies  Journal of Social Work Practice, 13(2), 135145  
",195,7,0.813184056963239,3,0.821716646353404
394,"Human health and social work activities","Optimization for Data Science",0.784948945045471,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","[BOOK][B] Trends and Issues in Interdisciplinary Behavior and Social Science: Proceedings of the 5th International Congress on Interdisciplinary Behavior and Social  light on researches about depression, especially in the Malaysian context as there is a big lack
of  Besides, the systematic data collection and analysis method are suggested to be replicated
for other phenomenological studies  Journal of Social Work Practice, 13(2), 135145  
",195,15,0.81723126967748,3,0.813406884670258
399,"Human health and social work activities","Model Driven Engineering",0.781599581241608,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","Efficiency, Correctness, and the Authority of Automation: Technology in College Basic Writing Instruction 101 DATA ANALYSIS  Yet there also are moments of authentic possibility for broader learning
and understanding through the use of the automated system  Virtually all remedial English at
the college level could be handled by automation, with the machine as an impartial judge  
",195,9,0.815234687593248,3,0.8056813677152
419,"Human health and social work activities","Bayesian Networks",0.816471695899963,"This module presents Bayesian Networks as graphic tools which are well consolidated and of wide use nowadays to model uncertainty and reason with in intelligent systems. Uncertainty is modelled with probabilities and reasoning is based on Bayes’ rule. It begins by explaining the meaning of the networks to model reasoning with uncertainty, both casual and non-casual, and both from a structural (qualitative) point of view and parametric (quantitative). The next step is to pose questions to the network, in other words, to infer knowledge from observations or data that is being collected. Thus, we can ask, for instance, for the diagnosis of a disease or the most likely explanation for the observed evidence. The algorithms can obtain the exact or an approximate answer, in the latter case probably using Monte Carlo simulation. The network is built by analysing the problem with an expert, but can also be induced from a database. This is a current issue: how to obtain a structure and parameters for the network and for that machine learning methods will be discussed. Finally, by knowing how to build the network and how to use it to perform queries, it will be possible to see its application on decision making and other applications of great interest within Artificial Intelligence: computer vision, automatic classification, filtering of email, etc.","Future Implications of the Psychopathy Construct for Criminology and Criminal Justice Policy and Practice Torture, 6 For example, Rhodes (2002) provides participation observation data showing that 
Sensors and machine learning algorithms have been designed that can measure affective
information  In addition, artificial intelligence advances that can make robots feel so to speak  
",195,5,0.810826969146728,3,0.815676748752594
437,"Agriculture, forestry and fishing","Statistical Methods in Data Mining",0.801384091377258,"Show the potential of statistical methods in data mining, with particular emphasis on classification, clustering, dimensionality reduction, anomaly detection and partial least squares methods. Develop the ability to apply statistical procedures to the analysis of large data sets, and to show how important those procedures are in decision making. Analyse real problems with specific software and identify suitable methodologies to deal with such problems. By the end of the semester, the students should know the main statistical procedures associated to data mining, and be familiar with other data mining techniques on a user level basis.Introduction. Data Mining Overview.Exploring data: Preprocessing, Visualization and Data Quality Classification

Classification Methods
- Classification with K-Nearest Neighbours
- Classification and Bayes Rule, Naïve Bayes
- Classification Trees
- Discriminant Analysis
- Logistic Regression
Evaluating the Performance of a Classifier
Comparing Classifiers

Clustering
Clustering Methods
- K-Means Clustering, Hierarchical Clustering
- EM for Mixture Model Density Estimation
Cluster Validation

Dimensionality Reduction
Principal Components
Independent Component Analysis
Multidimensional Scaling

Anomaly Detection
Preliminaries
Detecting Outliers
Evaluating the Performance of an Anomaly Detection Rule

Partial Least Squares
Introduction: More Variables than Objects
Partial Least Squares Regression","IoT based agriculture as a cloud and big data service: the beginning of digital India Agriculture System Mechanism QoS-aware (Parameter) Domains Data Classification Resource
Management Big Data  thedifferent classlabelsofusers.K-NNissupervisedmachinelearning
techniquewhich  Thefinalstepistointerprettheagriculturedatasubmittedbydifferentusersof  
",180,5,0.810140144824982,3,0.798100252946218
464,"Agriculture, forestry and fishing","Statistical Methods in Data Mining",0.790042161941528,"Show the potential of statistical methods in data mining, with particular emphasis on classification, clustering, dimensionality reduction, anomaly detection and partial least squares methods. Develop the ability to apply statistical procedures to the analysis of large data sets, and to show how important those procedures are in decision making. Analyse real problems with specific software and identify suitable methodologies to deal with such problems. By the end of the semester, the students should know the main statistical procedures associated to data mining, and be familiar with other data mining techniques on a user level basis.Introduction. Data Mining Overview.Exploring data: Preprocessing, Visualization and Data Quality Classification

Classification Methods
- Classification with K-Nearest Neighbours
- Classification and Bayes Rule, Naïve Bayes
- Classification Trees
- Discriminant Analysis
- Logistic Regression
Evaluating the Performance of a Classifier
Comparing Classifiers

Clustering
Clustering Methods
- K-Means Clustering, Hierarchical Clustering
- EM for Mixture Model Density Estimation
Cluster Validation

Dimensionality Reduction
Principal Components
Independent Component Analysis
Multidimensional Scaling

Anomaly Detection
Preliminaries
Detecting Outliers
Evaluating the Performance of an Anomaly Detection Rule

Partial Least Squares
Introduction: More Variables than Objects
Partial Least Squares Regression","From the Dust Bowl to Drones to Big Data: The Next Revolution in Agriculture Precision Agriculture and Big Data While precision agriculture (PA) and big data are related,
they are not  across years) variability asso- ciated with all aspects of agricultural pro- duction (Figure
1). Big data refers to the collection, analysis, and synthesis of large data sets that  
",180,5,0.810140144824982,3,0.798100252946218
478,"Agriculture, forestry and fishing","Statistical Methods in Data Mining",0.802874505519867,"Show the potential of statistical methods in data mining, with particular emphasis on classification, clustering, dimensionality reduction, anomaly detection and partial least squares methods. Develop the ability to apply statistical procedures to the analysis of large data sets, and to show how important those procedures are in decision making. Analyse real problems with specific software and identify suitable methodologies to deal with such problems. By the end of the semester, the students should know the main statistical procedures associated to data mining, and be familiar with other data mining techniques on a user level basis.Introduction. Data Mining Overview.Exploring data: Preprocessing, Visualization and Data Quality Classification

Classification Methods
- Classification with K-Nearest Neighbours
- Classification and Bayes Rule, Naïve Bayes
- Classification Trees
- Discriminant Analysis
- Logistic Regression
Evaluating the Performance of a Classifier
Comparing Classifiers

Clustering
Clustering Methods
- K-Means Clustering, Hierarchical Clustering
- EM for Mixture Model Density Estimation
Cluster Validation

Dimensionality Reduction
Principal Components
Independent Component Analysis
Multidimensional Scaling

Anomaly Detection
Preliminaries
Detecting Outliers
Evaluating the Performance of an Anomaly Detection Rule

Partial Least Squares
Introduction: More Variables than Objects
Partial Least Squares Regression","0245 Big data and occupational health vigilance: use of french medico-administrative databases for hypothesis generation regarding occupational risks in agriculture Poster Presentation. Methodology. 0245 Big data and occupational health vigilance: use of 
medico-administrative databases for hypothesis generation regarding occupational risks inagriculture  complementary methods relying on exploitation of already existing data, such as  
",180,5,0.810140144824982,3,0.798100252946218
518,"Agriculture, forestry and fishing","Systems for data science",0.814881920814514,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Research and Application of Spark Platform on Big Data Processing in Intelligent Agriculture of Jilin Province (2) The agricultural big data processing method of intelligent agriculture in Jilin  algorithm
characterized by dynamic and rapid expansion, combining the Spark streaming flow calculation
framework, able to real-time analyze continuous and rapid changes in the massive data  
",180,21,0.821209547065553,3,0.829106052716573
529,"Agriculture, forestry and fishing","Systems for data science",0.831108629703522,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","[PDF][PDF] A review on big data analytics in the field of agriculture Processing OLTP Analytical Big data Processing  We plan to work on precisionagriculture techniques. DISTRIBUTED NOSQL DATABASE FILE SYSTEM
PROGRAMMING COLUMN-DATA MODEL DOCUMENT-DATA MODEL  
",180,21,0.821209547065553,3,0.829106052716573
568,"Agriculture, forestry and fishing","Systems for data science",0.841327607631683,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Visualisation of Big Data in Agriculture and Rural Development There is also plan to implement a graphical user interface to allow user to add his/her
own data without a need of coding. 6.2 Developed applications 3D visualisation is bringing
new potential into analysis of Big data in the field of agriculture  
",180,21,0.821209547065553,3,0.829106052716573
600,"Arts, entertainment and recreation","Reliable and Interpretable Artificial Intelligence",0.846567511558533,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","[PDF][PDF] Augmented Imagination: Machine Learning Art as AutomatismIn one corner, there are designers focusing on applying the strengths of neural networks to the design field. They dream up new,intelligent, generative tools that, for example, help analyze data or produce a thousand variations of a design in an effort to select the best one ",35,35,0.819798954895565,3,0.809351762135824
613,"Arts, entertainment and recreation","Reliable and Interpretable Artificial Intelligence",0.791430413722992,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","A System for Evolving Art Using Supervised Learning and Aesthetic AnalogiesAesthetic experience is an important aspect of creativity and our perception of the world around us. Analogy is a tool we use as part of the creative process to translate our perceptions into creative works of art. In this paper we present our research on the ",35,35,0.819798954895565,3,0.809351762135824
618,"Arts, entertainment and recreation","Reliable and Interpretable Artificial Intelligence",0.790057361125946,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Art Design Scheme of Modern Urban Commercial Public Space Using Artificial Intelligence TechnologyThis article takes the urban public environment facilities as the research object, and cites the related subject theory as well as the mature research methods at home and abroad, to explore the specific characteristics of the commercial public space. We sum up the main ",35,35,0.819798954895565,3,0.809351762135824
640,"Electricity, gas, steam and air conditioning supply","Reliable and Interpretable Artificial Intelligence",0.824550032615662,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","[PDF][PDF] Machine Learning Based Prediction of Wind Power Electricity Generation from Seasonal Climate Forecasts Institute for Sustainable Economic Development I Johann Baumgartner Status Quo and Aim <U+25AA>
Mainly conceptual models used for this purpose: <U+25AA> High model set up effort incl. bias correction <U+25AA>
Spatially separated interdependencies hard to model <U+25AA> Machine learning models so far only ",105,35,0.819798954895565,3,0.81157108147939
644,"Electricity, gas, steam and air conditioning supply","Time Series Analysis",0.800567924976349,"This course examines models and statistical techniques used to study time series data. The main objective is to equip students with the methods and software tools they need for carrying out state-of-the-art empirical research on time series, with emphasis on applications in economics and finance.Basics aspects of time-domain and frequency-domain methods, methods for model-based estimation, model selection, diagnostics, forecasting, and computing as they relate to time series analysis. ARMA and seasonal ARIMA models, the Box-Jenkins approach for SARIMA modelling, spectral analysis, computing forecast for a variety of linear methods and models, nonlinear models, ARCH/GARCH models, risk models, state space models and the Kalman Filter, Monte Carlo simulation and other advanced topics if time permitted.","[PDF][PDF] Machine Learning Based Prediction of Wind Power Electricity Generation from Seasonal Climate Forecasts Institute for Sustainable Economic Development I Johann Baumgartner Status Quo and Aim <U+25AA>
Mainly conceptual models used for this purpose: <U+25AA> High model set up effort incl. bias correction <U+25AA>
Spatially separated interdependencies hard to model <U+25AA> Machine learning models so far only ",105,6,0.835573434829712,3,0.821572403113047
646,"Electricity, gas, steam and air conditioning supply","Time Series Analysis",0.800751388072968,"This course examines models and statistical techniques used to study time series data. The main objective is to equip students with the methods and software tools they need for carrying out state-of-the-art empirical research on time series, with emphasis on applications in economics and finance.Basics aspects of time-domain and frequency-domain methods, methods for model-based estimation, model selection, diagnostics, forecasting, and computing as they relate to time series analysis. ARMA and seasonal ARIMA models, the Box-Jenkins approach for SARIMA modelling, spectral analysis, computing forecast for a variety of linear methods and models, nonlinear models, ARCH/GARCH models, risk models, state space models and the Kalman Filter, Monte Carlo simulation and other advanced topics if time permitted.","Forecasting Residential Electricity Demand Through Machine Learning and Model SynthesisThis paper aims to develop a predictive model of residential electricity demand using techniques from statistical science, data analysis and econometrics. Residential energy intensity is investigated as a critical component of demand and evaluated as a predictor of ",105,6,0.835573434829712,3,0.821572403113047
675,"Electricity, gas, steam and air conditioning supply","Time Series Analysis",0.863397896289825,"This course examines models and statistical techniques used to study time series data. The main objective is to equip students with the methods and software tools they need for carrying out state-of-the-art empirical research on time series, with emphasis on applications in economics and finance.Basics aspects of time-domain and frequency-domain methods, methods for model-based estimation, model selection, diagnostics, forecasting, and computing as they relate to time series analysis. ARMA and seasonal ARIMA models, the Box-Jenkins approach for SARIMA modelling, spectral analysis, computing forecast for a variety of linear methods and models, nonlinear models, ARCH/GARCH models, risk models, state space models and the Kalman Filter, Monte Carlo simulation and other advanced topics if time permitted.","Machine learning based electricity demand forecastingIn this empirical study we develop forecasting models for electricity demand using publicly available data and three models based on machine learning algorithms. It compares accuracy of these models using different evaluation metrics. The data consist of several ",105,6,0.835573434829712,3,0.821572403113047
709,"Electricity, gas, steam and air conditioning supply","Reliable and Interpretable Artificial Intelligence",0.810718655586243,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","[PDF][PDF] Artificial Intelligence and Nord Pool's intraday electricity market Elbas: a demonstration and pragmatic evaluation of employing deep learning for price prediction This thesis demonstrates the use of deep learning for automating hourly price forecasts in continuous intraday electricity markets, using various types of neural networks on comprehensive sequential market data and cutting-edge image processing networks on ",105,35,0.819798954895565,3,0.81157108147939
733,"Electricity, gas, steam and air conditioning supply","Reliable and Interpretable Artificial Intelligence",0.799444556236267,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture."," In many developed countries, such as America, England, Japan and so on those used the
technology of Artificial Intelligence (AI), such as Neural Network, Data Mining, Machine Learning
and so on to apply to use in electricity energy forecasting for gain the best performance  
Study of electricity load forecasting based on multiple kernels learning and weighted support vector regression machine",105,35,0.819798954895565,3,0.81157108147939
743,"Financial service activities, except insurance and pension funding","Optimization for Data Science",0.824290812015533,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","Machine learning in finance: A topic modeling approachWe provide a first comprehensive structuring of the literature applying machine learning to finance. We use a probabilistic topic modeling approach to make sense of this diverse body of research spanning across the disciplines of finance, economics, computer sciences, and ",135,15,0.81723126967748,3,0.818863610426585
745,"Financial service activities, except insurance and pension funding","Information security and privacy",0.811107993125916,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","Expert Systems in Finance: Smart Financial Applications in Big Data EnvironmentsThroughout the industry, financial institutions seek to eliminate cumbersome authentication methods, such as PINs, passwords, and security questions, as these antiquated tactics prove increasingly weak. Thus, many organizations now aim to implement emerging ",135,10,0.806313174962997,3,0.804523984591166
752,"Financial service activities, except insurance and pension funding","Optimization for Data Science",0.81882655620575,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","Machine learning for quantitative finance: fast derivative pricing, hedging and fittingIn this paper, we show how we can deploy machine learning techniques in the context of traditional quant problems. We illustrate that for many classical problems, we can arrive at speed-ups of several orders of magnitude by deploying machine learning techniques based ",135,15,0.81723126967748,3,0.818863610426585
777,"Financial service activities, except insurance and pension funding","Systems for data science",0.837347686290741,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Big Data Analytics and Visualization: FinanceAll finance institutions have seen an explosion in their velocity, variety and volume of their internal 
datasets. New federal regulations requirement require leveraging internal and external data 
linking: [1] Customer service and transactional level data; [2] Social Media activity analysis (Sentimental ",135,21,0.821209547065553,3,0.841746191183726
788,"Financial service activities, except insurance and pension funding","Systems for data science",0.827805161476135,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Quantcloud: Enabling big data complex event processing for quantitative finance through a data-driven executionQuantitative Finance (QF) utilizes increasingly sophisticated mathematic models and advanced computer techniques to predict the movement of global markets and price the derivatives. Today, the rise of QF requires an integrated toolchain of enabling technologies ",135,21,0.821209547065553,3,0.841746191183726
797,"Financial service activities, except insurance and pension funding","Information security and privacy",0.809850931167603,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","Application of information systems aimed at big data use in the sphere of state finance management: Concept schemeThe article is devoted to the application of big data technologies in public administration, in particular, in public financial management. The paper outlines the basic principles of effective public financial management and describes the development trends of state ",135,10,0.806313174962997,3,0.804523984591166
805,"Financial service activities, except insurance and pension funding","Reliable and Interpretable Artificial Intelligence",0.876032769680023,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Introduction to the Minitrack on Machine Learning and Network Analytics in FinanceRecent years have seen a rapid evolution of methodologies in artificial intelligence and machine learning, and as a result, increasingly widespread use of these techniques in different domains. One of the most important application areas is finance, offering ",135,35,0.819798954895565,3,0.850226481755575
816,"Financial service activities, except insurance and pension funding","Reliable and Interpretable Artificial Intelligence",0.853184342384338,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","The Future of Fuzzy Sets in Finance: New Challenges in Machine Learning and Explainable AITraditional statistical analysis is oriented towards finding linear relationships between the variables under investigation, often accompanied by strict assumptions about the problem and data distributions. Moreover, traditional analysis endorses data reduction as much as ",135,35,0.819798954895565,3,0.850226481755575
824,"Financial service activities, except insurance and pension funding","Systems for data science",0.860085725784302,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Big Data: Epistemological Reflections and Impacts in Finance and Capital Market Studies.Objective and method: Access to data series plays a central role in the area of Finance. The increasing availability of large volumes of data, in different formats and at high frequency, combined with the technological advances in data storage and processing tools, have ",135,21,0.821209547065553,3,0.841746191183726
838,"Financial service activities, except insurance and pension funding","Optimization for Data Science",0.813473463058472,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","Influence of contextual factors on the adoption process of Robotic process automation (RPA): Case study at Stora Enso Finance DeliveryThe introduction will first describe the theoretical background for the research, followed by origin of the study and problematizing. These considerations are used to define the purpose statement and research questions. The chapter concludes with limitations and outline for the ",135,15,0.81723126967748,3,0.818863610426585
843,"Financial service activities, except insurance and pension funding","Information security and privacy",0.79261302947998,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","Reflection on Big Data Technology: Problems and Countermeasures in"" Big Data Credit Reporting"" of Internet Finance in ChinaWith the rapid development of Internet finance in China for the past few years, big data credit reporting agencies specifically for network credit information have been initially established. An analysis on the technology application, characteristics and operational difficulties of big  ",135,10,0.806313174962997,3,0.804523984591166
846,"Financial service activities, except insurance and pension funding","Reliable and Interpretable Artificial Intelligence",0.821462333202362,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","Machine Learning for Structured FinanceMachine learning and artificial intelligence have evolved beyond simple hype and have integrated themselves in business and in popular conversation as an increasing number of smart applications profoundly transform the way we work and live. This article defines ",135,35,0.819798954895565,3,0.850226481755575
883,"Public administration and defence, compulsory social security","Automated Planning",0.793143153190613,"Automated planning is a branch of Artificial intelligence aimed at obtaining plans (i.e. sequences of actions) for solving complex problems or for governing the behavior of intelligent agents, autonomous robots or unmanned vehicles. Planning techniques have been successfully applied in different domains, including industrial contexts, logistics, computer games, robotics or space exploration. In this seminar we will review the existing approaches for solving classical planning problems, such as state-space search, plan-space search, graph-based techniques or turning classical planning problems into propositional satisfiability problems. The course will then focus on the study of knowledge-based planning methods, such as control rule-based pruning or hierarchical task network-based planning techniques. These approaches exploit the domain knowledge provided by human experts to improve the performance of the planning algorithms. Finally, we will briefly introduce advanced planning algorithms, which are able to generate planning policies that take into account time constraints and/or partial observability conditions, which are common in real world applications.","The application of artificial intelligence in public administration for forecasting high crime risk transportation areas in urban environmentPublic administration has adopted information and communication technology in order to construct new intelligent systems and design new risk prevention strategies in transportation management. The ultimate goal is to improve the quality of the transportation services and ",110,5,0.809059846401215,3,0.804516156514486
889,"Public administration and defence, compulsory social security","Machine Perception",0.842807412147522,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Machine learning for public administration research, with application to organizational reputationAbstract Machine learning methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories ",110,24,0.819486998021603,3,0.83560715119044
899,"Public administration and defence, compulsory social security","Deep Learning ",0.82622492313385,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","A Study on Conversational Public Administration Service of the Chatbot Based on Artificial IntelligenceArtificial intelligence-based services are expanding into a new industrial revolution. There is artificial intelligence technology applied in real life due to the development of big data and deep learning related technology. And data analysis and intelligent assistant services that ",110,16,0.825420185923576,3,0.822322686513265
913,"Public administration and defence, compulsory social security","Machine Perception",0.816839694976807,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","This paper describes a model of digital governance that reproduces within the system essential features of public administration while establishing logic for their utilization. The ultimate goal is to be able to confine all participants to their respective roles and Administration by Algorithm? Public Management Meets Public Sector Machine Learning",110,24,0.819486998021603,3,0.83560715119044
926,"Public administration and defence, compulsory social security","Deep Learning ",0.850289225578308,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Bridging machine learning and cryptography in defence against adversarial attacksIn the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks. However, most of the deep learning architectures are vulnerable to so called adversarial examples. This ",110,16,0.825420185923576,3,0.822322686513265
927,"Public administration and defence, compulsory social security","Machine Perception",0.847174346446991,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Bridging machine learning and cryptography in defence against adversarial attacksIn the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks. However, most of the deep learning architectures are vulnerable to so called adversarial examples. This ",110,24,0.819486998021603,3,0.83560715119044
946,"Public administration and defence, compulsory social security","Deep Learning ",0.790453910827637,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Artificial Intelligence Within the Military Domain and Cyber WarfareThe potential uses of machine learning and artificial intelligence in the cyber security domain have had a recent surge of interest. Much of the research and discussions in this area primarily focuses on reactive uses of the technology such as enhancing capabilities in ",110,16,0.825420185923576,3,0.822322686513265
948,"Public administration and defence, compulsory social security","Autonomous Robots",0.788502991199493,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","Artificial Intelligence Within the Military Domain and Cyber WarfareThe potential uses of machine learning and artificial intelligence in the cyber security domain have had a recent surge of interest. Much of the research and discussions in this area primarily focuses on reactive uses of the technology such as enhancing capabilities in ",110,10,0.822185301780701,3,0.807299395402273
961,"Public administration and defence, compulsory social security","Autonomous Robots",0.837525069713593,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","Machine learning techniques for autonomous agents in military simulationsMultum in parvoIn military simulations, software agents are used to represent individuals, weapon platforms or aggregates thereof. Modeling the behavioral capabilities and limitations of such agents may be time-consuming, requiring extensive interaction with subject matter experts and ",110,10,0.822185301780701,3,0.807299395402273
962,"Public administration and defence, compulsory social security","Automated Planning",0.831791937351227,"Automated planning is a branch of Artificial intelligence aimed at obtaining plans (i.e. sequences of actions) for solving complex problems or for governing the behavior of intelligent agents, autonomous robots or unmanned vehicles. Planning techniques have been successfully applied in different domains, including industrial contexts, logistics, computer games, robotics or space exploration. In this seminar we will review the existing approaches for solving classical planning problems, such as state-space search, plan-space search, graph-based techniques or turning classical planning problems into propositional satisfiability problems. The course will then focus on the study of knowledge-based planning methods, such as control rule-based pruning or hierarchical task network-based planning techniques. These approaches exploit the domain knowledge provided by human experts to improve the performance of the planning algorithms. Finally, we will briefly introduce advanced planning algorithms, which are able to generate planning policies that take into account time constraints and/or partial observability conditions, which are common in real world applications.","Machine learning techniques for autonomous agents in military simulationsMultum in parvoIn military simulations, software agents are used to represent individuals, weapon platforms or aggregates thereof. Modeling the behavioral capabilities and limitations of such agents may be time-consuming, requiring extensive interaction with subject matter experts and ",110,5,0.809059846401215,3,0.804516156514486
973,"Public administration and defence, compulsory social security","Autonomous Robots",0.795870125293732,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","[PDF][PDF] Developments in Artificial IntelligenceOpportunities and Challenges for Military Modeling and SimulationOne of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is"" Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"". Simulation might play a significant role to play in these developments as it ",110,10,0.822185301780701,3,0.807299395402273
974,"Public administration and defence, compulsory social security","Automated Planning",0.788613379001617,"Automated planning is a branch of Artificial intelligence aimed at obtaining plans (i.e. sequences of actions) for solving complex problems or for governing the behavior of intelligent agents, autonomous robots or unmanned vehicles. Planning techniques have been successfully applied in different domains, including industrial contexts, logistics, computer games, robotics or space exploration. In this seminar we will review the existing approaches for solving classical planning problems, such as state-space search, plan-space search, graph-based techniques or turning classical planning problems into propositional satisfiability problems. The course will then focus on the study of knowledge-based planning methods, such as control rule-based pruning or hierarchical task network-based planning techniques. These approaches exploit the domain knowledge provided by human experts to improve the performance of the planning algorithms. Finally, we will briefly introduce advanced planning algorithms, which are able to generate planning policies that take into account time constraints and/or partial observability conditions, which are common in real world applications.","[PDF][PDF] Developments in Artificial IntelligenceOpportunities and Challenges for Military Modeling and SimulationOne of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is"" Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"". Simulation might play a significant role to play in these developments as it ",110,5,0.809059846401215,3,0.804516156514486
988,"Water supply, sewerage, waste management and remediation activities","Lab in data science",0.834270060062408,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Machine learning in coupled wildfire-water supply risk assessment: Data science toolkitThe frontier of wildfire-related risk assessment is moving into data science territory, and with good reason. Computational statistics, built on a foundation of high resolution remote sensing data, ground data, and theory, forms the basis of powerful risk assessment tools ",30,78,0.825876659307724,3,0.813219567139943
991,"Water supply, sewerage, waste management and remediation activities","Lab in data science",0.802918314933777,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Preparing for the Use of Big Data in Denmark's Waste Management SectorThis project explored the challenges and opportunities associated with prospective implementations of big data analytics in Denmark's waste industry. We found that while some waste management companies collect detailed data, they do not use or share their ",30,78,0.825876659307724,3,0.813219567139943
992,"Water supply, sewerage, waste management and remediation activities","Big Data",0.792351961135864,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Preparing for the Use of Big Data in Denmark's Waste Management SectorThis project explored the challenges and opportunities associated with prospective implementations of big data analytics in Denmark's waste industry. We found that while some waste management companies collect detailed data, they do not use or share their ",30,99,0.823880515315316,3,0.809739172458649
997,"Water supply, sewerage, waste management and remediation activities","Lab in data science",0.802470326423645,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","A framework of developing a big data platform for construction waste management: a Hong Kong studyBig data has shown great potentials in improving management discretion in many areas. The applications of big data in areas such as finance, computer science, health care and medical science have made continued success. Despite of big data's potentials, its ",30,78,0.825876659307724,3,0.813219567139943
999,"Water supply, sewerage, waste management and remediation activities","Big Data",0.801368355751038,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","A framework of developing a big data platform for construction waste management: a Hong Kong studyBig data has shown great potentials in improving management discretion in many areas. The applications of big data in areas such as finance, computer science, health care and medical science have made continued success. Despite of big data's potentials, its ",30,99,0.823880515315316,3,0.809739172458649
1000,"Water supply, sewerage, waste management and remediation activities","Big Data",0.835497200489044,"The key challenge of the information society is to turn data into information, information into knowledge, knowledge into value. This has become increasingly complex. Data comes in larger volumes, diverse shapes, from different sources. Data is more heterogeneous and less structured than forty years ago. Nevertheless, it still needs to be processed fast, with support for complex operations.This combination of requirements, together with the technologies that have emerged in order to address them, is typically referred to as ""Big Data."" This revolution has led to a completely new way to do business, e.g., develop new products and business models, but also to do science -- which is sometimes referred to as data-driven science or the ""fourth paradigm"".

Unfortunately, the quantity of data produced and available -- now in the Zettabyte range (that's 21 zeros) per year -- keeps growing faster than our ability to process it. Hence, new architectures and approaches for processing it were and are still needed. Harnessing them must involve a deep understanding of data not only in the large, but also in the small.

The field of databases evolves at a fast pace. In order to be prepared, to the extent possible, to the (r)evolutions that will take place in the next few decades, the emphasis of the lecture will be on the paradigms and core design ideas, while today's technologies will serve as supporting illustrations thereof.

After visiting this lecture, you should have gained an overview and understanding of the Big Data landscape, which is the basis on which one can make informed decisions, i.e., pick and orchestrate the relevant technologies together for addressing each business use case efficiently and consistently.	This course gives an overview of database technologies and of the most important database design principles that lay the foundations of the Big Data universe. The material is organized along three axes: data in the large, data in the small, data in the very small. A broad range of aspects is covered with a focus on how they fit all together in the big picture of the Big Data ecosystem.

- physical storage: distributed file systems (HDFS), object storage(S3), key-value stores

- logical storage: document stores (MongoDB), column stores (HBase), graph databases (neo4j), data warehouses (ROLAP)

- data formats and syntaxes (XML, JSON, RDF, Turtle, CSV, XBRL, YAML, protocol buffers, Avro)

- data shapes and models (tables, trees, graphs, cubes)

- type systems and schemas: atomic types, structured types (arrays, maps), set-based type systems (?, *, +)

- an overview of functional, declarative programming languages across data shapes (SQL, XQuery, JSONiq, Cypher, MDX)

- the most important query paradigms (selection, projection, joining, grouping, ordering, windowing)

- paradigms for parallel processing, two-stage (MapReduce) and DAG-based (Spark)

- resource management (YARN)

- what a data center is made of and why it matters (racks, nodes, ...)

- underlying architectures (internal machinery of HDFS, HBase, Spark, neo4j)

- optimization techniques (functional and declarative paradigms, query plans, rewrites, indexing)

- applications.

Large scale analytics and machine learning are outside of the scope of this course.","Remediation, convergence, and big data: Conceptual limits of cross-platform social mediaThe era of multiplatform media and big data provide new opportunities to reconsider data access by media companies. Outlined here is the discussion surrounding data access from media institutional logic and user-centric perspectives in the contexts of digitalization and ",30,99,0.823880515315316,3,0.809739172458649
1021,"Real estate activities","A Network Tour of Data Science",0.876896739006042,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[BOOK][B] Real Estate Analysis in the Information Age: Techniques for Big Data and Statistical ModelingThe creation, accumulation, and use of copious amounts of data are driving rapid change across a wide variety of industries and academic disciplines. This 'Big Data'phenomenon is the result of recent developments in computational technology and improved data gathering ",60,53,0.824819772873285,3,0.841992795467377
1022,"Real estate activities","Artificial Intelligence and Data Science",0.868548929691315,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","[BOOK][B] Real Estate Analysis in the Information Age: Techniques for Big Data and Statistical ModelingThe creation, accumulation, and use of copious amounts of data are driving rapid change across a wide variety of industries and academic disciplines. This 'Big Data'phenomenon is the result of recent developments in computational technology and improved data gathering ",60,47,0.827168932620515,3,0.863841891288757
1027,"Real estate activities","A Network Tour of Data Science",0.78852105140686,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Real estate bubble prediction based on big dataDisclosed herein are a computer apparatus, non-transitory computer readable medium, and method for predicting real estate bubbles based on big data analysis. Historical variable data associated with real estate assets are obtained from remote data sources. Portions of ",60,53,0.824819772873285,3,0.841992795467377
1036,"Real estate activities","Artificial Intelligence and Data Science",0.869271516799927,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Urban data streams and machine learning: a case of swiss real estate marketIn this paper, we show how using publicly available data streams and machine learning algorithms one can develop practical data driven services with no input from domain experts as a form of prior knowledge. We report the initial steps toward development of a real estate  ",60,47,0.827168932620515,3,0.863841891288757
1041,"Real estate activities","Statistical Learning Theory",0.844056606292725,"The course covers advanced methods of statistical learning :Statistical learning theory;variational methods and optimization, e.g., maximum entropy techniques, information bottleneck, deterministic and simulated annealing; clustering for vectorial, histogram and relational data; model selection; graphical models.The course surveys recent methods of statistical learning. The fundamentals of machine learning as presented in the course ""Introduction to Machine Learning"" are expanded and in particular, the theory of statistical learning is discussed.        # Theory of estimators: How can we measure the quality of a statistical estimator? We already discussed bias and variance of estimators very briefly, but the interesting part is yet to come.

# Variational methods and optimization: We consider optimization approaches for problems where the optimizer is a probability distribution. Concepts we will discuss in this context include:

* Maximum Entropy
* Information Bottleneck
* Deterministic Annealing

# Clustering: The problem of sorting data into groups without using training samples. This requires a definition of ``similarity'' between data points and adequate optimization procedures.

# Model selection: We have already discussed how to fit a model to a data set in ML I, which usually involved adjusting model parameters for a given type of model. Model selection refers to the question of how complex the chosen model should be. As we already know, simple and complex models both have advantages and drawbacks alike.

# Statistical physics models: approaches for large systems approximate optimization, which originate in the statistical physics (free energy minimization applied to spin glasses and other models); sampling methods based on these models","Comparison of expert algorithms with machine learning models for real estate appraisalMachine learning models require numerous training examples to provide reliable predictions of real estate prices. Expert algorithms could be applied wherever only several training samples are available. The accuracy of two expert algorithms based on the sales ",60,8,0.840308278799057,3,0.856091519196828
1046,"Real estate activities","A Network Tour of Data Science",0.860560595989227,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","News-based sentiment analysis in real estate: A supervised machine learning approach with support vector networksWith the rapid growth of news, information and opinionated data available in digital form, accompanied by a swift progress of textual analysis techniques, the field of sentiment analysis became a hotspot in the area of natural language processing. Additionally ",60,53,0.824819772873285,3,0.841992795467377
1047,"Real estate activities","Artificial Intelligence and Data Science",0.853705227375031,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","News-based sentiment analysis in real estate: A supervised machine learning approach with support vector networksWith the rapid growth of news, information and opinionated data available in digital form, accompanied by a swift progress of textual analysis techniques, the field of sentiment analysis became a hotspot in the area of natural language processing. Additionally ",60,47,0.827168932620515,3,0.863841891288757
1050,"Real estate activities","Statistical Learning Theory",0.85603404045105,"The course covers advanced methods of statistical learning :Statistical learning theory;variational methods and optimization, e.g., maximum entropy techniques, information bottleneck, deterministic and simulated annealing; clustering for vectorial, histogram and relational data; model selection; graphical models.The course surveys recent methods of statistical learning. The fundamentals of machine learning as presented in the course ""Introduction to Machine Learning"" are expanded and in particular, the theory of statistical learning is discussed.        # Theory of estimators: How can we measure the quality of a statistical estimator? We already discussed bias and variance of estimators very briefly, but the interesting part is yet to come.

# Variational methods and optimization: We consider optimization approaches for problems where the optimizer is a probability distribution. Concepts we will discuss in this context include:

* Maximum Entropy
* Information Bottleneck
* Deterministic Annealing

# Clustering: The problem of sorting data into groups without using training samples. This requires a definition of ``similarity'' between data points and adequate optimization procedures.

# Model selection: We have already discussed how to fit a model to a data set in ML I, which usually involved adjusting model parameters for a given type of model. Model selection refers to the question of how complex the chosen model should be. As we already know, simple and complex models both have advantages and drawbacks alike.

# Statistical physics models: approaches for large systems approximate optimization, which originate in the statistical physics (free energy minimization applied to spin glasses and other models); sampling methods based on these models","Machine Learning Vs. Spatial Econometric Models: Modeling the Impact of Transportation Infrastructure on Real Estate PricesLinear regression with Ordinary Least Squares and spatial econometric models are statistical methods widely employed to measure the impact of transportation infrastructure locations on real estate prices. Efthymiou and Antoniou (1, 2, 3) developed different types of ",60,8,0.840308278799057,3,0.856091519196828
1065,"Real estate activities","Statistical Learning Theory",0.86818391084671,"The course covers advanced methods of statistical learning :Statistical learning theory;variational methods and optimization, e.g., maximum entropy techniques, information bottleneck, deterministic and simulated annealing; clustering for vectorial, histogram and relational data; model selection; graphical models.The course surveys recent methods of statistical learning. The fundamentals of machine learning as presented in the course ""Introduction to Machine Learning"" are expanded and in particular, the theory of statistical learning is discussed.        # Theory of estimators: How can we measure the quality of a statistical estimator? We already discussed bias and variance of estimators very briefly, but the interesting part is yet to come.

# Variational methods and optimization: We consider optimization approaches for problems where the optimizer is a probability distribution. Concepts we will discuss in this context include:

* Maximum Entropy
* Information Bottleneck
* Deterministic Annealing

# Clustering: The problem of sorting data into groups without using training samples. This requires a definition of ``similarity'' between data points and adequate optimization procedures.

# Model selection: We have already discussed how to fit a model to a data set in ML I, which usually involved adjusting model parameters for a given type of model. Model selection refers to the question of how complex the chosen model should be. As we already know, simple and complex models both have advantages and drawbacks alike.

# Statistical physics models: approaches for large systems approximate optimization, which originate in the statistical physics (free energy minimization applied to spin glasses and other models); sampling methods based on these models","A machine learning approach to big data regression analysis of real estate prices for inferential and predictive purposesThe hedonic price regressions have mainly been used for inference. In contrast, machine learning employed on big data has a great potential for prediction. To contribute to the integration of these two strategies, this article proposes a machine learning approach to the ",60,8,0.840308278799057,3,0.856091519196828
5,"Legal and accounting activities","Evolutionary Computation",0.837674617767334,"This module introduces, first, the different models of symbolic and sub-symbolic intelligent systems, respectively: knowledge-based systems and artificial neural networks. Their characteristics, their constituent elements, advantages and disadvantages of each model and its application domain, are indicated for each of them. Special emphasis is placed on existing synergies with evolutionary computation to resolve the major difficulties that may be encountered in building such systems: knowledge extraction, selection of the best neural architecture and the process of training the system. Subsequently, we will study evolutionary computation, mainly genetic algorithms and genetic programming, which provide mechanisms for automatic construction of intelligent self-adaptive systems or robust systems, both symbolic and sub-symbolic. Finally, we will analyse the current trends in evolutionary computation and the most recent research results. The student will be provided with a promising line of research to follow in order to obtain the PhD degree","[PDF][PDF] Bankruptcy risk prediction models based on artificial neural networks As part of the Artificial Intelligence, the neural networks are systems that use approximation
methods based  31 December 2015 in Romania there were a number of 773781 active legal
entities  978-0-9742114-1-1. Walczak, S.(2001)An empirical analysis of data requirements for  
",80,4,0.821203500032425,2,0.817140609025955
26,"Legal and accounting activities","Language Engineering",0.800338208675385,"Language Engineering (LE) is the set of techniques, resources and tools to solve problems by using more or less an automated language. This course aims to introduce students to the overall framework, which is currently the LE. The second part of the subject will explain the two main principles of most language treatment systems, such as the content representation models and the creation and maintenance of lexical resources, both pillars of any system and any use. In the third part of the course the student will be introduced three of the major commercial applications of LE, such as information retrieval (associated with the search for data or items of information in a text) and text mining, where besides extracting data type information, we will extract relationships between them. The existing application on the market, and the more immediate trends (for example the analysis of forums for opinions) will also be discussed and explained.","Big data in accounting Ernst &Young Limited (EY) is the third case study. Its scope of accounting activities includes:
advisory, assurance and tax services and as a result it demonstrates great scientific interest  BigData can be very  visual data analytics (Gepp et. al, 2018). Another suggestion is meta  
",80,6,0.806979159514109,2,0.799652099609375
30,"Legal and accounting activities","Enterprise Integration",0.821722388267517,"The main goal of this course is to provide a broad and in-depth view of the concepts, methodologies, and technologies associated with systems integration, including the integration of applications, services, and inter-organizational business processes. The topics addressed in this course are positioned at a key point between the application infrastructure and the business processes in an organization, and the aim is to understand the relationships and dependencies between the two. The course will also provide insight into how it is possible to devise a distributed and integrated application infrastructure. The concrete learning objectives are as follows: 1. To provide an in-depth view of the main concepts and integration solutions in the field of integration; 2. To develop a systematic and process-oriented vision of how integration problems should be addressed; 3. To acquire a practical knowledge of the state-of-the-art integration platforms, based on lab projects; 4. To understand the critical role that integration solutions have in the design and implementation of business processes.The course aims at providing a coherent structure of integration topics that can be found in different parts of the ACM/AIS IS 2010 curriculum, such as “Enterprise Systems” and
“Application Development”. When appropriate, this syllabus is labeled with topics from that curiculum and also from the ACM CCS 2012 taxonomy:
1. Evolution of information systems
a. essential functions of information systems in business organizations;
b. evolution of information systems architecture over the years; point-to-point vs. centralized integration;
c. integration based on the concept of service.
ACM/AIS IS 2010.1 Information Systems in Organizations
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise Information Systems
2. Introduction to integration platforms
a. message exchange;
b. message schema and transformation;
c. ports and adapters;
d. orchestrations;
e. business rules.
ACM/AIS IS 2010.3 Systems Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Service buses;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business rules
3. Messaging systems
a. fundamental concepts;
b. message transactions;
c. message acknowledgments;
d. message correlation;
e. messaging platforms.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise interoperability > Enterprise application integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Message queues
4. Message brokers
a. message-level integration vs. orchestration-level integration;
b. publish-subscribe with message filters;
c. message properties;
d. message correlation;
e. asynchronous messaging.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
5. Adapters
a. three-tier client-server model;
b. capture of the user interface;
c. integration through files;
d. database access APIs;
e. retrieving data in XML;
f. data access in orchestrations;
g. methods and interfaces;
h. interface discovery and dynamic invocations;
i. Web service invocation in orchestrations.
ACM/AIS IS 2010.3 Data Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration
ACM CCS 2012 Information Systems > World Wide Web > Web services
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
6. Services and SOA
a. services and applications;
b. service composition;
c. service orchestration;
d. business processes;
e. service design principles;
f. benefits of SOA;
g. support for human workflows.
ACM/AIS IS 2010.3 Service oriented architecture
ACM CCS 2012 Applied Computing > Enterprise Computing > Service-oriented architectures;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Business process management systems
7. Service orchestrations
a. block structure;
b. beginning the flow;
c. message construction;
d. flow control with loops, decisions, and parallelism;
e. orchestrations as sub-processes;
f. concurrent events;
g. correlations;
h. exception handling;
i. transactions and compensation.
ACM/AIS IS 2010.ES Business process integration
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors
8. Inter-organizational integration
a. electronic data exchange;
b. introduction to supply chain management;
c. supply chain coordination;
d. electronic commerce;
e. negotiation protocols.
ACM/AIS IS 2010.1 Supply Chain Management
ACM/AIS IS 2010.ES Production logistics
ACM CCS 2012 Information Systems > World Wide Web > Web applications > Electronic commerce > Electronic data interchange;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Cross-organizational business processes
9. Internet of things
a. physical world and virtual world integration;
b. traceability systems;
c. sensors and complex event processing;
d. logistics systems based on RFID.
ACM/AIS IS 2010.ES Enterprise Systems > Production logistics
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise computing > Event-driven architectures
ACM CCS 2012 Information systems > Information systems applications > Spatial-temporal systems > Data streaming","Bits and bolts Real estate 68 Low Low Legal and accounting activities, etc  online renting of data or computing
capacities), as well as back and front office integration systems such as customer relationship
management (CRM), and enterprise resource planning (ERP) software. Page 14  
",80,7,0.81805990423475,2,0.820902764797211
37,"Legal and accounting activities","Enterprise Integration",0.820083141326904,"The main goal of this course is to provide a broad and in-depth view of the concepts, methodologies, and technologies associated with systems integration, including the integration of applications, services, and inter-organizational business processes. The topics addressed in this course are positioned at a key point between the application infrastructure and the business processes in an organization, and the aim is to understand the relationships and dependencies between the two. The course will also provide insight into how it is possible to devise a distributed and integrated application infrastructure. The concrete learning objectives are as follows: 1. To provide an in-depth view of the main concepts and integration solutions in the field of integration; 2. To develop a systematic and process-oriented vision of how integration problems should be addressed; 3. To acquire a practical knowledge of the state-of-the-art integration platforms, based on lab projects; 4. To understand the critical role that integration solutions have in the design and implementation of business processes.The course aims at providing a coherent structure of integration topics that can be found in different parts of the ACM/AIS IS 2010 curriculum, such as “Enterprise Systems” and
“Application Development”. When appropriate, this syllabus is labeled with topics from that curiculum and also from the ACM CCS 2012 taxonomy:
1. Evolution of information systems
a. essential functions of information systems in business organizations;
b. evolution of information systems architecture over the years; point-to-point vs. centralized integration;
c. integration based on the concept of service.
ACM/AIS IS 2010.1 Information Systems in Organizations
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise Information Systems
2. Introduction to integration platforms
a. message exchange;
b. message schema and transformation;
c. ports and adapters;
d. orchestrations;
e. business rules.
ACM/AIS IS 2010.3 Systems Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Service buses;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business rules
3. Messaging systems
a. fundamental concepts;
b. message transactions;
c. message acknowledgments;
d. message correlation;
e. messaging platforms.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise interoperability > Enterprise application integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Message queues
4. Message brokers
a. message-level integration vs. orchestration-level integration;
b. publish-subscribe with message filters;
c. message properties;
d. message correlation;
e. asynchronous messaging.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
5. Adapters
a. three-tier client-server model;
b. capture of the user interface;
c. integration through files;
d. database access APIs;
e. retrieving data in XML;
f. data access in orchestrations;
g. methods and interfaces;
h. interface discovery and dynamic invocations;
i. Web service invocation in orchestrations.
ACM/AIS IS 2010.3 Data Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration
ACM CCS 2012 Information Systems > World Wide Web > Web services
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
6. Services and SOA
a. services and applications;
b. service composition;
c. service orchestration;
d. business processes;
e. service design principles;
f. benefits of SOA;
g. support for human workflows.
ACM/AIS IS 2010.3 Service oriented architecture
ACM CCS 2012 Applied Computing > Enterprise Computing > Service-oriented architectures;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Business process management systems
7. Service orchestrations
a. block structure;
b. beginning the flow;
c. message construction;
d. flow control with loops, decisions, and parallelism;
e. orchestrations as sub-processes;
f. concurrent events;
g. correlations;
h. exception handling;
i. transactions and compensation.
ACM/AIS IS 2010.ES Business process integration
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors
8. Inter-organizational integration
a. electronic data exchange;
b. introduction to supply chain management;
c. supply chain coordination;
d. electronic commerce;
e. negotiation protocols.
ACM/AIS IS 2010.1 Supply Chain Management
ACM/AIS IS 2010.ES Production logistics
ACM CCS 2012 Information Systems > World Wide Web > Web applications > Electronic commerce > Electronic data interchange;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Cross-organizational business processes
9. Internet of things
a. physical world and virtual world integration;
b. traceability systems;
c. sensors and complex event processing;
d. logistics systems based on RFID.
ACM/AIS IS 2010.ES Enterprise Systems > Production logistics
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise computing > Event-driven architectures
ACM CCS 2012 Information systems > Information systems applications > Spatial-temporal systems > Data streaming","[BOOK][B] The Routledge Companion to Accounting Information Systems interests include continuous auditing and monitoring, audit data analytics and artificial intelligence
in auditing  process management for AIS in the context of new legal requirements for  constituting
different decision-making environments and what they call data environments  
",80,7,0.81805990423475,2,0.820902764797211
49,"Legal and accounting activities","Language Engineering",0.798965990543365,"Language Engineering (LE) is the set of techniques, resources and tools to solve problems by using more or less an automated language. This course aims to introduce students to the overall framework, which is currently the LE. The second part of the subject will explain the two main principles of most language treatment systems, such as the content representation models and the creation and maintenance of lexical resources, both pillars of any system and any use. In the third part of the course the student will be introduced three of the major commercial applications of LE, such as information retrieval (associated with the search for data or items of information in a text) and text mining, where besides extracting data type information, we will extract relationships between them. The existing application on the market, and the more immediate trends (for example the analysis of forums for opinions) will also be discussed and explained.","What role for social sciences in innovation? related fields (including computer science, information systems, software engineering and artificialintelligence) are found  Availability of comprehensive, long-term and internationally comparabledata allows for  cite NPL; citations are frequently given by examiners or by patent  
",80,6,0.806979159514109,2,0.799652099609375
55,"Legal and accounting activities","Autonomous Robots",0.830757677555084,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","Adapting business framework conditions to deal with disruptive technologies in Denmark Participation in life-long learning is high but decreasing  Second, the ability to combine new
advanced technologies (such as sensors, advanced robotics and 3D printing), new processes
(such as data-driven production and artificial intelligence) and new business models  
",80,10,0.822185301780701,2,0.820486336946487
65,"Legal and accounting activities","Bid Data Measuring Systems",0.828857362270355,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Measuring Performance of Adaptive Supply Chains The data source for such a system is the reporting of individual units under accounting activities 
does not solely result from the will to satisfy specific customer requirements, but also the necessity
to adjust products to the legal requirements of  5.2 Sample and Data Collection  
",80,24,0.826420515775681,2,0.822841882705688
70,"Legal and accounting activities","Bid Data Measuring Systems",0.816826403141022,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","[BOOK][B] Decentralized Computing Using Blockchain Technologies and Smart Contracts: Emerging Research and Opportunities: Emerging Research and  With the advent of intelligent systems and self-learning machines, every device require freedom
of  chain, which is computationally very expensive and this makes the blockchain an immutabledata store  can be either an individual or a group of miners or a machine powered with  
",80,24,0.826420515775681,2,0.822841882705688
71,"Legal and accounting activities","Autonomous Robots",0.810214996337891,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","[BOOK][B] Decentralized Computing Using Blockchain Technologies and Smart Contracts: Emerging Research and Opportunities: Emerging Research and  With the advent of intelligent systems and self-learning machines, every device require freedom
of  chain, which is computationally very expensive and this makes the blockchain an immutabledata store  can be either an individual or a group of miners or a machine powered with  
",80,10,0.822185301780701,2,0.820486336946487
74,"Legal and accounting activities","Evolutionary Computation",0.796606600284576,"This module introduces, first, the different models of symbolic and sub-symbolic intelligent systems, respectively: knowledge-based systems and artificial neural networks. Their characteristics, their constituent elements, advantages and disadvantages of each model and its application domain, are indicated for each of them. Special emphasis is placed on existing synergies with evolutionary computation to resolve the major difficulties that may be encountered in building such systems: knowledge extraction, selection of the best neural architecture and the process of training the system. Subsequently, we will study evolutionary computation, mainly genetic algorithms and genetic programming, which provide mechanisms for automatic construction of intelligent self-adaptive systems or robust systems, both symbolic and sub-symbolic. Finally, we will analyse the current trends in evolutionary computation and the most recent research results. The student will be provided with a promising line of research to follow in order to obtain the PhD degree","[BOOK][B] Decentralized Computing Using Blockchain Technologies and Smart Contracts: Emerging Research and Opportunities: Emerging Research and  With the advent of intelligent systems and self-learning machines, every device require freedom
of  chain, which is computationally very expensive and this makes the blockchain an immutabledata store  can be either an individual or a group of miners or a machine powered with  
",80,4,0.821203500032425,2,0.817140609025955
87,"Accomodation and food service activities","Data Mining",0.857196450233459,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","Big data in food safety: An overview A list of the most used analysis methods for big data is shown in Table 3. These meth  These
systems are developed using data mining techniques (collaborative filtering, content based filtering
and hybrid  To the author's knowledge, these systems are not yet applied in food safety  
",145,9,0.830869707796309,2,0.838083505630493
101,"Accomodation and food service activities","Systems for data science",0.829715192317963,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","A big data and cloud computing specification, standards and architecture: agricultural and food informatics data and cloud computing specification, standards and architecture: agricultural and food
informatics  The real-time data storage and management architecture plays important role. This
paper introduces big data, includes the background and definitions, characteristics, related  
",145,21,0.821209547065553,2,0.823469132184982
119,"Accomodation and food service activities","Data visualization",0.78154045343399,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","A semantic network analysis of big data regarding food exhibition at convention center The purpose of this study was to visualize the semantic network with big data related to food
exhibition at convention center. For this, this study collected data containing 'coex food
exhibition/bexco food exhibition' keywords from web pages and news on Google during one  
",145,10,0.81925385594368,2,0.791678756475449
120,"Accomodation and food service activities","Statistical Modelling of Spatial Data",0.833192110061646,"In environmental sciences one often deals with spatial data. When analysing such data the focus is either on exploring their structure (dependence on explanatory variables, autocorrelation) and/or on spatial prediction. The course provides an introduction to geostatistical methods that are useful for such analyses.        The course will provide an overview of the basic concepts and stochastic models that are used to model spatial data. In addition, participants will learn a number of geostatistical techniques and acquire familiarity with R software that is useful for analyzing spatial data.        After an introductory discussion of the types of problems and the kind of data that arise in environmental research, an introduction into linear geostatistics (models: stationary and intrinsic random processes, modelling large-scale spatial patterns by linear regression, modelling autocorrelation by variogram; kriging: mean square prediction of spatial data) will be taught. The lectures will be complemented by data analyses that the participants have to do themselves.","A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning Taking advantage of a geo-big data approach and multilevel regression model, we make a
contribution to current literature by  should offer deeper spatial insights into intra-urban foodscape
and provide more nuanced understanding of food deserts  2. Methodology and data. 2.1  
",145,5,0.829264938831329,2,0.824029803276062
126,"Accomodation and food service activities","Statistical Modelling of Spatial Data",0.814867496490479,"In environmental sciences one often deals with spatial data. When analysing such data the focus is either on exploring their structure (dependence on explanatory variables, autocorrelation) and/or on spatial prediction. The course provides an introduction to geostatistical methods that are useful for such analyses.        The course will provide an overview of the basic concepts and stochastic models that are used to model spatial data. In addition, participants will learn a number of geostatistical techniques and acquire familiarity with R software that is useful for analyzing spatial data.        After an introductory discussion of the types of problems and the kind of data that arise in environmental research, an introduction into linear geostatistics (models: stationary and intrinsic random processes, modelling large-scale spatial patterns by linear regression, modelling autocorrelation by variogram; kriging: mean square prediction of spatial data) will be taught. The lectures will be complemented by data analyses that the participants have to do themselves.","Nutritional Culturomics and Big Data: Macroscopic Patterns of Change in Food, Nutrition and Diet Choices and diet, their knowledge, awareness and understanding of the interdepend- ence of food
consumption, health  [15]), and mathematical tools to handle complex data sets (eg  application
of Artificial Intelligence for large-scale content analysis [18, 19] or random fractal theory to  
",145,5,0.829264938831329,2,0.824029803276062
129,"Accomodation and food service activities","Data visualization",0.801817059516907,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","Nutritional Culturomics and Big Data: Macroscopic Patterns of Change in Food, Nutrition and Diet Choices and diet, their knowledge, awareness and understanding of the interdepend- ence of food
consumption, health  [15]), and mathematical tools to handle complex data sets (eg  application
of Artificial Intelligence for large-scale content analysis [18, 19] or random fractal theory to  
",145,10,0.81925385594368,2,0.791678756475449
132,"Accomodation and food service activities","Biomedical Informatics",0.834618866443634,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Research and application of big data-based co-regulation model in food safety governance.This paper is aimed to construct a food safety and nutrition information collection and analysis
platform with new internet technologies including big data to improve the analytic capacity and
the data-mining capability, and thereby to provide more accurate and comprehensive food  
",145,18,0.817355437411202,2,0.820109218358994
153,"Accomodation and food service activities","Data Mining",0.818970561027527,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","Big data mining for predicting stochastic variables in food supply chains at different scales Abstract. Food products supply chains incorporate multiple scales in space and time in demand
and supply sides  Modern big data based data mining and machine learning and tools in the larger
domain of artificial intelligence are ideal for such complex problems  
",145,9,0.830869707796309,2,0.838083505630493
154,"Accomodation and food service activities","Systems for data science",0.817223072052002,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Big data mining for predicting stochastic variables in food supply chains at different scales Abstract. Food products supply chains incorporate multiple scales in space and time in demand
and supply sides  Modern big data based data mining and machine learning and tools in the larger
domain of artificial intelligence are ideal for such complex problems  
",145,21,0.821209547065553,2,0.823469132184982
160,"Accomodation and food service activities","Enterprise Integration",0.849928319454193,"The main goal of this course is to provide a broad and in-depth view of the concepts, methodologies, and technologies associated with systems integration, including the integration of applications, services, and inter-organizational business processes. The topics addressed in this course are positioned at a key point between the application infrastructure and the business processes in an organization, and the aim is to understand the relationships and dependencies between the two. The course will also provide insight into how it is possible to devise a distributed and integrated application infrastructure. The concrete learning objectives are as follows: 1. To provide an in-depth view of the main concepts and integration solutions in the field of integration; 2. To develop a systematic and process-oriented vision of how integration problems should be addressed; 3. To acquire a practical knowledge of the state-of-the-art integration platforms, based on lab projects; 4. To understand the critical role that integration solutions have in the design and implementation of business processes.The course aims at providing a coherent structure of integration topics that can be found in different parts of the ACM/AIS IS 2010 curriculum, such as “Enterprise Systems” and
“Application Development”. When appropriate, this syllabus is labeled with topics from that curiculum and also from the ACM CCS 2012 taxonomy:
1. Evolution of information systems
a. essential functions of information systems in business organizations;
b. evolution of information systems architecture over the years; point-to-point vs. centralized integration;
c. integration based on the concept of service.
ACM/AIS IS 2010.1 Information Systems in Organizations
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise Information Systems
2. Introduction to integration platforms
a. message exchange;
b. message schema and transformation;
c. ports and adapters;
d. orchestrations;
e. business rules.
ACM/AIS IS 2010.3 Systems Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Service buses;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business rules
3. Messaging systems
a. fundamental concepts;
b. message transactions;
c. message acknowledgments;
d. message correlation;
e. messaging platforms.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise interoperability > Enterprise application integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Message queues
4. Message brokers
a. message-level integration vs. orchestration-level integration;
b. publish-subscribe with message filters;
c. message properties;
d. message correlation;
e. asynchronous messaging.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
5. Adapters
a. three-tier client-server model;
b. capture of the user interface;
c. integration through files;
d. database access APIs;
e. retrieving data in XML;
f. data access in orchestrations;
g. methods and interfaces;
h. interface discovery and dynamic invocations;
i. Web service invocation in orchestrations.
ACM/AIS IS 2010.3 Data Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration
ACM CCS 2012 Information Systems > World Wide Web > Web services
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
6. Services and SOA
a. services and applications;
b. service composition;
c. service orchestration;
d. business processes;
e. service design principles;
f. benefits of SOA;
g. support for human workflows.
ACM/AIS IS 2010.3 Service oriented architecture
ACM CCS 2012 Applied Computing > Enterprise Computing > Service-oriented architectures;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Business process management systems
7. Service orchestrations
a. block structure;
b. beginning the flow;
c. message construction;
d. flow control with loops, decisions, and parallelism;
e. orchestrations as sub-processes;
f. concurrent events;
g. correlations;
h. exception handling;
i. transactions and compensation.
ACM/AIS IS 2010.ES Business process integration
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors
8. Inter-organizational integration
a. electronic data exchange;
b. introduction to supply chain management;
c. supply chain coordination;
d. electronic commerce;
e. negotiation protocols.
ACM/AIS IS 2010.1 Supply Chain Management
ACM/AIS IS 2010.ES Production logistics
ACM CCS 2012 Information Systems > World Wide Web > Web applications > Electronic commerce > Electronic data interchange;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Cross-organizational business processes
9. Internet of things
a. physical world and virtual world integration;
b. traceability systems;
c. sensors and complex event processing;
d. logistics systems based on RFID.
ACM/AIS IS 2010.ES Enterprise Systems > Production logistics
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise computing > Event-driven architectures
ACM CCS 2012 Information systems > Information systems applications > Spatial-temporal systems > Data streaming","[PDF][PDF] Recommender System Based Tensor Candecomp Parafact Algorithm-ALS to Handle Sparse Data In Food Commerce Information ServicesRecommender systems have been widely researched in many applications especially in e-commerce services with the aim to make clear and easy communication between consumer and provider. Simple examples of Recommender systems would include personal and ",145,7,0.81805990423475,2,0.817902863025665
183,"Accomodation and food service activities","Biomedical Informatics",0.805599570274353,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector of currently available studies on emerging technologies in agriculture and food sector (A&F  The
opportunities of the new big-data-augmented methodology are shown in comparison to existing 
with special attention to use of bigger volumes of data, machine learning and ontology  
",145,18,0.817355437411202,2,0.820109218358994
219,"Accomodation and food service activities","Enterprise Integration",0.785877406597137,"The main goal of this course is to provide a broad and in-depth view of the concepts, methodologies, and technologies associated with systems integration, including the integration of applications, services, and inter-organizational business processes. The topics addressed in this course are positioned at a key point between the application infrastructure and the business processes in an organization, and the aim is to understand the relationships and dependencies between the two. The course will also provide insight into how it is possible to devise a distributed and integrated application infrastructure. The concrete learning objectives are as follows: 1. To provide an in-depth view of the main concepts and integration solutions in the field of integration; 2. To develop a systematic and process-oriented vision of how integration problems should be addressed; 3. To acquire a practical knowledge of the state-of-the-art integration platforms, based on lab projects; 4. To understand the critical role that integration solutions have in the design and implementation of business processes.The course aims at providing a coherent structure of integration topics that can be found in different parts of the ACM/AIS IS 2010 curriculum, such as “Enterprise Systems” and
“Application Development”. When appropriate, this syllabus is labeled with topics from that curiculum and also from the ACM CCS 2012 taxonomy:
1. Evolution of information systems
a. essential functions of information systems in business organizations;
b. evolution of information systems architecture over the years; point-to-point vs. centralized integration;
c. integration based on the concept of service.
ACM/AIS IS 2010.1 Information Systems in Organizations
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise Information Systems
2. Introduction to integration platforms
a. message exchange;
b. message schema and transformation;
c. ports and adapters;
d. orchestrations;
e. business rules.
ACM/AIS IS 2010.3 Systems Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Service buses;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business rules
3. Messaging systems
a. fundamental concepts;
b. message transactions;
c. message acknowledgments;
d. message correlation;
e. messaging platforms.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise interoperability > Enterprise application integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Message queues
4. Message brokers
a. message-level integration vs. orchestration-level integration;
b. publish-subscribe with message filters;
c. message properties;
d. message correlation;
e. asynchronous messaging.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
5. Adapters
a. three-tier client-server model;
b. capture of the user interface;
c. integration through files;
d. database access APIs;
e. retrieving data in XML;
f. data access in orchestrations;
g. methods and interfaces;
h. interface discovery and dynamic invocations;
i. Web service invocation in orchestrations.
ACM/AIS IS 2010.3 Data Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration
ACM CCS 2012 Information Systems > World Wide Web > Web services
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
6. Services and SOA
a. services and applications;
b. service composition;
c. service orchestration;
d. business processes;
e. service design principles;
f. benefits of SOA;
g. support for human workflows.
ACM/AIS IS 2010.3 Service oriented architecture
ACM CCS 2012 Applied Computing > Enterprise Computing > Service-oriented architectures;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Business process management systems
7. Service orchestrations
a. block structure;
b. beginning the flow;
c. message construction;
d. flow control with loops, decisions, and parallelism;
e. orchestrations as sub-processes;
f. concurrent events;
g. correlations;
h. exception handling;
i. transactions and compensation.
ACM/AIS IS 2010.ES Business process integration
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors
8. Inter-organizational integration
a. electronic data exchange;
b. introduction to supply chain management;
c. supply chain coordination;
d. electronic commerce;
e. negotiation protocols.
ACM/AIS IS 2010.1 Supply Chain Management
ACM/AIS IS 2010.ES Production logistics
ACM CCS 2012 Information Systems > World Wide Web > Web applications > Electronic commerce > Electronic data interchange;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Cross-organizational business processes
9. Internet of things
a. physical world and virtual world integration;
b. traceability systems;
c. sensors and complex event processing;
d. logistics systems based on RFID.
ACM/AIS IS 2010.ES Enterprise Systems > Production logistics
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise computing > Event-driven architectures
ACM CCS 2012 Information systems > Information systems applications > Spatial-temporal systems > Data streaming","[CITATION][C] Application of a business intelligence tool within the context of big data in a food industry companySmart ManufacturingPotential of New Digital Technologies and Big Data in the Food Industry",145,7,0.81805990423475,2,0.817902863025665
232,"Human health and social work activities","Distributed information systems",0.838661968708038,"This course introduces the key concepts and algorithms from the areas of information retrieval, data mining and knowledge bases, which constitute the foundations of today's Web-based distributed information systems. Information Retrieval

1.Information Retrieval - Introduction 2.Text-Based Information Retrieval 3.Vector Space Retrieval 4.Probabilistic Information Retrieval 5.Query Expansion 6.Inverted Index 7.Distributed Retrieval 8.Latent Semantic Indexing 9.Word Embeddings 10. Link-Based Ranking

Data Mining

1.Data Mining ¿ Introduction 2. Association Rule Mining 3. Clustering 4. Classification 5. Mining Social Graphs 6. Classification Methodology 7. Document Classification 8. Recommender Systems

Knowledge Bases

1. Semi-structured data 2. Semantic Web 3. RDF Resource Description Framework 4. Semantic Web Resources 5. Information Extraction 6. Taxonomy Induction 7. Ontology Mapping. By the end of the course, the student must be able to:
Characterize the main tasks performed by information systems, namely data, information and knowledge management
Apply semi-structured data models, their representation through Web standards and algorithms for storing and processing semi-structured data
Apply fundamental models and techniques of text retrieval and their use in Web search engines
Apply main categories of data mining techniques, local rules, predictive and descriptive models, and master representative algorithms for each of the categories
Apply collaborative information management models, like crowd-sourcing, recommender systems, social networks","Identifying child abuse through text mining and machine learning is related to work in the area of data exploration and supervised classification based  risk modeling
(PRM) tools coupled with data mining and machine-learning algorithms should  a linear prediction
model (45.2% sensitivity, 82.4% specificity) using administrative data from 716  
",195,3,0.81862876812617,2,0.836484879255295
255,"Human health and social work activities","Introduction to natural language processing",0.881980776786804,"The objective of this course is to present the main models, formalisms and algorithms necessary for the development of applications in the field of natural language information processing. The concepts introduced during the lectures will be applied during practical sessions.Several models and algorithms for automated textual data processing will be described: (1) morpho-lexical level: electronic lexica, spelling checkers, ...; (2) syntactic level: regular, context-free, stochastic grammars, parsing algorithms, ...; (3) semantic level: models and formalisms for the representation of meaning, ...

Several application domains will be presented: Linguistic engineering, Information Retrieval, Text mining (automated knowledge extraction), Textual Data Analysis (automated document classification, visualization of textual data).

Keywords
Natural Language Processing; Computationnal Linguisitics; Part-of-Speech tagging; Parsing.By the end of the course, the student must be able to:
Compose key NLP elements to develop higher level processing chains
Assess / Evaluate NLP based systems
Choose appropriate solutions for solving typical NLP subproblems (tokenizing, tagging, parsing)
Describe the typical problems and processing layers in NLP
Analyze NLP problems to decompose them in adequate independant components.","A review of existing applications and techniques for narrative text analysis in electronic medical records ARC (Automated Retrieval Console): An algorithm based on an artificial intelligence program,
which  That result suggested that the major variety and contexts for the PHI in the social work notes
is more difficult to model  Data quality is an important barrier to NLP and text mining  
",195,3,0.835448265075684,2,0.838435351848602
259,"Human health and social work activities","Data visualization",0.866838216781616,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","A review of existing applications and techniques for narrative text analysis in electronic medical records ARC (Automated Retrieval Console): An algorithm based on an artificial intelligence program,
which  That result suggested that the major variety and contexts for the PHI in the social work notes
is more difficult to model  Data quality is an important barrier to NLP and text mining  
",195,10,0.81925385594368,2,0.849512100219727
274,"Human health and social work activities","Intelligent Agents and Multi-Agent Systems",0.782415747642517,"Multiagent systems are systems consisting of several autonomous entities called agents that interact among themselves in order to solve problems that exceed the individual capabilities of each or solving them in a more efficient way. This interaction is the main object of research in multiagent systems, and it has contributed to different disciplines such as Social Science, Game Theory and Artificial Intelligence. In this module, besides studying these contributions, we will introduce the students to the practice of research in any area related to multi-agent systems, and the preparation of papers describing the results of its research activity","[BOOK][B] Handbook of Research Methods in Complexity Science: Theory and Applications in complexity science Emily S. Ihara is an Associate Professor of Social Work at George  on brain
dynamics and structure by analys- ing fMRI and EEG data and he is  His research includes
applications in Social and economic systems, business, artificial intelligence and robotics  
",195,4,0.801681563258171,2,0.800006747245789
280,"Human health and social work activities","Biomedical Informatics",0.855551600456238,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","[BOOK][B] The future of work in Australia: anticipating how new technologies will reshape labour markets, occupations and skill requirements He holds qualifications in political science, social policy, and social work  recent developments
in information and communication technology (ICT), computer-based technologies (CBT) andartificial intelligence (AI) have  Big data has helped to facilitate significant advances in AI  
",195,18,0.817355437411202,2,0.822475731372833
287,"Human health and social work activities","Bioinformatics",0.825613975524902,"Acquiring insight in handling and analysis of molecular biologica data using computational techniques.
Understanding the underlying principles of a selection of computational techniques and models that are frequently used in bioinformatics.
Being able to select the appropriate technique to solve a given problem, and being able to apply it.
Knowing how to use, access, search the most important public molecular biological databases.The bioinformatics course aims to give students essential insights in the most important computational techniques used for the analysis of molecular (system) biological data. The student will also learn how to select the right strategy for a given task.

Part I. Scope of Bioinformatics

1. introduction
2. essential background
3. databases

Part II. Analysis of Biomolecules

4. pairwise sequence alignment
5. multiple sequence alignment
6. sequence search
7. profiles and motifs
8. phylogenetic trees
9. protein structure

Part III. Analysis of Living Systems

10. molecular functions
11. genome analysis
12. multi-omics
13. network biology
14. human disease

The practical sessions give students a chance to apply some of the discussed algorithms to solve concrete problems.","[HTML][HTML] Automated screening for Fragile X premutation carriers based on linguistic and cognitive computational phenotypes Building on prior approaches that analyze data among multiple genotypes and
multiple phenotypes 22 , we used statistical and machine-learning methods to develop
a feature selection module and a data-driven classifier (Fig  
",195,6,0.80968498190244,2,0.823960900306702
289,"Human health and social work activities","Bioinformatics",0.822307825088501,"Bioinformatics aims at developing computational methods and algorithms to process biological data and uses mathematical and statistical modelling to generate testable hypotheses about biological entities and processes. The goal of this course is to introduce the basic techniques that support the most recent developments on this field. Additionally, it enables the development of the ability to critically assess research publications in this field. Practical assignments during the course aim at developing the student's ability to develop software for bioinformatics.Introduction, Molecular biology main concepts, Introduction to algorithms and complexity
Graphs and genetics
DNA sequence analysis
Pairwise alignment
Multiple Sequence alignment
Motif finding
NGS data, algorithms and data structures
Probabilistic models
Gene expression data analysis
Data mining
Unsupervised Learning: Clustering and Biclustering
Molecular phylogenetics
Supervised Learning: Decision trees, Bayesian methods
Integrative data analysis
Seminar","[HTML][HTML] Automated screening for Fragile X premutation carriers based on linguistic and cognitive computational phenotypes Building on prior approaches that analyze data among multiple genotypes and
multiple phenotypes 22 , we used statistical and machine-learning methods to develop
a feature selection module and a data-driven classifier (Fig  
",195,6,0.80968498190244,2,0.823960900306702
293,"Human health and social work activities","Distributed information systems",0.834307789802551,"This course introduces the key concepts and algorithms from the areas of information retrieval, data mining and knowledge bases, which constitute the foundations of today's Web-based distributed information systems. Information Retrieval

1.Information Retrieval - Introduction 2.Text-Based Information Retrieval 3.Vector Space Retrieval 4.Probabilistic Information Retrieval 5.Query Expansion 6.Inverted Index 7.Distributed Retrieval 8.Latent Semantic Indexing 9.Word Embeddings 10. Link-Based Ranking

Data Mining

1.Data Mining ¿ Introduction 2. Association Rule Mining 3. Clustering 4. Classification 5. Mining Social Graphs 6. Classification Methodology 7. Document Classification 8. Recommender Systems

Knowledge Bases

1. Semi-structured data 2. Semantic Web 3. RDF Resource Description Framework 4. Semantic Web Resources 5. Information Extraction 6. Taxonomy Induction 7. Ontology Mapping. By the end of the course, the student must be able to:
Characterize the main tasks performed by information systems, namely data, information and knowledge management
Apply semi-structured data models, their representation through Web standards and algorithms for storing and processing semi-structured data
Apply fundamental models and techniques of text retrieval and their use in Web search engines
Apply main categories of data mining techniques, local rules, predictive and descriptive models, and master representative algorithms for each of the categories
Apply collaborative information management models, like crowd-sourcing, recommender systems, social networks","Web analytics enhancing Project Planning: the case of Digital Marketing campaigns: Qualitative study of structured Web analytics data in Project Management enable the process of trace and read virtual traffic, by learning how the user interacts  time and
date at which it occurred, and the characteristics of the machine from which  research on project
planning with Web mining, especially looking at Web analytics data: this combination  
",195,3,0.81862876812617,2,0.836484879255295
294,"Human health and social work activities","Data visualization",0.832185983657837,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","Web analytics enhancing Project Planning: the case of Digital Marketing campaigns: Qualitative study of structured Web analytics data in Project Management enable the process of trace and read virtual traffic, by learning how the user interacts  time and
date at which it occurred, and the characteristics of the machine from which  research on project
planning with Web mining, especially looking at Web analytics data: this combination  
",195,10,0.81925385594368,2,0.849512100219727
296,"Human health and social work activities","Bid Data Measuring Systems",0.784844696521759,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Health maintenance advisory technology for specific business sectors, eg utilities or tourism; G06Q50/10Services; G06Q50/22Socialwork  be passed to the query engine 72 as a confirmed result for machine learning algorithms
that  The user condition may be sensed by accessing such data from the server system, at  
",195,24,0.826420515775681,2,0.81271168589592
331,"Human health and social work activities","Intelligent Agents and Multi-Agent Systems",0.81759774684906,"Multiagent systems are systems consisting of several autonomous entities called agents that interact among themselves in order to solve problems that exceed the individual capabilities of each or solving them in a more efficient way. This interaction is the main object of research in multiagent systems, and it has contributed to different disciplines such as Social Science, Game Theory and Artificial Intelligence. In this module, besides studying these contributions, we will introduce the students to the practice of research in any area related to multi-agent systems, and the preparation of papers describing the results of its research activity","Analysing large volumes of complex qualitative data-Reflections from a group of international experts Georgia Philip is a Research Fellow in the School of Social Work, at the University of East Anglia 
analysis?' In so doing, he considers the advantages and challenges of using Machine Learning
to assist with coding and help researchers handle large volumes of data in a  
",195,4,0.801681563258171,2,0.800006747245789
335,"Human health and social work activities","Research Methodology",0.837987840175629,"This seminar tries to inform and guide the students about techniques, most common standards and systems for the practice of scientific research and its methodological bases and documentaries. The topics are as follows: General Approach (scientific knowledge and its purpose, problems of scientific research, research works); Scientific Work (choice of subject, setting objectives, formulating hypotheses, choice of work method, choice of tools and resources. Phases of work); Information Search (sources, publications, bibliographical searches, access to scientific documentation, internet, etc.); Work Writing (rules, principles, tips, style, language, etc.) and Presentation and Defence of Work (legal aspects, formal aspects, personal aspects, visual aids to support the presentation)","[BOOK][B] Scientific Reasoning and Argumentation: The Roles of Domain-specific and Domain-general Knowledge Library of Congress Cataloging-in-Publication Data Names: Fischer, Frank, 1942- editor  Christian
Ghanem, Theories and Methods of Social Work, Katholische Stiftungshochschule München
(KSH  for the limits of domain-generality based on work in machine learning and natural  
",195,4,0.810593381524086,2,0.817110300064087
346,"Human health and social work activities","Fairness, Explainability, and Accountability for ML",0.808589100837708,"- Familiarize students with the ethical implications of applying Big Data and ML tools to socially-sensitive domains; teach them to think critically about these issues.
- Overview the long-established philosophical, sociological, and economic literature on these subjects.
- Provide students with a tool-box of technical solutions for addressing - at least partially - the ethical and societal issues of ML and Big data.        As ML continues to advance and make its way into different aspects of modern life, both the designers and users of the technology need to think seriously about its impact on individuals and society. We will study some of the ethical implications of applying ML tools to socially sensitive domains, such as employment, education, credit ledning, and criminal justice. We will discuss at length what it means for an algorithm to be fair; who should be held responsible when algorithmic decisions negatively impacts certain demographic groups or individuals; and last but not least, how algorithmic decisions can be explained to a non-technical audience. Throughout the course, we will focus on technical solutions that have been recently proposed by the ML community to tackle the above issues. We will critically discuss the advantages and shortcomings of these proposals in comparison with non-technical alternatives.","[PDF][PDF] The future of well-being in a tech-saturated world Dangers: Tiziana Dearing, a professor at the Boston College School of Social Work, said, People's
well  lives, technology companies will find new, invasive ways to exploit data generated on  As
we enter the Artificial Intelligence era we must examine and make transparent how  
",195,3,0.80324133237203,2,0.801738321781158
348,"Human health and social work activities","Biomedical Informatics",0.789399862289429,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","[PDF][PDF] The future of well-being in a tech-saturated world Dangers: Tiziana Dearing, a professor at the Boston College School of Social Work, said, People's
well  lives, technology companies will find new, invasive ways to exploit data generated on  As
we enter the Artificial Intelligence era we must examine and make transparent how  
",195,18,0.817355437411202,2,0.822475731372833
360,"Human health and social work activities","Bid Data Measuring Systems",0.840578675270081,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Tracking and Controlling Inter-System Processing Events Using Event Tokens or tourism; G06Q50/10Services; G06Q50/22Health care, eg hospitals; Social work;  disclosure
relate to data processing, artificial intelligence, and using artificial intelligence-enabled data  event
token management computing platform 110 may receive adjudication data from a  
",195,24,0.826420515775681,2,0.81271168589592
364,"Human health and social work activities","Data Analysis and Integration",0.816417157649994,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Tracking and Controlling Inter-System Processing Events Using Event Tokens or tourism; G06Q50/10Services; G06Q50/22Health care, eg hospitals; Social work;  disclosure
relate to data processing, artificial intelligence, and using artificial intelligence-enabled data  event
token management computing platform 110 may receive adjudication data from a  
",195,20,0.825102084875107,2,0.82326066493988
384,"Human health and social work activities","Data Analysis and Integration",0.830104172229767,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","[BOOK][B] Toward information justice: Technology, politics, and policy for data in higher education administration 31 2.2 Big Data in Higher Education  scientists and users to accept current data practices and
outcomes as natural or inevitable, and to make data use the  Even in manual technolo- gies, the
technique reduces the human to machine, carrying out tasks as if human practitioners  
",195,20,0.825102084875107,2,0.82326066493988
392,"Human health and social work activities","Research Methodology",0.796232759952545,"This seminar tries to inform and guide the students about techniques, most common standards and systems for the practice of scientific research and its methodological bases and documentaries. The topics are as follows: General Approach (scientific knowledge and its purpose, problems of scientific research, research works); Scientific Work (choice of subject, setting objectives, formulating hypotheses, choice of work method, choice of tools and resources. Phases of work); Information Search (sources, publications, bibliographical searches, access to scientific documentation, internet, etc.); Work Writing (rules, principles, tips, style, language, etc.) and Presentation and Defence of Work (legal aspects, formal aspects, personal aspects, visual aids to support the presentation)","[BOOK][B] Trends and Issues in Interdisciplinary Behavior and Social Science: Proceedings of the 5th International Congress on Interdisciplinary Behavior and Social  light on researches about depression, especially in the Malaysian context as there is a big lack
of  Besides, the systematic data collection and analysis method are suggested to be replicated
for other phenomenological studies  Journal of Social Work Practice, 13(2), 135145  
",195,4,0.810593381524086,2,0.817110300064087
393,"Human health and social work activities","Fairness, Explainability, and Accountability for ML",0.794887542724609,"- Familiarize students with the ethical implications of applying Big Data and ML tools to socially-sensitive domains; teach them to think critically about these issues.
- Overview the long-established philosophical, sociological, and economic literature on these subjects.
- Provide students with a tool-box of technical solutions for addressing - at least partially - the ethical and societal issues of ML and Big data.        As ML continues to advance and make its way into different aspects of modern life, both the designers and users of the technology need to think seriously about its impact on individuals and society. We will study some of the ethical implications of applying ML tools to socially sensitive domains, such as employment, education, credit ledning, and criminal justice. We will discuss at length what it means for an algorithm to be fair; who should be held responsible when algorithmic decisions negatively impacts certain demographic groups or individuals; and last but not least, how algorithmic decisions can be explained to a non-technical audience. Throughout the course, we will focus on technical solutions that have been recently proposed by the ML community to tackle the above issues. We will critically discuss the advantages and shortcomings of these proposals in comparison with non-technical alternatives.","[BOOK][B] Trends and Issues in Interdisciplinary Behavior and Social Science: Proceedings of the 5th International Congress on Interdisciplinary Behavior and Social  light on researches about depression, especially in the Malaysian context as there is a big lack
of  Besides, the systematic data collection and analysis method are suggested to be replicated
for other phenomenological studies  Journal of Social Work Practice, 13(2), 135145  
",195,3,0.80324133237203,2,0.801738321781158
395,"Human health and social work activities","Introduction to natural language processing",0.7948899269104,"The objective of this course is to present the main models, formalisms and algorithms necessary for the development of applications in the field of natural language information processing. The concepts introduced during the lectures will be applied during practical sessions.Several models and algorithms for automated textual data processing will be described: (1) morpho-lexical level: electronic lexica, spelling checkers, ...; (2) syntactic level: regular, context-free, stochastic grammars, parsing algorithms, ...; (3) semantic level: models and formalisms for the representation of meaning, ...

Several application domains will be presented: Linguistic engineering, Information Retrieval, Text mining (automated knowledge extraction), Textual Data Analysis (automated document classification, visualization of textual data).

Keywords
Natural Language Processing; Computationnal Linguisitics; Part-of-Speech tagging; Parsing.By the end of the course, the student must be able to:
Compose key NLP elements to develop higher level processing chains
Assess / Evaluate NLP based systems
Choose appropriate solutions for solving typical NLP subproblems (tokenizing, tagging, parsing)
Describe the typical problems and processing layers in NLP
Analyze NLP problems to decompose them in adequate independant components.","Efficiency, Correctness, and the Authority of Automation: Technology in College Basic Writing Instruction 101 DATA ANALYSIS  Yet there also are moments of authentic possibility for broader learning
and understanding through the use of the automated system  Virtually all remedial English at
the college level could be handled by automation, with the machine as an impartial judge  
",195,3,0.835448265075684,2,0.838435351848602
424,"Agriculture, forestry and fishing","Applied biostatistics",0.849672734737396,"This course covers topics in applied biostatistics, with an emphasis on practical aspects of data analysis using R statistical software. Topics include types of studies and their design and analysis, high dimensional data analysis (genetic/genomic) and other topics as time and interest permit.Types of studies
Design and analysis of studies
R statistical software
Reproducible research techniques and tools
Report writing
Exploratory data analysis
Liniear modeling (regression, anova)
Generalized linear modeling (logistic, Poission)
Survival analysis
Discrete data analysis
Meta-analysis
High dimensional data analysis (genetics/genomics applications)
Additional topics as time and interest permit
Keywords
Data analysis, reproducible research, statistical methods, R, biostatistical data analysis, statistical data analysis. By the end of the course, the student must be able to:
Interpret analysis results
Justify analysis plan
Plan analysis for a given dataset
Analyze various types of biostatistical data
Synthesize analysis into a written report
Report plan of analysis and results obtained
Transversal skills
Write a scientific or technical report.
Assess one's own level of skill acquisition, and plan their on-going learning goals.
Take feedback (critique) and respond in an appropriate manner.
Use a work methodology appropriate to the task.","A review on the practice of big data analysis in agriculture to the other research areas employing big data analysis, agriculture ranks at  stations, humans
as sensors, web-based data, GIS geospatial data, feeds from  web services, mobile applications,
statistical analysis, modeling, simulation, benchmarking, big data storage, message  
",180,4,0.835054457187653,2,0.842559844255447
442,"Agriculture, forestry and fishing","Enterprise Integration",0.858299314975739,"The main goal of this course is to provide a broad and in-depth view of the concepts, methodologies, and technologies associated with systems integration, including the integration of applications, services, and inter-organizational business processes. The topics addressed in this course are positioned at a key point between the application infrastructure and the business processes in an organization, and the aim is to understand the relationships and dependencies between the two. The course will also provide insight into how it is possible to devise a distributed and integrated application infrastructure. The concrete learning objectives are as follows: 1. To provide an in-depth view of the main concepts and integration solutions in the field of integration; 2. To develop a systematic and process-oriented vision of how integration problems should be addressed; 3. To acquire a practical knowledge of the state-of-the-art integration platforms, based on lab projects; 4. To understand the critical role that integration solutions have in the design and implementation of business processes.The course aims at providing a coherent structure of integration topics that can be found in different parts of the ACM/AIS IS 2010 curriculum, such as “Enterprise Systems” and
“Application Development”. When appropriate, this syllabus is labeled with topics from that curiculum and also from the ACM CCS 2012 taxonomy:
1. Evolution of information systems
a. essential functions of information systems in business organizations;
b. evolution of information systems architecture over the years; point-to-point vs. centralized integration;
c. integration based on the concept of service.
ACM/AIS IS 2010.1 Information Systems in Organizations
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise Information Systems
2. Introduction to integration platforms
a. message exchange;
b. message schema and transformation;
c. ports and adapters;
d. orchestrations;
e. business rules.
ACM/AIS IS 2010.3 Systems Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Service buses;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business rules
3. Messaging systems
a. fundamental concepts;
b. message transactions;
c. message acknowledgments;
d. message correlation;
e. messaging platforms.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise interoperability > Enterprise application integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Message queues
4. Message brokers
a. message-level integration vs. orchestration-level integration;
b. publish-subscribe with message filters;
c. message properties;
d. message correlation;
e. asynchronous messaging.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
5. Adapters
a. three-tier client-server model;
b. capture of the user interface;
c. integration through files;
d. database access APIs;
e. retrieving data in XML;
f. data access in orchestrations;
g. methods and interfaces;
h. interface discovery and dynamic invocations;
i. Web service invocation in orchestrations.
ACM/AIS IS 2010.3 Data Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration
ACM CCS 2012 Information Systems > World Wide Web > Web services
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
6. Services and SOA
a. services and applications;
b. service composition;
c. service orchestration;
d. business processes;
e. service design principles;
f. benefits of SOA;
g. support for human workflows.
ACM/AIS IS 2010.3 Service oriented architecture
ACM CCS 2012 Applied Computing > Enterprise Computing > Service-oriented architectures;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Business process management systems
7. Service orchestrations
a. block structure;
b. beginning the flow;
c. message construction;
d. flow control with loops, decisions, and parallelism;
e. orchestrations as sub-processes;
f. concurrent events;
g. correlations;
h. exception handling;
i. transactions and compensation.
ACM/AIS IS 2010.ES Business process integration
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors
8. Inter-organizational integration
a. electronic data exchange;
b. introduction to supply chain management;
c. supply chain coordination;
d. electronic commerce;
e. negotiation protocols.
ACM/AIS IS 2010.1 Supply Chain Management
ACM/AIS IS 2010.ES Production logistics
ACM CCS 2012 Information Systems > World Wide Web > Web applications > Electronic commerce > Electronic data interchange;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Cross-organizational business processes
9. Internet of things
a. physical world and virtual world integration;
b. traceability systems;
c. sensors and complex event processing;
d. logistics systems based on RFID.
ACM/AIS IS 2010.ES Enterprise Systems > Production logistics
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise computing > Event-driven architectures
ACM CCS 2012 Information systems > Information systems applications > Spatial-temporal systems > Data streaming","IoT, big data science & analytics, cloud computing and mobile app based hybrid system for smart agriculture Aadhar linked agricultural information network can be easily conceptualized using mobile
communication and big data analytics operating on geo-spatial data already available with Ministry
of agriculture, ISRO, Survey of India to optimize resource availability, spread and  
",180,7,0.81805990423475,2,0.832004517316818
456,"Agriculture, forestry and fishing","Biomedical Informatics",0.807395160198212,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","New trends in precision agriculture: a novel cloud-based system for enabling data storage and agricultural task planning and automationIt is well-known that information and communication technologies enable many tasks in the context of precision agriculture. In fact, more and more farmers and food and agriculture companies are using precision agriculture-based systems to enhance not only their products ",180,18,0.817355437411202,2,0.819420605897903
492,"Agriculture, forestry and fishing","Mathematical foundations of signal processing",0.804996967315674,"Signal processing tools are presented from an intuitive geometric point of view which is at the heart of all modern signal processing techniques. Student will develop the mathematical depth and rigor needed for the study of advanced topics in signal processing.
Content
From Euclid to Hilbert applied to inverse problems (vector spaces; Hilbert spaces; approximations, projections and decompositions; bases)

Sequences, Discrete-Time Systems, Functions and Continuous-Time Systems (flipped class review of discrete-time Fourier transform; z-transform; DFT; Fourier transform and Fourier series).

Sampling and Interpolation (sampling and interpolation with finite-dimensional vectors, sequences and functions)

 

Computerized tomography fundamentals (line integrals and projections, Radon transform, Fourier projection/slice theorem, filtered backprojection algorithm, algebraic reconstruction techniques).
 

Array signal processing fundamentals (spatial filtering and beamforming, adaptive beamforming, acoustic and EM source localization techniques).
 

Compressed sensing and finite rate of innovation (overview and definitions, reconstruction methods and applications)
Euclidean Distance Matrices (definition, properties and applications).By the end of the course, the student must be able to:
Master the right tools to tackle advanced signal and data processing problems
Develop an intuitive understanding of signal processing through a geometrical approach
Get to know the applications that are of interest today
Learn about topics that are at the forefront of signal processing research.","Multi-sensor Data Fusion Algorithm of Wisdom Agriculture Based on Fusion SetIn wisdom agriculture, the advanced high-tech equipment is applied and human input is reduced to lower the operation and management costs and enhance agricultural management efficiency. In this thesis, a multi-sensor data fusion algorithm based on fusion ",180,2,0.816374719142914,2,0.816374719142914
493,"Agriculture, forestry and fishing","Optimization for Data Science",0.803340435028076,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","Multi-sensor Data Fusion Algorithm of Wisdom Agriculture Based on Fusion SetIn wisdom agriculture, the advanced high-tech equipment is applied and human input is reduced to lower the operation and management costs and enhance agricultural management efficiency. In this thesis, a multi-sensor data fusion algorithm based on fusion ",180,15,0.81723126967748,2,0.826172560453415
509,"Agriculture, forestry and fishing","Enterprise Integration",0.805709719657898,"The main goal of this course is to provide a broad and in-depth view of the concepts, methodologies, and technologies associated with systems integration, including the integration of applications, services, and inter-organizational business processes. The topics addressed in this course are positioned at a key point between the application infrastructure and the business processes in an organization, and the aim is to understand the relationships and dependencies between the two. The course will also provide insight into how it is possible to devise a distributed and integrated application infrastructure. The concrete learning objectives are as follows: 1. To provide an in-depth view of the main concepts and integration solutions in the field of integration; 2. To develop a systematic and process-oriented vision of how integration problems should be addressed; 3. To acquire a practical knowledge of the state-of-the-art integration platforms, based on lab projects; 4. To understand the critical role that integration solutions have in the design and implementation of business processes.The course aims at providing a coherent structure of integration topics that can be found in different parts of the ACM/AIS IS 2010 curriculum, such as “Enterprise Systems” and
“Application Development”. When appropriate, this syllabus is labeled with topics from that curiculum and also from the ACM CCS 2012 taxonomy:
1. Evolution of information systems
a. essential functions of information systems in business organizations;
b. evolution of information systems architecture over the years; point-to-point vs. centralized integration;
c. integration based on the concept of service.
ACM/AIS IS 2010.1 Information Systems in Organizations
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise Information Systems
2. Introduction to integration platforms
a. message exchange;
b. message schema and transformation;
c. ports and adapters;
d. orchestrations;
e. business rules.
ACM/AIS IS 2010.3 Systems Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Service buses;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business rules
3. Messaging systems
a. fundamental concepts;
b. message transactions;
c. message acknowledgments;
d. message correlation;
e. messaging platforms.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise interoperability > Enterprise application integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Message queues
4. Message brokers
a. message-level integration vs. orchestration-level integration;
b. publish-subscribe with message filters;
c. message properties;
d. message correlation;
e. asynchronous messaging.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
5. Adapters
a. three-tier client-server model;
b. capture of the user interface;
c. integration through files;
d. database access APIs;
e. retrieving data in XML;
f. data access in orchestrations;
g. methods and interfaces;
h. interface discovery and dynamic invocations;
i. Web service invocation in orchestrations.
ACM/AIS IS 2010.3 Data Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration
ACM CCS 2012 Information Systems > World Wide Web > Web services
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
6. Services and SOA
a. services and applications;
b. service composition;
c. service orchestration;
d. business processes;
e. service design principles;
f. benefits of SOA;
g. support for human workflows.
ACM/AIS IS 2010.3 Service oriented architecture
ACM CCS 2012 Applied Computing > Enterprise Computing > Service-oriented architectures;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Business process management systems
7. Service orchestrations
a. block structure;
b. beginning the flow;
c. message construction;
d. flow control with loops, decisions, and parallelism;
e. orchestrations as sub-processes;
f. concurrent events;
g. correlations;
h. exception handling;
i. transactions and compensation.
ACM/AIS IS 2010.ES Business process integration
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors
8. Inter-organizational integration
a. electronic data exchange;
b. introduction to supply chain management;
c. supply chain coordination;
d. electronic commerce;
e. negotiation protocols.
ACM/AIS IS 2010.1 Supply Chain Management
ACM/AIS IS 2010.ES Production logistics
ACM CCS 2012 Information Systems > World Wide Web > Web applications > Electronic commerce > Electronic data interchange;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Cross-organizational business processes
9. Internet of things
a. physical world and virtual world integration;
b. traceability systems;
c. sensors and complex event processing;
d. logistics systems based on RFID.
ACM/AIS IS 2010.ES Enterprise Systems > Production logistics
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise computing > Event-driven architectures
ACM CCS 2012 Information systems > Information systems applications > Spatial-temporal systems > Data streaming","Machine learning based data processing and latency reduction in the internet of things for agricultureThe Internet of Things is best stated as a network of things that have the ability to generate and share information between themselves and interact with the environment according to the percepts from this environment. This network between these devices and humans ",180,7,0.81805990423475,2,0.832004517316818
538,"Agriculture, forestry and fishing","Mathematical foundations of signal processing",0.827752470970154,"Signal processing tools are presented from an intuitive geometric point of view which is at the heart of all modern signal processing techniques. Student will develop the mathematical depth and rigor needed for the study of advanced topics in signal processing.
Content
From Euclid to Hilbert applied to inverse problems (vector spaces; Hilbert spaces; approximations, projections and decompositions; bases)

Sequences, Discrete-Time Systems, Functions and Continuous-Time Systems (flipped class review of discrete-time Fourier transform; z-transform; DFT; Fourier transform and Fourier series).

Sampling and Interpolation (sampling and interpolation with finite-dimensional vectors, sequences and functions)

 

Computerized tomography fundamentals (line integrals and projections, Radon transform, Fourier projection/slice theorem, filtered backprojection algorithm, algebraic reconstruction techniques).
 

Array signal processing fundamentals (spatial filtering and beamforming, adaptive beamforming, acoustic and EM source localization techniques).
 

Compressed sensing and finite rate of innovation (overview and definitions, reconstruction methods and applications)
Euclidean Distance Matrices (definition, properties and applications).By the end of the course, the student must be able to:
Master the right tools to tackle advanced signal and data processing problems
Develop an intuitive understanding of signal processing through a geometrical approach
Get to know the applications that are of interest today
Learn about topics that are at the forefront of signal processing research.","Artificial Intelligence on Remote Sensing Data for Precision Agriculture ApplicationsPrecision agriculture benefits greatly from information provided by high spatial resolution and high temporal frequency remotely sensed images. It requires effective methodologies and algorithms to exact information from the huge volume, dimension and variety of raw ",180,2,0.816374719142914,2,0.816374719142914
546,"Agriculture, forestry and fishing","Biomedical Informatics",0.831446051597595,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector and shortcomings of currently available studies on emerging technologies in agriculture and
food  The opportunities of the new big-data-augmented methodology are shown in comparison
to  with special attention to use of bigger volumes of data, machine learning and ontology  
",180,18,0.817355437411202,2,0.819420605897903
584,"Agriculture, forestry and fishing","Applied biostatistics",0.835446953773499,"This course covers topics in applied biostatistics, with an emphasis on practical aspects of data analysis using R statistical software. Topics include types of studies and their design and analysis, high dimensional data analysis (genetic/genomic) and other topics as time and interest permit.Types of studies
Design and analysis of studies
R statistical software
Reproducible research techniques and tools
Report writing
Exploratory data analysis
Liniear modeling (regression, anova)
Generalized linear modeling (logistic, Poission)
Survival analysis
Discrete data analysis
Meta-analysis
High dimensional data analysis (genetics/genomics applications)
Additional topics as time and interest permit
Keywords
Data analysis, reproducible research, statistical methods, R, biostatistical data analysis, statistical data analysis. By the end of the course, the student must be able to:
Interpret analysis results
Justify analysis plan
Plan analysis for a given dataset
Analyze various types of biostatistical data
Synthesize analysis into a written report
Report plan of analysis and results obtained
Transversal skills
Write a scientific or technical report.
Assess one's own level of skill acquisition, and plan their on-going learning goals.
Take feedback (critique) and respond in an appropriate manner.
Use a work methodology appropriate to the task.","[CITATION][C] Analysis of agriculture data using data mining techniques: application of big data in the food and agriculture sectors: an analysis of the current models and results of a novel approach using machine learning techniques with retail scanner data",180,4,0.835054457187653,2,0.842559844255447
591,"Agriculture, forestry and fishing","Optimization for Data Science",0.849004685878754,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","[CITATION][C]  Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning Techniques with Retail Scanner DataBig data analytics framework for agriculture",180,15,0.81723126967748,2,0.826172560453415
605,"Arts, entertainment and recreation","Computer Vision",0.816683411598206,"Computer vision techniques are designed to extract properties of the world from a set of images. The guidance of an autonomous vehicle, the automated evaluation of the quality of a piece of pottery or an automatic immersion of a graphic character in a film are some examples of current applications of computer vision. The module's aim is to introduce students to the problems of vision and study the most common techniques for automatic analysis of images by computers. Special emphasis will be given to the study of the physical and geometrical fundamentals of vision. We will study issues such as imaging, modeling and calibration of cameras, stereovision, self-calibration, modeling and monitoring and object detection and analysis of human facial expression.","Computer art design based on artificial intelligenceThis article studies the design of computer art based on artificial intelligence in the digital media environment, and mainly discusses the re-creation of the classic animated images. By analyzing and summarizing the animated images, the components needed to form the ",35,2,0.810524791479111,2,0.810524791479111
606,"Arts, entertainment and recreation","Computer Vision",0.804366171360016,"Computer vision focuses on techniques and models for acquiring and analyzing images in order to understand objects and scenes in the real world. Computer vision is important for the construction of intelligent methods and techniques for (autonomous) systems that interpret sensory information and use that information to generate intelligent and goal-directed behavior.

Computer vision methods include image segmentation, object recognition and profiling, motion estimation, event detection, 3D scene reconstruction, human-behaviour analysis, faces and gesture recognition. The methods are studied using elements of geometry, physics and statistics.","Computer art design based on artificial intelligenceThis article studies the design of computer art based on artificial intelligence in the digital media environment, and mainly discusses the re-creation of the classic animated images. By analyzing and summarizing the animated images, the components needed to form the ",35,2,0.810524791479111,2,0.810524791479111
611,"Arts, entertainment and recreation","Natural Language Processing",0.809405922889709,"The purpose of this seminar derives from the need to fill a gap in the teaching of subjects that are, generally speaking, on Language Engineering. On the one hand, when we talk about Engineering, then we talk about design, methodologies, techniques, systems, and components; on the other hand, when we talk about language then we talk about grammars, corpora, dictionaries, etc. Usually, the teaching of these subjects often has a tendency, perhaps excessive, to one side or another. This seminar aims to provide a unified view of both sides, from the fundamentals to applications. The area of Linguistic Engineering is considered to be one of the areas where most research and development efforts will lie in the next few years, if we are to achieve the goal of having machines that really make our lives easier in a simple way. The seminar is focused, in the first part, on the state of the art technologies, followed by a second part where we will explore in depth technologies that allow supporting applications on the market. For practical reasons, the practice work will be focused in word processing technologies.","A System for Evolving Art Using Supervised Learning and Aesthetic AnalogiesAesthetic experience is an important aspect of creativity and our perception of the world around us. Analogy is a tool we use as part of the creative process to translate our perceptions into creative works of art. In this paper we present our research on the ",35,3,0.800235470136007,2,0.805308520793915
621,"Arts, entertainment and recreation","Bioinformatics",0.786673545837402,"Acquiring insight in handling and analysis of molecular biologica data using computational techniques.
Understanding the underlying principles of a selection of computational techniques and models that are frequently used in bioinformatics.
Being able to select the appropriate technique to solve a given problem, and being able to apply it.
Knowing how to use, access, search the most important public molecular biological databases.The bioinformatics course aims to give students essential insights in the most important computational techniques used for the analysis of molecular (system) biological data. The student will also learn how to select the right strategy for a given task.

Part I. Scope of Bioinformatics

1. introduction
2. essential background
3. databases

Part II. Analysis of Biomolecules

4. pairwise sequence alignment
5. multiple sequence alignment
6. sequence search
7. profiles and motifs
8. phylogenetic trees
9. protein structure

Part III. Analysis of Living Systems

10. molecular functions
11. genome analysis
12. multi-omics
13. network biology
14. human disease

The practical sessions give students a chance to apply some of the discussed algorithms to solve concrete problems.","[HTML][HTML] Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Artwith Reflections on Present AIM Background: The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970's led to a major change in the paradigm that ",35,6,0.80968498190244,2,0.785917609930038
622,"Arts, entertainment and recreation","Optimization for Data Science",0.786615133285522,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","[HTML][HTML] Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Artwith Reflections on Present AIM Background: The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970's led to a major change in the paradigm that ",35,15,0.81723126967748,2,0.81492081284523
623,"Arts, entertainment and recreation","Bioinformatics",0.785161674022675,"Bioinformatics aims at developing computational methods and algorithms to process biological data and uses mathematical and statistical modelling to generate testable hypotheses about biological entities and processes. The goal of this course is to introduce the basic techniques that support the most recent developments on this field. Additionally, it enables the development of the ability to critically assess research publications in this field. Practical assignments during the course aim at developing the student's ability to develop software for bioinformatics.Introduction, Molecular biology main concepts, Introduction to algorithms and complexity
Graphs and genetics
DNA sequence analysis
Pairwise alignment
Multiple Sequence alignment
Motif finding
NGS data, algorithms and data structures
Probabilistic models
Gene expression data analysis
Data mining
Unsupervised Learning: Clustering and Biclustering
Molecular phylogenetics
Supervised Learning: Decision trees, Bayesian methods
Integrative data analysis
Seminar","[HTML][HTML] Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Artwith Reflections on Present AIM Background: The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970's led to a major change in the paradigm that ",35,6,0.80968498190244,2,0.785917609930038
626,"Arts, entertainment and recreation","Optimization for Data Science",0.843226492404938,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","Machine Learning in Supply Chain Planning--When Art & Science Converge.This article discusses the important AI subset of Machine Learning and its application to the area of supply chain planning and optimization. It defines machine learning and how it relates to other advanced analytic methods including AI; predictive, prescriptive and ",35,15,0.81723126967748,2,0.81492081284523
631,"Arts, entertainment and recreation","Natural Language Processing",0.80121111869812,"Over the past few years, research towards natural language processing has shown strong evidence as to the effectiveness of models that involve both hierarchical structure as well as statistical learning from corpora. In this profile you will study the state-of-the-art statistical models for complex language processing tasks such as parsing, language modeling and machine translation.

A characteristic of some of these models is that they involve defining probability measures over hierarchical structure, e.g., trees and graphs. The profile covers supervised as well as unsupervised methods for learning these models directly from large training corpora and provides the necessary background for research in Computational Linguistics and Natural Language Processing.","[PDF][PDF] AIA. Artificial intelligence for artWe observe the success of artificial neural networks in simulating human performance on a number of tasks: such as image recognition, natural language processing, etc. However, there are limits to state of-the-art AI that separate it from human-like intelligence. Humans ",35,3,0.800235470136007,2,0.805308520793915
638,"Electricity, gas, steam and air conditioning supply","Network Science",0.833787858486176,"This course provides an introduction to the study of complex networks, including algorithms, models and applications to both artificial and real networks, including social, biological and technological networks, all sharing common features and properties. The course addresses the development of scalable algorithms and data structures so that we can efficiently study large complex networks, but also in the creation of theoretical models capable of describing empirically observed patterns. The number of applications is enormous, including web search engines, evolutionary dynamics, information diffusion on Internet, social networks and blogs, network resilience, network-driven phenomena in epidemiology and computer viruses, networks dynamics, with connections in the social sciences, physics, computational biology, and economics.Introduction to complex systems and networks science: Theory and basic concepts. Properties and characterization of biological, social and technological networks. Network models and random graphs. Efficient representation of large (sparse) networks. Succinct data-structures and coding strategies. Design and analysis of efficient and scalable algorithms for large network processing and analysis, including both sampling and randomization techniques. Databases and distributed platforms for the analysis of large networks. Link analysis and random walks. Community finding and graph partitioning. Ranking algorithms. Vertex relabeling. Dynamical processes on complex networks: The impact of network structure on economic, social and biological systems. Introduction to stochastic processes, Monte-Carlo simulations and large-scale multi-agent systems. Disease spreading and tolerance to attacks. Models of peer-influence and opinion formation. Game theory and population dynamics. Public goods problems, cooperation and reputation dynamics. Decision-making on (static and adaptive) interaction networks.","Big data analytics for discovering electricity consumption patterns in smart citiesNew technologies such as sensor networks have been incorporated into the management of buildings for organizations and cities. Sensor networks have led to an exponential increase in the volume of data available in recent years, which can be used to extract consumption ",105,5,0.808625149726868,2,0.81266850233078
735,"Electricity, gas, steam and air conditioning supply","Network Science",0.791549146175385,"This course provides an introduction to the study of complex networks, including algorithms, models and applications to both artificial and real networks, including social, biological and technological networks, all sharing common features and properties. The course addresses the development of scalable algorithms and data structures so that we can efficiently study large complex networks, but also in the creation of theoretical models capable of describing empirically observed patterns. The number of applications is enormous, including web search engines, evolutionary dynamics, information diffusion on Internet, social networks and blogs, network resilience, network-driven phenomena in epidemiology and computer viruses, networks dynamics, with connections in the social sciences, physics, computational biology, and economics.Introduction to complex systems and networks science: Theory and basic concepts. Properties and characterization of biological, social and technological networks. Network models and random graphs. Efficient representation of large (sparse) networks. Succinct data-structures and coding strategies. Design and analysis of efficient and scalable algorithms for large network processing and analysis, including both sampling and randomization techniques. Databases and distributed platforms for the analysis of large networks. Link analysis and random walks. Community finding and graph partitioning. Ranking algorithms. Vertex relabeling. Dynamical processes on complex networks: The impact of network structure on economic, social and biological systems. Introduction to stochastic processes, Monte-Carlo simulations and large-scale multi-agent systems. Disease spreading and tolerance to attacks. Models of peer-influence and opinion formation. Game theory and population dynamics. Public goods problems, cooperation and reputation dynamics. Decision-making on (static and adaptive) interaction networks.","Machine learning methods for the analysis of data of an Electricity Distribution Network OperatorOnce every few decades an invention changes the landscape of some aspects of our life. Industrial revolutions improved our everyday lives whilst medical revolutions expanded our lifespans. In the path we're leading, most of sciences will be reduced to computer science ",105,5,0.808625149726868,2,0.81266850233078
740,"Financial service activities, except insurance and pension funding","Computational Intelligence Lab",0.836783766746521,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","Machine learning in finance: A topic modeling approachWe provide a first comprehensive structuring of the literature applying machine learning to finance. We use a probabilistic topic modeling approach to make sense of this diverse body of research spanning across the disciplines of finance, economics, computer sciences, and ",135,11,0.82310460914265,2,0.825629889965057
741,"Financial service activities, except insurance and pension funding","Advanced Topics in Machine Learning",0.832185089588165,"In this seminar, recent papers of the pattern recognition and machine learning literature are presented and discussed. Possible topics cover statistical models in computer vision, graphical models and machine learning.        The seminar ""Advanced Topics in Machine Learning"" familiarizes students with recent developments in pattern recognition and machine learning. Original articles have to be presented and critically reviewed. The students will learn how to structure a scientific presentation in English which covers the key ideas of a scientific paper. An important goal of the seminar presentation is to summarize the essential ideas of the paper in sufficient depth while omitting details which are not essential for the understanding of the work. The presentation style will play an important role and should reach the level of professional scientific presentations.        The seminar will cover a number of recent papers which have emerged as important contributions to the pattern recognition and machine learning literature. The topics will vary from year to year but they are centered on methodological issues in machine learning like new learning algorithms, ensemble methods or new statistical models for machine learning applications. Frequently, papers are selected from computer vision or bioinformatics - two fields, which relies more and more on machine learning methodology and statistical models.","Machine learning in finance: A topic modeling approachWe provide a first comprehensive structuring of the literature applying machine learning to finance. We use a probabilistic topic modeling approach to make sense of this diverse body of research spanning across the disciplines of finance, economics, computer sciences, and ",135,4,0.827056899666786,2,0.833037286996841
748,"Financial service activities, except insurance and pension funding","Information Security",0.794683456420898,"This course provides an introduction to Information Security. The focus
is on fundamental concepts and models, basic cryptography, protocols and system security, and privacy and data protection. While the emphasis is on foundations, case studies will be given that examine different realizations of these ideas in practice.        Master fundamental concepts in Information Security and their
application to system building. (See objectives listed below for more details).        1. Introduction and Motivation (OBJECTIVE: Broad conceptual overview of information security) Motivation: implications of IT on society/economy, Classical security problems, Approaches to 
defining security and security goals, Abstractions, assumptions, and trust, Risk management and the human factor, Course verview. 2. Foundations of Cryptography (OBJECTIVE: Understand basic 
cryptographic mechanisms and applications) Introduction, Basic concepts in cryptography: Overview, Types of Security, computational hardness, Abstraction of channel security properties, Symmetric 
encryption, Hash functions, Message authentication codes, Public-key distribution, Public-key cryptosystems, Digital signatures, Application case studies, Comparison of encryption at different layers, VPN, SSL, Digital payment systems, blind signatures, e-cash, Time stamping 3. Key Management and Public-key Infrastructures (OBJECTIVE: Understand the basic mechanisms relevant in an Internet context) Key management in distributed systems, Exact characterization of requirements, the role of trust, Public-key Certificates, Public-key Infrastructures, Digital evidence and non-repudiation, Application case studies, Kerberos, X.509, PGP. 4. Security Protocols (OBJECTIVE: Understand network-oriented security, i.e.. how to employ building blocks to secure applications in (open) networks) Introduction, Requirements/properties, Establishing shared secrets, Principal and message origin authentication, Environmental assumptions, Dolev-Yao intruder model and 
variants, Illustrative examples, Formal models and reasoning, Trace-based interleaving semantics, Inductive verification, or model-checking for falsification, Techniques for protocol design, 
Application case study 1: from Needham-Schroeder Shared-Key to Kerberos, Application case study 2: from DH to IKE. 5. Access Control and Security Policies (OBJECTIVES: Study system-oriented security, i.e., policies, models, and mechanisms) Motivation (relationship to CIA, relationship to Crypto) and examples Concepts: policies versus models versus mechanisms, DAC and MAC, Modeling formalism, Access Control Matrix Model, Roll Based Access Control, Bell-LaPadula, Harrison-Ruzzo-Ullmann, Information flow, Chinese Wall, Biba, Clark-Wilson, System mechanisms: Operating Systems, Hardware Security Features, Reference Monitors, File-system protection, Application case studies 6. Anonymity and Privacy (OBJECTIVE: examine protection goals beyond standard CIA and corresponding mechanisms) Motivation and Definitions, Privacy, policies and policy languages, mechanisms, problems, Anonymity: simple mechanisms (pseudonyms, proxies), Application case studies: mix networks and crowds. 7. Larger application case study: GSM, mobility
","Expert Systems in Finance: Smart Financial Applications in Big Data EnvironmentsThroughout the industry, financial institutions seek to eliminate cumbersome authentication methods, such as PINs, passwords, and security questions, as these antiquated tactics prove increasingly weak. Thus, many organizations now aim to implement emerging ",135,4,0.811841815710068,2,0.801391869783401
750,"Financial service activities, except insurance and pension funding","Machine learning",0.824619829654694,"Machine learning and data analysis are becoming increasingly central in many sciences and applications. In this course, fundamental principles and methods of machine learning will be introduced, analyzed and practically implemented.Basic regression and classification concepts and methods: Linear models, overfitting, linear regression, Ridge regression, logistic regression, and k-NN.
Fundamental concepts: cost-functions and optimization, cross-validation and bias-variance trade-off, curse of dimensionality.
Unsupervised learning: k-Means Clustering, Gaussian mixture models and the EM algorithm.
Dimensionality reduction: PCA and matrix factorization, word embeddings
Advanced methods: generalized linear models, SVMs and Kernel methods, Neural networks and deep learning. By the end of the course, the student must be able to:
Define the following basic machine learning problems: Regression, classification, clustering, dimensionality reduction, time-series
Explain the main differences between them
Implement algorithms for these machine learning models
Optimize the main trade-offs such as overfitting, and computational cost vs accuracy
Implement machine learning methods to real-world problems, and rigorously evaluate their performance using cross-validation. Experience common pitfalls and how to overcome them
Explain and understand the fundamental theory presented for ML methods","Machine learning for quantitative finance: fast derivative pricing, hedging and fittingIn this paper, we show how we can deploy machine learning techniques in the context of traditional quant problems. We illustrate that for many classical problems, we can arrive at speed-ups of several orders of magnitude by deploying machine learning techniques based ",135,4,0.840624004602432,2,0.832675695419312
751,"Financial service activities, except insurance and pension funding","Artificial Intelligence for Robotics",0.820502936840057,"This course provides tools from statistics and machine learning enabling the participants to deploy them as part of typical perception pipelines. All methods provided within the course will be discussed in context of and motivated by example applications from robotics. The accompanying exercises will involve implementations and evaluations using typical robotic datasets.        Working knowledge of basic methods from statistics and machine learning.        Probability Recap; Basic Concepts of Machine Learning; Regression; Dimensionality Reduction; Clustering; Support Vector Machines; Deep Learning;","Machine learning for quantitative finance: fast derivative pricing, hedging and fittingIn this paper, we show how we can deploy machine learning techniques in the context of traditional quant problems. We illustrate that for many classical problems, we can arrive at speed-ups of several orders of magnitude by deploying machine learning techniques based ",135,4,0.830730319023132,2,0.835348665714264
794,"Financial service activities, except insurance and pension funding","Machine learning",0.840731561183929,"Machine learning and data analysis are becoming increasingly central in many sciences and applications. In this course, fundamental principles and methods of machine learning will be introduced, analyzed and practically implemented.Basic regression and classification concepts and methods: Linear models, overfitting, linear regression, Ridge regression, logistic regression, and k-NN.
Fundamental concepts: cost-functions and optimization, cross-validation and bias-variance trade-off, curse of dimensionality.
Unsupervised learning: k-Means Clustering, Gaussian mixture models and the EM algorithm.
Dimensionality reduction: PCA and matrix factorization, word embeddings
Advanced methods: generalized linear models, SVMs and Kernel methods, Neural networks and deep learning. By the end of the course, the student must be able to:
Define the following basic machine learning problems: Regression, classification, clustering, dimensionality reduction, time-series
Explain the main differences between them
Implement algorithms for these machine learning models
Optimize the main trade-offs such as overfitting, and computational cost vs accuracy
Implement machine learning methods to real-world problems, and rigorously evaluate their performance using cross-validation. Experience common pitfalls and how to overcome them
Explain and understand the fundamental theory presented for ML methods","[PDF][PDF] Machine Learning Projection Methods for Macro-Finance ModelsThis paper develops a global solution method to solve large state space macro-finance models using machine learning. Our new method, an artificial neural network expectation algorithm, is not only considerably faster but also as precise and more scalable than the ",135,4,0.840624004602432,2,0.832675695419312
798,"Financial service activities, except insurance and pension funding","Information Security",0.808100283145905,"This course provides an introduction to Information Security. The focus
is on fundamental concepts and models, basic cryptography, protocols and system security, and privacy and data protection. While the emphasis is on foundations, case studies will be given that examine different realizations of these ideas in practice.        Master fundamental concepts in Information Security and their
application to system building. (See objectives listed below for more details).        1. Introduction and Motivation (OBJECTIVE: Broad conceptual overview of information security) Motivation: implications of IT on society/economy, Classical security problems, Approaches to 
defining security and security goals, Abstractions, assumptions, and trust, Risk management and the human factor, Course verview. 2. Foundations of Cryptography (OBJECTIVE: Understand basic 
cryptographic mechanisms and applications) Introduction, Basic concepts in cryptography: Overview, Types of Security, computational hardness, Abstraction of channel security properties, Symmetric 
encryption, Hash functions, Message authentication codes, Public-key distribution, Public-key cryptosystems, Digital signatures, Application case studies, Comparison of encryption at different layers, VPN, SSL, Digital payment systems, blind signatures, e-cash, Time stamping 3. Key Management and Public-key Infrastructures (OBJECTIVE: Understand the basic mechanisms relevant in an Internet context) Key management in distributed systems, Exact characterization of requirements, the role of trust, Public-key Certificates, Public-key Infrastructures, Digital evidence and non-repudiation, Application case studies, Kerberos, X.509, PGP. 4. Security Protocols (OBJECTIVE: Understand network-oriented security, i.e.. how to employ building blocks to secure applications in (open) networks) Introduction, Requirements/properties, Establishing shared secrets, Principal and message origin authentication, Environmental assumptions, Dolev-Yao intruder model and 
variants, Illustrative examples, Formal models and reasoning, Trace-based interleaving semantics, Inductive verification, or model-checking for falsification, Techniques for protocol design, 
Application case study 1: from Needham-Schroeder Shared-Key to Kerberos, Application case study 2: from DH to IKE. 5. Access Control and Security Policies (OBJECTIVES: Study system-oriented security, i.e., policies, models, and mechanisms) Motivation (relationship to CIA, relationship to Crypto) and examples Concepts: policies versus models versus mechanisms, DAC and MAC, Modeling formalism, Access Control Matrix Model, Roll Based Access Control, Bell-LaPadula, Harrison-Ruzzo-Ullmann, Information flow, Chinese Wall, Biba, Clark-Wilson, System mechanisms: Operating Systems, Hardware Security Features, Reference Monitors, File-system protection, Application case studies 6. Anonymity and Privacy (OBJECTIVE: examine protection goals beyond standard CIA and corresponding mechanisms) Motivation and Definitions, Privacy, policies and policy languages, mechanisms, problems, Anonymity: simple mechanisms (pseudonyms, proxies), Application case studies: mix networks and crowds. 7. Larger application case study: GSM, mobility
","Application of information systems aimed at big data use in the sphere of state finance management: Concept schemeThe article is devoted to the application of big data technologies in public administration, in particular, in public financial management. The paper outlines the basic principles of effective public financial management and describes the development trends of state ",135,4,0.811841815710068,2,0.801391869783401
828,"Financial service activities, except insurance and pension funding","Hardware Architectures for Machine Learning",0.829664051532745,"The seminar covers recent results in the increasingly important field of hardware acceleration for data science and machine learning, both in dedicated machines or in data centers.        The seminar aims at students interested in the system aspects of machine learning, who are willing to bridge the gap across traditional disciplines: machine learning, databases, systems, and computer architecture.        The seminar is intended to cover recent results in the increasingly important field of hardware acceleration for data science and machine learning, both in dedicated machines or in data centers.","Research on the Core Competence and Training System of Computer Speciality for Big Data Processing in Colleges and Universities of Finance and EconomicsIn big data era, the computer professionals should be equipped with the ability of big data processing and analysis. And multi-disciplinary collaborative innovation and interdisciplinary cross-learning ability is becoming more and more important. According to ",135,4,0.815778344869614,2,0.843169331550598
851,"Financial service activities, except insurance and pension funding","Advanced Topics in Machine Learning",0.833889484405518,"In this seminar, recent papers of the pattern recognition and machine learning literature are presented and discussed. Possible topics cover statistical models in computer vision, graphical models and machine learning.        The seminar ""Advanced Topics in Machine Learning"" familiarizes students with recent developments in pattern recognition and machine learning. Original articles have to be presented and critically reviewed. The students will learn how to structure a scientific presentation in English which covers the key ideas of a scientific paper. An important goal of the seminar presentation is to summarize the essential ideas of the paper in sufficient depth while omitting details which are not essential for the understanding of the work. The presentation style will play an important role and should reach the level of professional scientific presentations.        The seminar will cover a number of recent papers which have emerged as important contributions to the pattern recognition and machine learning literature. The topics will vary from year to year but they are centered on methodological issues in machine learning like new learning algorithms, ensemble methods or new statistical models for machine learning applications. Frequently, papers are selected from computer vision or bioinformatics - two fields, which relies more and more on machine learning methodology and statistical models.","Essays on machine learning for economics and financeEconometrics and machine learning are quite close and related concepts. Nowadays, it is always more important to extract value from raw data, and distilling actionable insights from quantitative values as well as qualitative features. In order to deal with these topics, the first ",135,4,0.827056899666786,2,0.833037286996841
868,"Financial service activities, except insurance and pension funding","Computational Intelligence Lab",0.814476013183594,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","Using Robotic Process Automation in Finance organizations: Case studySoftware robotics has emerged as a new technological development over the last few years, offering a lot of potential for optimizing and improving processes. Software robotics offers a new tool also for finance organizations, where there are still number of manual tasks being ",135,11,0.82310460914265,2,0.825629889965057
871,"Financial service activities, except insurance and pension funding","Hardware Architectures for Machine Learning",0.856674611568451,"The seminar covers recent results in the increasingly important field of hardware acceleration for data science and machine learning, both in dedicated machines or in data centers.        The seminar aims at students interested in the system aspects of machine learning, who are willing to bridge the gap across traditional disciplines: machine learning, databases, systems, and computer architecture.        The seminar is intended to cover recent results in the increasingly important field of hardware acceleration for data science and machine learning, both in dedicated machines or in data centers.","Machine learning with applications to financeThe impact of data driven, machine learning technologies across a wide variety of fields is undeniable. The financial industry, which relies heavily on predictive modeling being no exception. In this work we summarize two widely used machine learning models: support ",135,4,0.815778344869614,2,0.843169331550598
872,"Financial service activities, except insurance and pension funding","Artificial Intelligence for Robotics",0.85019439458847,"This course provides tools from statistics and machine learning enabling the participants to deploy them as part of typical perception pipelines. All methods provided within the course will be discussed in context of and motivated by example applications from robotics. The accompanying exercises will involve implementations and evaluations using typical robotic datasets.        Working knowledge of basic methods from statistics and machine learning.        Probability Recap; Basic Concepts of Machine Learning; Regression; Dimensionality Reduction; Clustering; Support Vector Machines; Deep Learning;","Machine learning with applications to financeThe impact of data driven, machine learning technologies across a wide variety of fields is undeniable. The financial industry, which relies heavily on predictive modeling being no exception. In this work we summarize two widely used machine learning models: support ",135,4,0.830730319023132,2,0.835348665714264
876,"Public administration and defence, compulsory social security","Model Driven Engineering",0.798747837543488,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","To do more, better, faster and more cheaply: Using big data in public administrationBig data have become a game-changer for modern public administration in those areas in which they are used. Although their application is still limited in the public sector, their use develops dynamically in areas where they bring tangible results in terms of efficiency and ",110,9,0.815234687593248,2,0.810649544000626
886,"Public administration and defence, compulsory social security","Deep Learning",0.8564173579216,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Machine learning for public administration research, with application to organizational reputationAbstract Machine learning methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories ",110,13,0.819467407006484,2,0.854275315999985
897,"Public administration and defence, compulsory social security","Artificial Intelligence and Data Science",0.836762070655823,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","A Study on Conversational Public Administration Service of the Chatbot Based on Artificial IntelligenceArtificial intelligence-based services are expanding into a new industrial revolution. There is artificial intelligence technology applied in real life due to the development of big data and deep learning related technology. And data analysis and intelligent assistant services that ",110,47,0.827168932620515,2,0.820236206054688
907,"Public administration and defence, compulsory social security","Web Science",0.794245541095734,"Web Science studies the phenomena related to the analysis and design of sociotechnical systems. Sociology plays an important role in the design of the web of the future. This course introduces the principles of web science. The design systems used in web science are presented, including information retrieval mechanisms, recommender systems and sentiment analysis systems. Then, the terms Social Computing and Citizen Science are defined, paying special attention to artificial societies and trust and reputation mechanisms. Finally, social decision-making mechanisms based on preference aggregation are revised.","Digital and Intelligent Public Administration: transformations in the era of artificial intelligenceThis article addresses the impact of the digital era and it specifically refers to information and communication technologies (ICT) in Public Administration. It is based on the international approach and underscores the importance of incorporating new technologies established by ",110,7,0.813184056963239,2,0.798393309116364
911,"Public administration and defence, compulsory social security","Model Driven Engineering",0.822551250457764,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","This paper describes a model of digital governance that reproduces within the system essential features of public administration while establishing logic for their utilization. The ultimate goal is to be able to confine all participants to their respective roles and Administration by Algorithm? Public Management Meets Public Sector Machine Learning",110,9,0.815234687593248,2,0.810649544000626
925,"Public administration and defence, compulsory social security","Deep Learning",0.852133274078369,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Bridging machine learning and cryptography in defence against adversarial attacksIn the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks. However, most of the deep learning architectures are vulnerable to so called adversarial examples. This ",110,13,0.819467407006484,2,0.854275315999985
941,"Public administration and defence, compulsory social security","Evolutionary Computation",0.823480188846588,"This module introduces, first, the different models of symbolic and sub-symbolic intelligent systems, respectively: knowledge-based systems and artificial neural networks. Their characteristics, their constituent elements, advantages and disadvantages of each model and its application domain, are indicated for each of them. Special emphasis is placed on existing synergies with evolutionary computation to resolve the major difficulties that may be encountered in building such systems: knowledge extraction, selection of the best neural architecture and the process of training the system. Subsequently, we will study evolutionary computation, mainly genetic algorithms and genetic programming, which provide mechanisms for automatic construction of intelligent self-adaptive systems or robust systems, both symbolic and sub-symbolic. Finally, we will analyse the current trends in evolutionary computation and the most recent research results. The student will be provided with a promising line of research to follow in order to obtain the PhD degree","Facilitation of Trust in Automation: A Qualitative Study of Behaviour and Attitudes Towards Emerging Technology in Military CultureThe research in this field is limited due to the inherent technological limitations of existing systems, of which has saturated the literature at this point (Barnes, et al., 2014). The core of existing research centres along assessment of emerging and novel interfaces for the pursuit ",110,4,0.821203500032425,2,0.825266391038895
944,"Public administration and defence, compulsory social security","Web Science",0.802541077136993,"Web Science studies the phenomena related to the analysis and design of sociotechnical systems. Sociology plays an important role in the design of the web of the future. This course introduces the principles of web science. The design systems used in web science are presented, including information retrieval mechanisms, recommender systems and sentiment analysis systems. Then, the terms Social Computing and Citizen Science are defined, paying special attention to artificial societies and trust and reputation mechanisms. Finally, social decision-making mechanisms based on preference aggregation are revised.","Facilitation of Trust in Automation: A Qualitative Study of Behaviour and Attitudes Towards Emerging Technology in Military CultureThe research in this field is limited due to the inherent technological limitations of existing systems, of which has saturated the literature at this point (Barnes, et al., 2014). The core of existing research centres along assessment of emerging and novel interfaces for the pursuit ",110,7,0.813184056963239,2,0.798393309116364
969,"Public administration and defence, compulsory social security","Artificial Intelligence and Data Science",0.803710341453552,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Research on Military Logistics based on Big DataThis document gives formatting instructions for authors preparing papers for publication. With the arrival of the era of big data, the mature use of cloud computing and data mining technology, the big data mode have been widely applied in logistics. For military logistics, a ",110,47,0.827168932620515,2,0.820236206054688
971,"Public administration and defence, compulsory social security","Decision Support Models",0.800627470016479,"At the completion of the course, the student will: be familiar with distinct decision-making strategies and traps in the evaluation of options and in the allocation of resources in private and public contexts; be familiar with key theoretical and methodological concepts of decision-making and decision aid relevant for the best practice of decision engineering; be familiar with models, processes and tools for helping to structure and explore decisions characterized by multiple objectives, uncertainty, complexity and differences of opinion; be familiar with examples of real-world decision analysis and decision conferencing applications in organizations; be familiar with other topics considered relevant for engineering decisions, covering problem structuring methods, heuristics and biases and group decision and negotiation; have developed skills in decision analysis and modeling; • be able to select and use specialized decision support software in different decision contexts.The decision making problematic: Definition of the decision problem. Importance of decision making in
engineering and management. Characteristics of the decision context.
Decision making strategies. Uncertainty and complexity. Value and risk.
What is Decision Analysis (DA)? DA objectives. The seven fundamental steps of DA. DA schools of thought
and theoretical foundations. The problem of decision aiding.
Intervention strategies: From optimization to the learning paradigm. Value and utility analysis. Decision
conference and facilitation.
Concepts, models, techniques and software for decision support:
1. Decision trees and influence diagrams; case studies; PRECISION TREE.
2. Bayesian networks; case studies; NETICA.
3. Probabilities modeling and risk analysis; case studies; @RISK.
4. Cognitive mapping; case studies; DECISION EXPLORER.
5. Multiple criteria evaluation models; case studies; MACBETH.
6. Resource allocation and negotiation; case studies; PROBE and MACBETH.Teaching is mostly organized by groups of models, techniques and software for decision support that can
assist different types of decision problems. For each type of decision problem, teaching is based on the
presentation of methods, models and techniques to assist decision-makers, followed by a discussion of
real world case studies and of key methodological aspects, and on the use of decision support tools. For
some topics students also carry out practical exercises.
Evaluation is done through two groupwork assignments and one individual exam. In one groupwork
students structure problems characterized by uncertainty, build models and implement them in appropriate
software; in another groupwork students build a multicriteria evaluation model to assist a decision-maker
in a real problem.","[PDF][PDF] Developments in Artificial IntelligenceOpportunities and Challenges for Military Modeling and SimulationOne of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is"" Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"". Simulation might play a significant role to play in these developments as it ",110,6,0.812497705221176,2,0.808965057134628
979,"Public administration and defence, compulsory social security","Decision Support Models",0.817302644252777,"At the completion of the course, the student will: be familiar with distinct decision-making strategies and traps in the evaluation of options and in the allocation of resources in private and public contexts; be familiar with key theoretical and methodological concepts of decision-making and decision aid relevant for the best practice of decision engineering; be familiar with models, processes and tools for helping to structure and explore decisions characterized by multiple objectives, uncertainty, complexity and differences of opinion; be familiar with examples of real-world decision analysis and decision conferencing applications in organizations; be familiar with other topics considered relevant for engineering decisions, covering problem structuring methods, heuristics and biases and group decision and negotiation; have developed skills in decision analysis and modeling; • be able to select and use specialized decision support software in different decision contexts.The decision making problematic: Definition of the decision problem. Importance of decision making in
engineering and management. Characteristics of the decision context.
Decision making strategies. Uncertainty and complexity. Value and risk.
What is Decision Analysis (DA)? DA objectives. The seven fundamental steps of DA. DA schools of thought
and theoretical foundations. The problem of decision aiding.
Intervention strategies: From optimization to the learning paradigm. Value and utility analysis. Decision
conference and facilitation.
Concepts, models, techniques and software for decision support:
1. Decision trees and influence diagrams; case studies; PRECISION TREE.
2. Bayesian networks; case studies; NETICA.
3. Probabilities modeling and risk analysis; case studies; @RISK.
4. Cognitive mapping; case studies; DECISION EXPLORER.
5. Multiple criteria evaluation models; case studies; MACBETH.
6. Resource allocation and negotiation; case studies; PROBE and MACBETH.Teaching is mostly organized by groups of models, techniques and software for decision support that can
assist different types of decision problems. For each type of decision problem, teaching is based on the
presentation of methods, models and techniques to assist decision-makers, followed by a discussion of
real world case studies and of key methodological aspects, and on the use of decision support tools. For
some topics students also carry out practical exercises.
Evaluation is done through two groupwork assignments and one individual exam. In one groupwork
students structure problems characterized by uncertainty, build models and implement them in appropriate
software; in another groupwork students build a multicriteria evaluation model to assist a decision-maker
in a real problem.","Are We Flooding Pilots with Data?Effects of Situational Awareness Automation Support Concepts on Decision-Making in Modern Military Air OperationsWithin highly dynamic situations, the amount of relevant information that a pilot needs to process to make an informed decision can be substantial. With an ever increasing amount of data available to the pilot there is a real risk that not all relevant data can be taken into ",110,6,0.812497705221176,2,0.808965057134628
982,"Public administration and defence, compulsory social security","Evolutionary Computation",0.827052593231201,"This module introduces, first, the different models of symbolic and sub-symbolic intelligent systems, respectively: knowledge-based systems and artificial neural networks. Their characteristics, their constituent elements, advantages and disadvantages of each model and its application domain, are indicated for each of them. Special emphasis is placed on existing synergies with evolutionary computation to resolve the major difficulties that may be encountered in building such systems: knowledge extraction, selection of the best neural architecture and the process of training the system. Subsequently, we will study evolutionary computation, mainly genetic algorithms and genetic programming, which provide mechanisms for automatic construction of intelligent self-adaptive systems or robust systems, both symbolic and sub-symbolic. Finally, we will analyse the current trends in evolutionary computation and the most recent research results. The student will be provided with a promising line of research to follow in order to obtain the PhD degree","Article deals with a set of problems linked to a Engineer Force Protection Provision algorithm design and evaluation of input factors series. This algorithm is generally compatible with The NATO Force Protection Process Model adjusting it to a part of engineer forces' decision Intrusion Detection of Data Platform Based on Extreme Learning Machine in Civil and Military Integration",110,4,0.821203500032425,2,0.825266391038895
986,"Water supply, sewerage, waste management and remediation activities","Artificial Intelligence and Data Science",0.844752490520477,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Machine learning in coupled wildfire-water supply risk assessment: Data science toolkitThe frontier of wildfire-related risk assessment is moving into data science territory, and with good reason. Computational statistics, built on a foundation of high resolution remote sensing data, ground data, and theory, forms the basis of powerful risk assessment tools ",30,47,0.827168932620515,2,0.844783216714859
987,"Water supply, sewerage, waste management and remediation activities","Data Analystics for Smart Grids",0.838845729827881,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Machine learning in coupled wildfire-water supply risk assessment: Data science toolkitThe frontier of wildfire-related risk assessment is moving into data science territory, and with good reason. Computational statistics, built on a foundation of high resolution remote sensing data, ground data, and theory, forms the basis of powerful risk assessment tools ",30,67,0.828709795403836,2,0.815577387809753
993,"Water supply, sewerage, waste management and remediation activities","Data Analystics for Smart Grids",0.792309045791626,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Preparing for the Use of Big Data in Denmark's Waste Management SectorThis project explored the challenges and opportunities associated with prospective implementations of big data analytics in Denmark's waste industry. We found that while some waste management companies collect detailed data, they do not use or share their ",30,67,0.828709795403836,2,0.815577387809753
998,"Water supply, sewerage, waste management and remediation activities","Information Retrieval",0.801931381225586,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","A framework of developing a big data platform for construction waste management: a Hong Kong studyBig data has shown great potentials in improving management discretion in many areas. The applications of big data in areas such as finance, computer science, health care and medical science have made continued success. Despite of big data's potentials, its ",30,48,0.813798369218906,2,0.808693736791611
1003,"Water supply, sewerage, waste management and remediation activities","Information Retrieval",0.815456092357635,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Remediation, convergence, and big data: Conceptual limits of cross-platform social mediaThe era of multiplatform media and big data provide new opportunities to reconsider data access by media companies. Outlined here is the discussion surrounding data access from media institutional logic and user-centric perspectives in the contexts of digitalization and ",30,48,0.813798369218906,2,0.808693736791611
1005,"Water supply, sewerage, waste management and remediation activities","Bid Data Measuring Systems",0.834664523601532,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Apparatus and method of leveraging semi-supervised machine learning principals to perform root cause analysis and derivation for remediation of issues in a Embodiments of the innovation relate to a host device having a memory and a processor, the host device configured to determine an anomaly associated with an attribute of a computer environment resource of the computer infrastructure. The host device is configured ",30,24,0.826420515775681,2,0.845591932535172
1007,"Water supply, sewerage, waste management and remediation activities","Machine Perception",0.814440250396729,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Apparatus and method of leveraging semi-supervised machine learning principals to perform root cause analysis and derivation for remediation of issues in a Embodiments of the innovation relate to a host device having a memory and a processor, the host device configured to determine an anomaly associated with an attribute of a computer environment resource of the computer infrastructure. The host device is configured ",30,24,0.819486998021603,2,0.833968371152878
1008,"Water supply, sewerage, waste management and remediation activities","Machine Learning",0.808839619159698,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","Apparatus and method of leveraging semi-supervised machine learning principals to perform root cause analysis and derivation for remediation of issues in a Embodiments of the innovation relate to a host device having a memory and a processor, the host device configured to determine an anomaly associated with an attribute of a computer environment resource of the computer infrastructure. The host device is configured ",30,79,0.825409763975988,2,0.834318041801453
1010,"Water supply, sewerage, waste management and remediation activities","Machine Learning",0.859796464443207,"The seminar provides students with general knowledge about the topic of Machine
Learning, being itself an introduction to the various modules and seminars that are
part of the subject: Bayesian Networks, Machine Learning and Neural Networks. Machine Learning deals with building computer systems that optimise performance
criteria using previous data or experience. A situation where learning is required is
when there is no human experience or it is not easily explained. Another is when the
problem to be solved changes over time or depends on a particular environment.
Machine Learning transforms data into knowledge and provides general purpose
systems that adapt to circumstances. Among the many successful applications that
can be cited are: speech recognition or handwritten text, autonomous robot
navigation, document information retrieval, cooperative filtering, diagnostic systems,
DNA microarrays analysis, etc.
This module presents several methods based on different fields such as Statistics,
Pattern Recognition, Artificial Intelligence and Data Mining. The aim is to know these
methods from a unified perspective, noting which problems can be solved, as well as
the limitations and circumstances of using each one of them.","Apparatus and method of adjusting a sensitivity buffer of semi-supervised machine learning principals for remediation of issues in a computer environmentIn a host device, a method for performing an anomaly analysis of a computer environment includes applying a learned behavior function to a data training set and to a set of data elements received from at least one computer environment resource to define at least one ",30,79,0.825409763975988,2,0.834318041801453
1012,"Water supply, sewerage, waste management and remediation activities","Bid Data Measuring Systems",0.856519341468811,"This course covers measurement systems as large data sources. The objective is to provide a view of the link between real-world observation and actuation made possible by assigning to each object, device, or system properties that can be quantified. This is one of the concepts underlying the Internet of Things (IoT), where through the use of sensors, an entire infrastructure is closely linked to information and communication technologies; where intelligent monitoring and management can be achieved through the use of embedded devices in a network. In this dynamical system, the devices are interconnected to transmit useful measurement information and control instructions through distributed sensor networks. The course will provide students with the ability to interact with experts in the design and implementation of systems that acquire experimental data and to understand the characteristics of the data to process them using the most appropriate and efficient algorithms and techniques to obtain useful information.(1) Measurement systems (data acquisition systems and
embedded systems). Interface with the real world: sensors and
actuators; electrical conduction; electrical signal conversion of raw
data; basic data processing; storage and transmission of data. (2)
Telemetry. (3) Experimental data source equipment and software:
intelligent sensors, RFID, wired and wireless distributed systems and
networks (wireless sensor networks, body area networks, etc.). (4)
Ubiquitous systems and computing. (5) Examples of application:
environmental monitoring, health monitoring, assisted physiotherapy,
energy measurement and quality of energy, intelligent agriculture,
etc.Final Exam (60% weight) and laboratory (40% weight). The lab has multiple tasks to perform in groups of up to 3 students. Students get an individual grade for their preparation and performance during the lab. The group receives a common note for the short lab report delivered at the end of each laboratory. For one of the tasks, the group must submit a formal report according to the guidelines published on the site of the course unit.","Apparatus and method of adjusting a sensitivity buffer of semi-supervised machine learning principals for remediation of issues in a computer environmentIn a host device, a method for performing an anomaly analysis of a computer environment includes applying a learned behavior function to a data training set and to a set of data elements received from at least one computer environment resource to define at least one ",30,24,0.826420515775681,2,0.845591932535172
1013,"Water supply, sewerage, waste management and remediation activities","Machine Perception",0.853496491909027,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Apparatus and method of adjusting a sensitivity buffer of semi-supervised machine learning principals for remediation of issues in a computer environmentIn a host device, a method for performing an anomaly analysis of a computer environment includes applying a learned behavior function to a data training set and to a set of data elements received from at least one computer environment resource to define at least one ",30,24,0.819486998021603,2,0.833968371152878
1014,"Water supply, sewerage, waste management and remediation activities","Artificial Intelligence and Data Science",0.844813942909241,"At the core of Data Science are methods for analysis of large volumes of data. Recently much more data has become available in electronic form, methods for analysis and modelling these data for prediction, classification and optimisation have become much more effective. Recent technical innovations, such as Deep Learning, provide increasingly powerful tools that make it possible to find complex patterns in very large datasets.

Much of the Master's AI is about Data Science. The obligatory courses on Machine Learning address key technology and theory for modelling large amounts of data. The courses on Machine Learning, Natural Language Processing, Information Retrieval and Computational Intelligence all have a strong focus on data-driven methods. For the “AI courses” in the curriculum students can choose advanced courses on these topics: Machine Learning 2, Computer Vision 2, Natural Language Processing 2, Information Retrieval 2, Deep Learning, Data Mining Techniques, Information Visualisation and Probabilistic Robotics. All these courses are about modelling data. These can be complemented by courses outside AI, for example on distributed computer systems, privacy and ethical questions, or on statistics.","Apparatus and method of adjusting a sensitivity buffer of semi-supervised machine learning principals for remediation of issues in a computer environmentIn a host device, a method for performing an anomaly analysis of a computer environment includes applying a learned behavior function to a data training set and to a set of data elements received from at least one computer environment resource to define at least one ",30,47,0.827168932620515,2,0.844783216714859
1015,"Real estate activities","Data Analystics for Smart Grids",0.821671426296234,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Automation of the technical due diligence with artificial intelligence in the real estate industryOver the real estate lifecycle numerous documents and data are generated. The majority of building-related data is collected in day-to-day operations, such as maintenance protocols, contracts or energy consumptions. Previous successes in the classification already help to ",60,67,0.828709795403836,2,0.80303767323494
1026,"Real estate activities","Risk, rare events and extremes",0.791414618492126,"Modelling of rare events, such as stock market crashes, storms and catastrophic structural failures, is important. This course will describe the special models and methods that are relevant to such modelling, including the mathematical bases, statistical tools and applications.
Content
Mathematical bases: behaviour of maxima and threshold exceedances in large samples, both for independent and dependent data. Poisson process modelling.
Statistical methods: modelling using the GEV and GP distributions, for independent and dependent data. Likelihood and Bayesian inference. Non-stationarity. Extremal coefficients. Multivariate extreme-value distributions. Max-stable processes.
Applications: Environmental, financial, and engineering applications. Use of R for extremal modelling.By the end of the course, the student must be able to:
Recognize situations where statistical analysis of extrema is appropriate
Manipulate mathematical objects related to the study of extrema
Analyze empirical data on extremes using appropriate statistical methods
Construct appropriate statistical models for extremal data
Interpret such models in terms of underlying phenomena
Infer properties of real systems in terms of probability models for extremes.","Real estate bubble prediction based on big dataDisclosed herein are a computer apparatus, non-transitory computer readable medium, and method for predicting real estate bubbles based on big data analysis. Historical variable data associated with real estate assets are obtained from remote data sources. Portions of ",60,3,0.829016705354055,2,0.819871217012405
1028,"Real estate activities","Data Analystics for Smart Grids",0.784403920173645,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Real estate bubble prediction based on big dataDisclosed herein are a computer apparatus, non-transitory computer readable medium, and method for predicting real estate bubbles based on big data analysis. Historical variable data associated with real estate assets are obtained from remote data sources. Portions of ",60,67,0.828709795403836,2,0.80303767323494
1051,"Real estate activities","Risk, rare events and extremes",0.848327815532684,"Modelling of rare events, such as stock market crashes, storms and catastrophic structural failures, is important. This course will describe the special models and methods that are relevant to such modelling, including the mathematical bases, statistical tools and applications.
Content
Mathematical bases: behaviour of maxima and threshold exceedances in large samples, both for independent and dependent data. Poisson process modelling.
Statistical methods: modelling using the GEV and GP distributions, for independent and dependent data. Likelihood and Bayesian inference. Non-stationarity. Extremal coefficients. Multivariate extreme-value distributions. Max-stable processes.
Applications: Environmental, financial, and engineering applications. Use of R for extremal modelling.By the end of the course, the student must be able to:
Recognize situations where statistical analysis of extrema is appropriate
Manipulate mathematical objects related to the study of extrema
Analyze empirical data on extremes using appropriate statistical methods
Construct appropriate statistical models for extremal data
Interpret such models in terms of underlying phenomena
Infer properties of real systems in terms of probability models for extremes.","Machine Learning Vs. Spatial Econometric Models: Modeling the Impact of Transportation Infrastructure on Real Estate PricesLinear regression with Ordinary Least Squares and spatial econometric models are statistical methods widely employed to measure the impact of transportation infrastructure locations on real estate prices. Efthymiou and Antoniou (1, 2, 3) developed different types of ",60,3,0.829016705354055,2,0.819871217012405
1053,"Real estate activities","Machine learning",0.829209983348846,"Machine learning and data analysis are becoming increasingly central in many sciences and applications. In this course, fundamental principles and methods of machine learning will be introduced, analyzed and practically implemented.Basic regression and classification concepts and methods: Linear models, overfitting, linear regression, Ridge regression, logistic regression, and k-NN.
Fundamental concepts: cost-functions and optimization, cross-validation and bias-variance trade-off, curse of dimensionality.
Unsupervised learning: k-Means Clustering, Gaussian mixture models and the EM algorithm.
Dimensionality reduction: PCA and matrix factorization, word embeddings
Advanced methods: generalized linear models, SVMs and Kernel methods, Neural networks and deep learning. By the end of the course, the student must be able to:
Define the following basic machine learning problems: Regression, classification, clustering, dimensionality reduction, time-series
Explain the main differences between them
Implement algorithms for these machine learning models
Optimize the main trade-offs such as overfitting, and computational cost vs accuracy
Implement machine learning methods to real-world problems, and rigorously evaluate their performance using cross-validation. Experience common pitfalls and how to overcome them
Explain and understand the fundamental theory presented for ML methods","Machine Learning Vs. Spatial Econometric Models: Modeling the Impact of Transportation Infrastructure on Real Estate PricesLinear regression with Ordinary Least Squares and spatial econometric models are statistical methods widely employed to measure the impact of transportation infrastructure locations on real estate prices. Efthymiou and Antoniou (1, 2, 3) developed different types of ",60,4,0.840624004602432,2,0.848572313785553
1066,"Real estate activities","Machine learning",0.86793464422226,"Machine learning and data analysis are becoming increasingly central in many sciences and applications. In this course, fundamental principles and methods of machine learning will be introduced, analyzed and practically implemented.Basic regression and classification concepts and methods: Linear models, overfitting, linear regression, Ridge regression, logistic regression, and k-NN.
Fundamental concepts: cost-functions and optimization, cross-validation and bias-variance trade-off, curse of dimensionality.
Unsupervised learning: k-Means Clustering, Gaussian mixture models and the EM algorithm.
Dimensionality reduction: PCA and matrix factorization, word embeddings
Advanced methods: generalized linear models, SVMs and Kernel methods, Neural networks and deep learning. By the end of the course, the student must be able to:
Define the following basic machine learning problems: Regression, classification, clustering, dimensionality reduction, time-series
Explain the main differences between them
Implement algorithms for these machine learning models
Optimize the main trade-offs such as overfitting, and computational cost vs accuracy
Implement machine learning methods to real-world problems, and rigorously evaluate their performance using cross-validation. Experience common pitfalls and how to overcome them
Explain and understand the fundamental theory presented for ML methods","A machine learning approach to big data regression analysis of real estate prices for inferential and predictive purposesThe hedonic price regressions have mainly been used for inference. In contrast, machine learning employed on big data has a great potential for prediction. To contribute to the integration of these two strategies, this article proposes a machine learning approach to the ",60,4,0.840624004602432,2,0.848572313785553
1069,"Real estate activities","DD2437 Artificial Neural Networks and Deep Architectures",0.854984939098358,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","A machine learning approach to big data regression analysis of real estate prices for inferential and predictive purposesThe hedonic price regressions have mainly been used for inference. In contrast, machine learning employed on big data has a great potential for prediction. To contribute to the integration of these two strategies, this article proposes a machine learning approach to the ",60,33,0.829086807641116,2,0.828874617815018
1072,"Real estate activities","DD2437 Artificial Neural Networks and Deep Architectures",0.802764296531677,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Comparing three machine learning algorithms in the task of appraising commercial real estateIn a unique opportunity to examine rare appraisal data from the commercial real estate sector, the accuracy of three machine learning algorithms is compared in the task of appraising commercial real estate. The algorithms; random forests, support vector ",60,33,0.829086807641116,2,0.828874617815018
6,"Legal and accounting activities","A Network Tour of Data Science",0.831792891025543,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","[PDF][PDF] Bankruptcy risk prediction models based on artificial neural networks As part of the Artificial Intelligence, the neural networks are systems that use approximation
methods based  31 December 2015 in Romania there were a number of 773781 active legal
entities  978-0-9742114-1-1. Walczak, S.(2001)An empirical analysis of data requirements for  
",80,53,0.824819772873285,1,0.831792891025543
9,"Legal and accounting activities","Network Science",0.82765519618988,"This course provides an introduction to the study of complex networks, including algorithms, models and applications to both artificial and real networks, including social, biological and technological networks, all sharing common features and properties. The course addresses the development of scalable algorithms and data structures so that we can efficiently study large complex networks, but also in the creation of theoretical models capable of describing empirically observed patterns. The number of applications is enormous, including web search engines, evolutionary dynamics, information diffusion on Internet, social networks and blogs, network resilience, network-driven phenomena in epidemiology and computer viruses, networks dynamics, with connections in the social sciences, physics, computational biology, and economics.Introduction to complex systems and networks science: Theory and basic concepts. Properties and characterization of biological, social and technological networks. Network models and random graphs. Efficient representation of large (sparse) networks. Succinct data-structures and coding strategies. Design and analysis of efficient and scalable algorithms for large network processing and analysis, including both sampling and randomization techniques. Databases and distributed platforms for the analysis of large networks. Link analysis and random walks. Community finding and graph partitioning. Ranking algorithms. Vertex relabeling. Dynamical processes on complex networks: The impact of network structure on economic, social and biological systems. Introduction to stochastic processes, Monte-Carlo simulations and large-scale multi-agent systems. Disease spreading and tolerance to attacks. Models of peer-influence and opinion formation. Game theory and population dynamics. Public goods problems, cooperation and reputation dynamics. Decision-making on (static and adaptive) interaction networks.","[PDF][PDF] Bankruptcy risk prediction models based on artificial neural networks As part of the Artificial Intelligence, the neural networks are systems that use approximation
methods based  31 December 2015 in Romania there were a number of 773781 active legal
entities  978-0-9742114-1-1. Walczak, S.(2001)An empirical analysis of data requirements for  
",80,5,0.808625149726868,1,0.82765519618988
11,"Legal and accounting activities","Information security and privacy",0.812998831272125,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","Influence of Artificial Intelligence on Activities and Competitiveness of an Organization view and edit this information), but also about the compliance with legal regulations (eg  Within
this group 85% of managers believe in the potential of artificial intelligence, 25% implement it 
The category of solutions integrating data from various sources (eg Twillio) and platforms  
",80,10,0.806313174962997,1,0.812998831272125
27,"Legal and accounting activities","Lab in data science",0.799083232879639,"This hands-on course teaches the tools & methods used by data scientists, from researching solutions to scaling up prototypes to Spark clusters. It exposes the students to the entire data science pipeline, from data acquisition to extracting valuable insights applied to real-world problems.
Content
 

1. Crash-course in Python for data scientists

Python packages: NumPy, Pandas, Matplotlib, Scikit-Learn
Interactive data science with web-based notebooks
Project #1: Curating data from a network of CO2 sensors
2. Distributed computing with an Apache Hadoop distribution

Understand main constituents: HDFS, Parquet, HBase, Hive, Zookeeper, Ambari, Spark, Spark Streaming, Yarn, Mesos, etc.
Project #2.1: Prepare a sandbox distribution
HDFS internals, best practices
Project #2.2: Configure HDFS, prepare files used in subsequent projects, choose appropriate compression, etc.
3. Distributed processing with Apache Spark

RDDs and best practices for order of operations, data partitioning, caching
Data science packages in Spark: GraphX, MLlib, etc.
Project #3: Large-scale processing of genomic data
4. Real-time data acquisition using Apache NiFi

Stream processing using Apache Spark Streaming
Project #4: Indexing tweets with NiFi and Solr
5. Final project - Summing it all up

Tapping into live traffic data sources from a major city: Acquisition & curation of live traffic sensors, estimation of speed of traffic on different road segments, and prediction of congestion using Spark, HBase, Kafka.
 

Keywords
Data Science, IoT, Machine Learning, Predictive Modeling, Big Data, Stream Processing, Apache Spark, Hadoop, Large-Scale Data Analysis. By the end of the course, the student must be able to:
Use standard Big Data tools and Data Science librairies
Carry out real-world projects with a variety of real datasets, both at rest and in motion
Design large scale data science and engineering problems
Present tangible solution to a real-world Data Science problem
Transversal skills
Demonstrate a capacity for creativity.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Write a scientific or technical report.","Big data in accounting Ernst &Young Limited (EY) is the third case study. Its scope of accounting activities includes:
advisory, assurance and tax services and as a result it demonstrates great scientific interest  BigData can be very  visual data analytics (Gepp et. al, 2018). Another suggestion is meta  
",80,78,0.825876659307724,1,0.799083232879639
28,"Legal and accounting activities","Data visualization",0.796548128128052,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","Big data in accounting Ernst &Young Limited (EY) is the third case study. Its scope of accounting activities includes:
advisory, assurance and tax services and as a result it demonstrates great scientific interest  BigData can be very  visual data analytics (Gepp et. al, 2018). Another suggestion is meta  
",80,10,0.81925385594368,1,0.796548128128052
33,"Legal and accounting activities","Data Administration in Information Systems",0.79371589422226,"The course on Data Administration in Information Systems aims at providing to students the skills needed to manage, optimize and effectively use modern database systems for managing large volumes of data. Students should be able to: 1. understand the internal mechanisms of a relational Database Management System (DBMS),, including storage management, indexing, processing and optimizing queries, transaction management, concurrency control, and recovery management 2. understand the tasks involved in database administration 3. optimize information access in databases that store very large amounts of data 4. acquire basic knowledge about the various architectures of parallel and distributed databases, including conventional (SQL) and unconventional (NoSQL) database systems.The course syllabus for Data Administration in Information Systems mostly includes topics from the Information Management (IM) area in the ACM CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:
1. Storage (sub-)systems
a. Storage technologies (e.g., RAID)
b. Replication
c. Architectures
ACM CS IM/Database Systems > Components of database systems
ACM CS IM/Physical Database Design > Storage and file structure
ACM CCS 2012 Information systems > Information storage systems > Information storage technologies
2. Indexing algorithms and file organization a. Record storage
b. Buffer management
c. Data access
ACM CS IM/Database Systems > Components of database systems ACM CS IM/Database Systems > Design of core DBMS functions ACM CS IM/Indexing
ACM CCS 2012 Information systems > Information storage systems > Record storage systems
ACM CCS 2012 Information systems > Database management system engines > Record and buffer management
3. Query processing
a. Query execution planning b. Algorithms
c. Optimization
ACM CS IM/Database Systems > Components of database systems
ACM CS IM/Database Systems > Design of core DBMS functions
ACM CCS 2012 Information systems > Database management system engines > Database query processing
4. Concurrency control a. Locking protocols
b. Timestamping protocols
c. Multi-version protocols
ACM CS IM/Database Systems > Components of database systems ACM CS IM/Database Systems > Design of core DBMS functions ACM CS IM/Transaction Processing > Concurrency control
ACM CCS 2012 Information systems > Database management system engines > Database transaction processing
5. Data recovery a. Logging
b. Failure of non-volatile storage c. Backups
ACM CS IM/Database Systems > Components of database systems ACM CS IM/Database Systems > Design of core DBMS functions ACM CS IM/Transaction Processing > Failure and recovery
ACM CCS 2012 Information systems > Database management system engines > Database transaction processing > Database recovery
6. Database optimization
a. Schema-level optimization b. Query optimization
ACM CS IM/Physical Database Design > Database efficiency and tuning
ACM CCS 2012 Information systems > Database management system engines > Database query processing
ACM CCS 2012 Information systems > Database design and models > Relational database model
ACM CCS 2012 Information systems > Data structures > Data access methods
7. Index optimization a. Clustering
b. Covering indexes
ACM CS IM/Physical Database Design > Database efficiency and tuning
ACM CS IM/Indexing
ACM CCS 2012 Information systems > Information storage systems > Record storage systems > Record storage alternatives
ACM CCS 2012 Information systems > Information storage systems > Record storage systems > Directory structures
8. Optimizing the hardware and the operating systems a. Threads, buffers and storage
b. Database performance
ACM CS IM/Physical Database Design > Database efficiency and tuning
ACM CS IM/Transaction Processing > Interaction of transaction management with storage, especially buffering ACM CCS 2012 Information systems > Database management system engines > Record and buffer management ACM CCS 2012 Information systems > Database administration > Database performance evaluation
9. Parallel and distributed databases
a. Architectures b. Partitioning
c. Algorithms
d. Systems based on map-reduce
ACM CS IM/Database Systems > Approaches for managing large volumes of data
ACM CS IM/Distributed Databases
ACM CCS 2012 Information systems > Database management system engines > Parallel and distributed DBMSs > Relational parallel and distributed DBMSs
ACM CCS 2012 Information systems > Database management system engines > Parallel and distributed DBMSs > MapReduce-based systems
10. NoSQL databases
a. Key-value storage databases b. Document databases
c. Column-oriented databases
d. Databases for graph data
ACM CS IM/Database Systems > Approaches for managing large volumes of data
ACM CCS 2012 Information systems > Database management system engines > Parallel and distributed DBMSs > Key-value stores
ACM CCS 2012 Information systems > Information retrieval. The evaluation of the Data Administration in Information Systems course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).
Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).
After the exam, students whose difference between the grade of the project (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.
Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics
1, 2 and 3 of the course, the second project will cover topics 4, 5 and 6 of the course, and the third project will cover topics 7, 8, 9 and 10.","Bits and bolts Real estate 68 Low Low Legal and accounting activities, etc  online renting of data or computing
capacities), as well as back and front office integration systems such as customer relationship
management (CRM), and enterprise resource planning (ERP) software. Page 14  
",80,2,0.794792950153351,1,0.79371589422226
40,"Legal and accounting activities","Introduction to natural language processing",0.829474091529846,"The objective of this course is to present the main models, formalisms and algorithms necessary for the development of applications in the field of natural language information processing. The concepts introduced during the lectures will be applied during practical sessions.Several models and algorithms for automated textual data processing will be described: (1) morpho-lexical level: electronic lexica, spelling checkers, ...; (2) syntactic level: regular, context-free, stochastic grammars, parsing algorithms, ...; (3) semantic level: models and formalisms for the representation of meaning, ...

Several application domains will be presented: Linguistic engineering, Information Retrieval, Text mining (automated knowledge extraction), Textual Data Analysis (automated document classification, visualization of textual data).

Keywords
Natural Language Processing; Computationnal Linguisitics; Part-of-Speech tagging; Parsing.By the end of the course, the student must be able to:
Compose key NLP elements to develop higher level processing chains
Assess / Evaluate NLP based systems
Choose appropriate solutions for solving typical NLP subproblems (tokenizing, tagging, parsing)
Describe the typical problems and processing layers in NLP
Analyze NLP problems to decompose them in adequate independant components.","Semantic Technologies for Business Decision Support processed and incorporated into the company structure, with the support of linguistic rules on
the intelligence phase  development, as it takes advantage of other disciplines as MachineLearning, Artificial Intelligence and Cognitive Science  It relies on data stored in a database  
",80,3,0.835448265075684,1,0.829474091529846
42,"Legal and accounting activities","Bioinformatics",0.821761965751648,"Bioinformatics aims at developing computational methods and algorithms to process biological data and uses mathematical and statistical modelling to generate testable hypotheses about biological entities and processes. The goal of this course is to introduce the basic techniques that support the most recent developments on this field. Additionally, it enables the development of the ability to critically assess research publications in this field. Practical assignments during the course aim at developing the student's ability to develop software for bioinformatics.Introduction, Molecular biology main concepts, Introduction to algorithms and complexity
Graphs and genetics
DNA sequence analysis
Pairwise alignment
Multiple Sequence alignment
Motif finding
NGS data, algorithms and data structures
Probabilistic models
Gene expression data analysis
Data mining
Unsupervised Learning: Clustering and Biclustering
Molecular phylogenetics
Supervised Learning: Decision trees, Bayesian methods
Integrative data analysis
Seminar","Semantic Technologies for Business Decision Support processed and incorporated into the company structure, with the support of linguistic rules on
the intelligence phase  development, as it takes advantage of other disciplines as MachineLearning, Artificial Intelligence and Cognitive Science  It relies on data stored in a database  
",80,6,0.80968498190244,1,0.821761965751648
47,"Legal and accounting activities","Intelligent Agents and Multi-Agent Systems",0.809990406036377,"Multiagent systems are systems consisting of several autonomous entities called agents that interact among themselves in order to solve problems that exceed the individual capabilities of each or solving them in a more efficient way. This interaction is the main object of research in multiagent systems, and it has contributed to different disciplines such as Social Science, Game Theory and Artificial Intelligence. In this module, besides studying these contributions, we will introduce the students to the practice of research in any area related to multi-agent systems, and the preparation of papers describing the results of its research activity","What role for social sciences in innovation? related fields (including computer science, information systems, software engineering and artificialintelligence) are found  Availability of comprehensive, long-term and internationally comparabledata allows for  cite NPL; citations are frequently given by examiners or by patent  
",80,4,0.801681563258171,1,0.809990406036377
48,"Legal and accounting activities","Web Science",0.809442043304443,"Web Science studies the phenomena related to the analysis and design of sociotechnical systems. Sociology plays an important role in the design of the web of the future. This course introduces the principles of web science. The design systems used in web science are presented, including information retrieval mechanisms, recommender systems and sentiment analysis systems. Then, the terms Social Computing and Citizen Science are defined, paying special attention to artificial societies and trust and reputation mechanisms. Finally, social decision-making mechanisms based on preference aggregation are revised.","What role for social sciences in innovation? related fields (including computer science, information systems, software engineering and artificialintelligence) are found  Availability of comprehensive, long-term and internationally comparabledata allows for  cite NPL; citations are frequently given by examiners or by patent  
",80,7,0.813184056963239,1,0.809442043304443
57,"Legal and accounting activities","Computational Intelligence Lab",0.810657560825348,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","Adapting business framework conditions to deal with disruptive technologies in Denmark Participation in life-long learning is high but decreasing  Second, the ability to combine new
advanced technologies (such as sensors, advanced robotics and 3D printing), new processes
(such as data-driven production and artificial intelligence) and new business models  
",80,11,0.82310460914265,1,0.810657560825348
62,"Legal and accounting activities","Estimation and Testing under Sparsity",0.800699770450592,"In high-dimensional models the number of parameters p is larger than the number of observations n. Therefore, classical (asymptotic) theory needs new methods and paradigms for estimation and testing. One of the key concepts here is ""sparsity"" which says that most of the parameters are actually not relevant and can be set to zero.        In high-dimensional models the number of parameters p is larger than the number of observations n. Therefore, classical (asymptotic) theory needs new methods and paradigms for estimation and testing. One of the key concepts here is ""sparsity"" which says that most of the parameters are actually not relevant and can be set to zero. A popular way to take sparsity into account is regularizing using the l_1-penalty. This leads to two lines of research. Firstly, we need to study the statistical properties of l_1-regularized estimators and related issues, for example their role as initial estimators in a one-step procedure for the construction of asymptotically linear estimators. Secondly, the l_1-approach has a special geometry which one can study in terms of properties of empirical processes. Therefore the lectures have two intertwined parts: one where statistical theory plays the main role and a second where probability theory is studied. Most results presented will be given a full proof, perhaps with parts left as exercises.","The compositional nature of productivity and innovation slowdown Indeed, our task is to understand if the 'data generating process' behind the productivity slowdown 
the socalled within, between, and covariance (or crosslevel) components (or effects  the following:
at the firm level, the within effect is interpreted as learning/innovation (change  
",80,1,0.800699770450592,1,0.800699770450592
63,"Legal and accounting activities","Bayesian Networks",0.787855088710785,"This module presents Bayesian Networks as graphic tools which are well consolidated and of wide use nowadays to model uncertainty and reason with in intelligent systems. Uncertainty is modelled with probabilities and reasoning is based on Bayes’ rule. It begins by explaining the meaning of the networks to model reasoning with uncertainty, both casual and non-casual, and both from a structural (qualitative) point of view and parametric (quantitative). The next step is to pose questions to the network, in other words, to infer knowledge from observations or data that is being collected. Thus, we can ask, for instance, for the diagnosis of a disease or the most likely explanation for the observed evidence. The algorithms can obtain the exact or an approximate answer, in the latter case probably using Monte Carlo simulation. The network is built by analysing the problem with an expert, but can also be induced from a database. This is a current issue: how to obtain a structure and parameters for the network and for that machine learning methods will be discussed. Finally, by knowing how to build the network and how to use it to perform queries, it will be possible to see its application on decision making and other applications of great interest within Artificial Intelligence: computer vision, automatic classification, filtering of email, etc.","The compositional nature of productivity and innovation slowdown Indeed, our task is to understand if the 'data generating process' behind the productivity slowdown 
the socalled within, between, and covariance (or crosslevel) components (or effects  the following:
at the firm level, the within effect is interpreted as learning/innovation (change  
",80,5,0.810826969146728,1,0.787855088710785
100,"Accomodation and food service activities","Cloud Computing and Virtualization",0.84547483921051,"Attain an integrated perspective of cloud computing and virtualization, with combined approaches for the design of modern large scale and distributed computing systems, and with their underlying mechanisms and algorithms. Understand a vertical approach to the various virtualization and cloud computing technologies, enhancing applications and services with improved flexibility, resource and economic efficiency, scalability and adaptability. To be able to develop reliable and scalable systems and applications, on cloud computing over current virtualization platforms and applications models. To be able to assess and evaluate solutions, given the alternatives and tradeoffs involved in the employment and management of virtualization infrastructure for cloud computing.Introduction to Virtualization and Cloud Computing, Infrastructure-as-a-Service, Platform-as-a-Service, Software-as-a-Service.
System-level virtualization: system VM architecture, CPU virtualization, OS core, memory, I/O; hardware support for virtualization, case studies (VMWare, QEMU/KVM, Xen).
Cloud computing systems (Amazon EC2, OpenStack, XenCloud, OpenNebula); VM scheduling, migration and replication; monitoring and scalability (CloudWatch, Autoscaling).
Process-level virtualization: Java VM specification and reference implementation, security model, code management and binary translation, just-in-time compilation and optimization,
garbage collection, case studies (Jikes RVM).
Cloud computing platforms (Azure, Google App Engine); distributed virtual machines; monitoring and scalability (Azure Fabric Controller).
Data and Storage services: block storage, file storage, key-value stores (Dynamo, S3, Datastore), tabular storage (BigTable, Percolator).
Cloud computing scalability: Map-reduce, dataflows (Pig, Dryad, OOzie), streams (S4), applications, monitoring, elasticity and optimization.
Cloud computing cross-cutting concerns: virtualization energy efficiency, dynamic provisioning, energy centered cloud design.","A big data and cloud computing specification, standards and architecture: agricultural and food informatics data and cloud computing specification, standards and architecture: agricultural and food
informatics  The real-time data storage and management architecture plays important role. This
paper introduces big data, includes the background and definitions, characteristics, related  
",145,1,0.84547483921051,1,0.84547483921051
102,"Accomodation and food service activities","Energy-Efficient Parallel Computing Systems for Data Analytics",0.829289555549622,"Advanced Parallel Computing Architectures and related design issues. It will cover multi-cores, many-cores, vector engines, GP-GPUs, application-specific processors and heterogeneous compute accelerators. Focus on integrated architectures for data analytics applications. Special emphasis given to energy-efficiency issues and hardware-software design for power and energy minimizazion.        Give in-depth understanding of the links and dependencies between architectures and their energy-efficient implementation and to get a comprehensive exposure to state-of-the-art computing platforma for data anlytics applications. Practical experience will also be gained through practical exercises and mini-projects (hardware and software) assigned on specific topics.        The course will cover advanced parallel computing architectures architectures, with an in-depth view on design challenges related to advanced silicon technology and state-of-the-art system integration options (nanometer silicon technology, novel storage devices, three-dimensional integration, advanced system packaging). The emphasis will be on programmable parallel architectures, namely, multi and many- cores, GPUs, vector accelerators, application-specific processors, heterogeneous platforms, and the complex design choices required to achieve scalability and energy proportionality. The course will will also delve into system design, touching on hardware-software tradeoffs and full-system analysis and optimization taking into account non-functional constraints and quality metrics, such as power consumption, thermal dissipation, reliability and variability. The application focus will be on emerging data analytics both in the cloud at at the edges (near-sensor analytics).","A big data and cloud computing specification, standards and architecture: agricultural and food informatics data and cloud computing specification, standards and architecture: agricultural and food
informatics  The real-time data storage and management architecture plays important role. This
paper introduces big data, includes the background and definitions, characteristics, related  
",145,2,0.806568711996078,1,0.829289555549622
104,"Accomodation and food service activities","High Performance Computing Systems",0.815248787403107,"Understanding of basic functional principles and development of “parallel thinking” when designing scalable and efficient parallel programs (software) for contemporary parallel and heterogeneous computing systems (e.g., APUs, CPU+GPU or CPU+FPGA). Students will obtain a practical (hands-on) experience to uncover real-world challenges and design trade-offs in different parallel environments, with a special emphasis on application optimization, task scheduling and utilization of popular programming models, frameworks and tools (e.g., CUDA and OpenCL). Students will also get acquainted with cutting-edge parallel computing trends, such as emergent memory technologies, data-intensive applications and efficient management, processing, and analysis of large volumes of data.1.        Analysis, characterization and performance modeling: fine-grain and coarse-grain parallelism at data and task levels; performance and efficiency metrics; performance and energy-efficiency modeling.
2.        Memory hierarchy, memory coherency and consistency, transactional memory.
3.        Techniques for application optimization and performance tuning on multi-core processors. Identifying and exploiting the parallelism. Code vectorization. Memory- and cache-aware code optimization. Optimization and processing for data-intensive applications.
4.        Introduction to massively parallel accelerators. Data-parallel computation and memory subsystem. Programming models for massively parallel systems. Techniques for application performance optimization for modern accelerators and co-processors (e.g., GPU).
5.        High-performance computing on heterogenous systems. Task scheduling and load balancing. Cooperation among processing elements. Data-locality, communication and contention.
6.        Performance optimization techniques, power and energy consumption, data-intensive applications and large-scale systems.","A big data and cloud computing specification, standards and architecture: agricultural and food informatics data and cloud computing specification, standards and architecture: agricultural and food
informatics  The real-time data storage and management architecture plays important role. This
paper introduces big data, includes the background and definitions, characteristics, related  
",145,1,0.815248787403107,1,0.815248787403107
118,"Accomodation and food service activities","Distributed information systems",0.78291654586792,"This course introduces the key concepts and algorithms from the areas of information retrieval, data mining and knowledge bases, which constitute the foundations of today's Web-based distributed information systems. Information Retrieval

1.Information Retrieval - Introduction 2.Text-Based Information Retrieval 3.Vector Space Retrieval 4.Probabilistic Information Retrieval 5.Query Expansion 6.Inverted Index 7.Distributed Retrieval 8.Latent Semantic Indexing 9.Word Embeddings 10. Link-Based Ranking

Data Mining

1.Data Mining ¿ Introduction 2. Association Rule Mining 3. Clustering 4. Classification 5. Mining Social Graphs 6. Classification Methodology 7. Document Classification 8. Recommender Systems

Knowledge Bases

1. Semi-structured data 2. Semantic Web 3. RDF Resource Description Framework 4. Semantic Web Resources 5. Information Extraction 6. Taxonomy Induction 7. Ontology Mapping. By the end of the course, the student must be able to:
Characterize the main tasks performed by information systems, namely data, information and knowledge management
Apply semi-structured data models, their representation through Web standards and algorithms for storing and processing semi-structured data
Apply fundamental models and techniques of text retrieval and their use in Web search engines
Apply main categories of data mining techniques, local rules, predictive and descriptive models, and master representative algorithms for each of the categories
Apply collaborative information management models, like crowd-sourcing, recommender systems, social networks","A semantic network analysis of big data regarding food exhibition at convention center The purpose of this study was to visualize the semantic network with big data related to food
exhibition at convention center. For this, this study collected data containing 'coex food
exhibition/bexco food exhibition' keywords from web pages and news on Google during one  
",145,3,0.81862876812617,1,0.78291654586792
138,"Accomodation and food service activities","Information security and privacy",0.802765607833862,"This course will provide a broad overview of information security and privacy topics, with the primary goal of giving students the knowledge and tools they will need ""in the field"" in order to deal with the security/privacy challenges they are likely to encounter in today's ""Big Data"" world. Data protection concepts: access control, encryption, compartmentalization

¿ Intrusion/hacking techniques, intrusion detection, advanced persistent threats

¿ Practices for management of personally identifying information

¿ Operational security practices and failures

¿ Data anonymization and de-anonymization techniques

¿ Information flow control

¿ Differential privacy

¿ Cryptographic tools for data security and privacy

¿ Policy, ethics, and legal considerations. By the end of the course, the student must be able to:
Understand the most important classes of information security/privacy risks in today's âBig Dataâ environment
Exercise a basic, critical set of âbest practicesâ for handling sensitive information
Exercise competent operational security practices in their home and professional lives
Understand at overview level the key technical tools available for security/privacy protection","Combining open data and machine learning to predict food security in EthiopiaFood security is commonly measured by means of surveys, requiring substantial time and budget. Open data can possibly serve as a cost-effective alternative to predict food security. In this paper a method is proposed that uses open data related to food insecurity drivers to ",145,10,0.806313174962997,1,0.802765607833862
168,"Accomodation and food service activities","Computational Intelligence Lab",0.830521762371063,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","Discussion on the Reform of Teaching Methods for Specialized Courses under the Background of Big Data-Taking Animal Food Technology as an Example E. Make full use of big data, help better feedback students' learning situation, and establish more 
feedback for the leaning situation of students using ""Xuexitong"" on the Animal Food Technology
Mooc  These detailed and specific data to be obtained, on the one hand, can help  
",145,11,0.82310460914265,1,0.830521762371063
169,"Accomodation and food service activities","Deep Learning",0.829087615013123,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Discussion on the Reform of Teaching Methods for Specialized Courses under the Background of Big Data-Taking Animal Food Technology as an Example E. Make full use of big data, help better feedback students' learning situation, and establish more 
feedback for the leaning situation of students using ""Xuexitong"" on the Animal Food Technology
Mooc  These detailed and specific data to be obtained, on the one hand, can help  
",145,13,0.819467407006484,1,0.829087615013123
172,"Accomodation and food service activities","Database Systems",0.841722667217255,"You have insight in the inner workings of database systems
You know how data is internally represented and how queries are efficiently evaluated
You are familiar with various algorithms that make the querying of large amounts of data feasible
You understand why certain design choices are made in the development of database systems and how these choices affect the performance of database systems. In today's digital society, database systems play an important role. Such systems ensure that large quantities of data can be efficiently manipulated and queried. In this course we look at the inner workings of such systems. Efficient algorithms for storing, indexing and querying of data form the backbone of this course. More specifically, the following topics will be considered in the database systems course:

The relational model and SQL
Storage en Indexing
Hash-based indexing (Linear, Extendible)
Tree-based indexing (B+tree, ISAM)
External Sorting
External merge sort
B+ tree sorting
Buffer management
Buffer replacement policies
I/O
Query evaluation
Query optimization
Join algorithms
Cardinality estimation
Query plans
Transaction Management
ACID
Serializability
Two phase locking
Concurrency control
Crash Recovery
Checkpoints
ARIES
","Systems Approach to Link Big Socio-ecological Geo-data to Food Systems Sustainability of (i) what information commonly needed by food system actors to response and adapt to
socio-ecological change and enhance the system performance, (ii) interoperability between
different types of data across scales, and (iii) sufficient guidance to utilize big data resources  
",145,2,0.844314962625504,1,0.841722667217255
211,"Accomodation and food service activities","Reliable and Interpretable Artificial Intelligence",0.796614408493042,"Creating reliable and explainable probabilistic models is a fundamental challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models.        The main objective of this course is to expose students to the latest and most exciting research in the area of explainable and interpretable artificial intelligence, a topic of fundamental and increasing importance. Upon completion of the course, the students should have mastered the underlying methods and be able to apply them to a variety of problems.

To facilitate deeper understanding, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material.        The course covers the following inter-connected directions. 

Part I: Robust and Explainable Deep Learning
-------------------------------------------------------------

Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. Forr example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data? 

Topics: 
- Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).
- Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)
- Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.
- Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.

Part II: Program Synthesis/Induction
------------------------------------------------

Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 

Topics: 
- Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.
- Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.
- Combining synthesis with learning: application to learning from code.
- Frameworks: PHOG, DeepCode.

Part III: Probabilistic Programming
----------------------------------------------

Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 

Topics: 

- Probabilistic Inference: sampling based, exact symbolic inference, semantics
- Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).
- Frameworks: PSI, Edward2, Venture.","[HTML][HTML] Big-data-augmented approach to emerging technologies identification: case of agriculture and food sector <U+041F><U+043E><U+0445><U+043E><U+0436><U+0438><U+0435> <U+043F><U+0443><U+0431><U+043B><U+0438><U+043A><U+0430><U+0446><U+0438><U+0438>. Mapping the Radical Innovations in Food Industry: A Text Mining Study.
Kuzminov I., Bakhtin PD, Khabirova E. et al  I: Advances in Artificial Intelligence and Its Applications 
for gaining insight into the underlying conceptual structure of the data  
",145,35,0.819798954895565,1,0.796614408493042
212,"Accomodation and food service activities","Apllied Computational Intelligence",0.793887913227081,"Heuristic search and optimization methods have been increasingly used in solving complex problems for a wide range of application domains, e.g., bioinformatics, microelectronics, energy, finance, etc. where the analytical formulation is not possible or extremely complex. The heuristic methods allow a very efficient and practical way to search for optimal solutions, i.e., although they do not guarantee the optimal solution, they usually reach solutions very close to the optimum. The generality of these methods also has the advantage of allowing a high degree of parallelization thus allowing significant gains in efficiency when addressing large-scale problems. In this context, this curricular unit addresses the generality of intelligent computing techniques with particular emphasis on search and optimization methods applied to complex problems in large spaces.1. Introduction and fundamental concepts of Intelligent Computing. 2. Formulation of problems: complexity analysis; types of objectives (simple, multiple, dynamic objective functions), etc. 3. Heuristic methods for search and optimization (GA, AC, SA, PSO, TS, DE, etc.). 4. Multi-objective optimization methods (NSGA-II, MOGA, MOPSO, MOSA, etc.). 5. Multi-objective optimization with restrictions. 6. Methodologies for comparison of algorithms (BP, CDF, etc.). 7. High Performance Computing Intelligence: Parallelization and Cloud Computing. 8. Applications to different domains.","[HTML][HTML] Big-data-augmented approach to emerging technologies identification: case of agriculture and food sector <U+041F><U+043E><U+0445><U+043E><U+0436><U+0438><U+0435> <U+043F><U+0443><U+0431><U+043B><U+0438><U+043A><U+0430><U+0446><U+0438><U+0438>. Mapping the Radical Innovations in Food Industry: A Text Mining Study.
Kuzminov I., Bakhtin PD, Khabirova E. et al  I: Advances in Artificial Intelligence and Its Applications 
for gaining insight into the underlying conceptual structure of the data  
",145,1,0.793887913227081,1,0.793887913227081
213,"Accomodation and food service activities","Language Engineering",0.786530613899231,"Language Engineering (LE) is the set of techniques, resources and tools to solve problems by using more or less an automated language. This course aims to introduce students to the overall framework, which is currently the LE. The second part of the subject will explain the two main principles of most language treatment systems, such as the content representation models and the creation and maintenance of lexical resources, both pillars of any system and any use. In the third part of the course the student will be introduced three of the major commercial applications of LE, such as information retrieval (associated with the search for data or items of information in a text) and text mining, where besides extracting data type information, we will extract relationships between them. The existing application on the market, and the more immediate trends (for example the analysis of forums for opinions) will also be discussed and explained.","[HTML][HTML] Big-data-augmented approach to emerging technologies identification: case of agriculture and food sector <U+041F><U+043E><U+0445><U+043E><U+0436><U+0438><U+0435> <U+043F><U+0443><U+0431><U+043B><U+0438><U+043A><U+0430><U+0446><U+0438><U+0438>. Mapping the Radical Innovations in Food Industry: A Text Mining Study.
Kuzminov I., Bakhtin PD, Khabirova E. et al  I: Advances in Artificial Intelligence and Its Applications 
for gaining insight into the underlying conceptual structure of the data  
",145,6,0.806979159514109,1,0.786530613899231
221,"Accomodation and food service activities","Time Series Analysis",0.869331896305084,"This course examines models and statistical techniques used to study time series data. The main objective is to equip students with the methods and software tools they need for carrying out state-of-the-art empirical research on time series, with emphasis on applications in economics and finance.Basics aspects of time-domain and frequency-domain methods, methods for model-based estimation, model selection, diagnostics, forecasting, and computing as they relate to time series analysis. ARMA and seasonal ARIMA models, the Box-Jenkins approach for SARIMA modelling, spectral analysis, computing forecast for a variety of linear methods and models, nonlinear models, ARCH/GARCH models, risk models, state space models and the Kalman Filter, Monte Carlo simulation and other advanced topics if time permitted."," presents exemplary applications of the smart manufacturing concept in food industry enterprises 
methods and solutions, in- cluding machine learning and artificial intelligence algorithms [Tao 
Traditional analyses use conventional algorithms and data that has been previ- ously  
[CITATION][C]  Demand Models in the Food and Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning ",145,6,0.835573434829712,1,0.869331896305084
222,"Accomodation and food service activities","Probabilistic Artificial Intelligence",0.860229849815369,"This course introduces core modeling techniques and algorithms from statistics, optimization, planning, and control and study applications in areas such as sensor networks, robotics, and the Internet.        How can we build systems that perform well in uncertain environments and unforeseen situations? How can we develop systems that exhibit ""intelligent"" behavior, without prescribing explicit rules? How can we build systems that learn from experience in order to improve their performance? We will study core modeling techniques and algorithms from statistics, optimization, planning, and control and study applications in areas such as sensor networks, robotics, and the Internet. The course is designed for upper-level undergraduate and graduate students.        Topics covered:
- Search (BFS, DFS, A*), constraint satisfaction and optimization
- Tutorial in logic (propositional, first-order)
- Probability
- Bayesian Networks (models, exact and approximative inference, learning) - Temporal models (Hidden Markov Models, Dynamic Bayesian Networks)
- Probabilistic palnning (MDPs, POMPDPs)
- Reinforcement learning
- Combining logic and probability"," presents exemplary applications of the smart manufacturing concept in food industry enterprises 
methods and solutions, in- cluding machine learning and artificial intelligence algorithms [Tao 
Traditional analyses use conventional algorithms and data that has been previ- ously  
[CITATION][C]  Demand Models in the Food and Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning ",145,3,0.82557342449824,1,0.860229849815369
224,"Accomodation and food service activities","Autonomous Robots",0.856077194213867,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics."," presents exemplary applications of the smart manufacturing concept in food industry enterprises 
methods and solutions, in- cluding machine learning and artificial intelligence algorithms [Tao 
Traditional analyses use conventional algorithms and data that has been previ- ously  
[CITATION][C]  Demand Models in the Food and Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning ",145,10,0.822185301780701,1,0.856077194213867
226,"Human health and social work activities","Hardware Architectures for Machine Learning",0.795499503612518,"The seminar covers recent results in the increasingly important field of hardware acceleration for data science and machine learning, both in dedicated machines or in data centers.        The seminar aims at students interested in the system aspects of machine learning, who are willing to bridge the gap across traditional disciplines: machine learning, databases, systems, and computer architecture.        The seminar is intended to cover recent results in the increasingly important field of hardware acceleration for data science and machine learning, both in dedicated machines or in data centers.","[PDF][PDF] Where is technology taking the economy the Internet, the cloud, big data, robotics, machine learning, and now artificial intelligence
together powerful  And data can't easily be owned either, it can be garnered from nonproprietary 
will still have jobs, especially those like kindergarten teaching or social work that require  
",195,4,0.815778344869614,1,0.795499503612518
230,"Human health and social work activities","Data Mining",0.900552034378052,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","Identifying child abuse through text mining and machine learning is related to work in the area of data exploration and supervised classification based  risk modeling
(PRM) tools coupled with data mining and machine-learning algorithms should  a linear prediction
model (45.2% sensitivity, 82.4% specificity) using administrative data from 716  
",195,9,0.830869707796309,1,0.900552034378052
231,"Human health and social work activities","Statistical Methods in Data Mining",0.862239360809326,"Show the potential of statistical methods in data mining, with particular emphasis on classification, clustering, dimensionality reduction, anomaly detection and partial least squares methods. Develop the ability to apply statistical procedures to the analysis of large data sets, and to show how important those procedures are in decision making. Analyse real problems with specific software and identify suitable methodologies to deal with such problems. By the end of the semester, the students should know the main statistical procedures associated to data mining, and be familiar with other data mining techniques on a user level basis.Introduction. Data Mining Overview.Exploring data: Preprocessing, Visualization and Data Quality Classification

Classification Methods
- Classification with K-Nearest Neighbours
- Classification and Bayes Rule, Naïve Bayes
- Classification Trees
- Discriminant Analysis
- Logistic Regression
Evaluating the Performance of a Classifier
Comparing Classifiers

Clustering
Clustering Methods
- K-Means Clustering, Hierarchical Clustering
- EM for Mixture Model Density Estimation
Cluster Validation

Dimensionality Reduction
Principal Components
Independent Component Analysis
Multidimensional Scaling

Anomaly Detection
Preliminaries
Detecting Outliers
Evaluating the Performance of an Anomaly Detection Rule

Partial Least Squares
Introduction: More Variables than Objects
Partial Least Squares Regression","Identifying child abuse through text mining and machine learning is related to work in the area of data exploration and supervised classification based  risk modeling
(PRM) tools coupled with data mining and machine-learning algorithms should  a linear prediction
model (45.2% sensitivity, 82.4% specificity) using administrative data from 716  
",195,5,0.810140144824982,1,0.862239360809326
237,"Human health and social work activities","Information Visualization",0.848170399665833,"The main goal is to provide students with knowledge in the área of Information Visualization, that allows them to design and develop high-impact visualizations of data and information, to effectively transmit qualitative and quantitative data. The area of Information Visualization will be introduced, after which we’ll teach a methodology for analyzing problema domains and conceiving effective visualizations. Afterwards, we'll d’scuss the different kinds of variables (continuous, nominal, ratio, etc.), data (tabular, networks, text, etc.) and patterns to visualize. Next, we’ll describe the different relevant physiological and psychological factors (memory, visual processing, etc.) relevant for the creation of good visualizations. We’ll study the most common kinds of visualizations adequate for different information types (graphs, time series, etc.) and interaction techniques (focus+context, overview+detail, panning+zoom, brushing, etc). Finally we’ll address issues related with the evaluation of the effectiveness of InfoVis applications.1.        Introduction
2.        Design Methodology
3.        Datasets and variables
4.        Human Factors in InfoVis
5.        Visualization Types
6.        Visualization Techniques
7.        Dynamic visualizations and animations
8.        Item and Attribute reduction 
9.        Legibility and fidelity of visualizations
10.        Evaluation of InfoVis Solutions
11.        Applications.  This course has a lab component in which the project is developed. This project is a visualization of a dataset chosen by the students. In the lab classes the several stages of the visualization design process will be performed and presented (providing the students with timely feedback), including: chouse of dataset, data preparation/cleaning, visualization sketches/proposals, different prototypes, and user evaluation of the final solution.
Projet: The project, developed throughout the semester with help from the lab classes, will consisto n the development of a visualization of a chosen dataset, resorting to web technology and libraries such as D3.js. At the end, students will deliver a prototype and project report.","[HTML][HTML] Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives The deep learning based approach we evaluate in this paper does not require any hand 
MIMIC-III contains de-identified clinical data of over 53,000 hospital admissions for adult patients 
approaches then use relevant concepts in a note as input to machine learning algorithms to  
",195,2,0.849806368350983,1,0.848170399665833
254,"Human health and social work activities","Decision Support Models",0.823052227497101,"At the completion of the course, the student will: be familiar with distinct decision-making strategies and traps in the evaluation of options and in the allocation of resources in private and public contexts; be familiar with key theoretical and methodological concepts of decision-making and decision aid relevant for the best practice of decision engineering; be familiar with models, processes and tools for helping to structure and explore decisions characterized by multiple objectives, uncertainty, complexity and differences of opinion; be familiar with examples of real-world decision analysis and decision conferencing applications in organizations; be familiar with other topics considered relevant for engineering decisions, covering problem structuring methods, heuristics and biases and group decision and negotiation; have developed skills in decision analysis and modeling; • be able to select and use specialized decision support software in different decision contexts.The decision making problematic: Definition of the decision problem. Importance of decision making in
engineering and management. Characteristics of the decision context.
Decision making strategies. Uncertainty and complexity. Value and risk.
What is Decision Analysis (DA)? DA objectives. The seven fundamental steps of DA. DA schools of thought
and theoretical foundations. The problem of decision aiding.
Intervention strategies: From optimization to the learning paradigm. Value and utility analysis. Decision
conference and facilitation.
Concepts, models, techniques and software for decision support:
1. Decision trees and influence diagrams; case studies; PRECISION TREE.
2. Bayesian networks; case studies; NETICA.
3. Probabilities modeling and risk analysis; case studies; @RISK.
4. Cognitive mapping; case studies; DECISION EXPLORER.
5. Multiple criteria evaluation models; case studies; MACBETH.
6. Resource allocation and negotiation; case studies; PROBE and MACBETH.Teaching is mostly organized by groups of models, techniques and software for decision support that can
assist different types of decision problems. For each type of decision problem, teaching is based on the
presentation of methods, models and techniques to assist decision-makers, followed by a discussion of
real world case studies and of key methodological aspects, and on the use of decision support tools. For
some topics students also carry out practical exercises.
Evaluation is done through two groupwork assignments and one individual exam. In one groupwork
students structure problems characterized by uncertainty, build models and implement them in appropriate
software; in another groupwork students build a multicriteria evaluation model to assist a decision-maker
in a real problem.","Can Predictive Algorithms Assist Decision-Making in Social Work with Children and Families? (1985) explored the potential of what they refer to as 'artificial intelligence' to develop 'expert  every
time' Decision Support Systems and Child and Family Social Work  In another instance where
a big data approach has been developed but has yet to be applied, Schwartz et al  
",195,6,0.812497705221176,1,0.823052227497101
271,"Human health and social work activities","OPERATIONS RESEARCH",0.819475471973419,"Studies quantitative methods for decision making, and the emphasis is on numerical algorithms to solve constrained optimisation programs. The methods studied are applicable to problems in many areas: computer science, economics, logistics, and industrial engineering.","[BOOK][B] Handbook of Research Methods in Complexity Science: Theory and Applications in complexity science Emily S. Ihara is an Associate Professor of Social Work at George  on brain
dynamics and structure by analys- ing fMRI and EEG data and he is  His research includes
applications in Social and economic systems, business, artificial intelligence and robotics  
",195,1,0.819475471973419,1,0.819475471973419
272,"Human health and social work activities","Network Science",0.800766408443451,"This course provides an introduction to the study of complex networks, including algorithms, models and applications to both artificial and real networks, including social, biological and technological networks, all sharing common features and properties. The course addresses the development of scalable algorithms and data structures so that we can efficiently study large complex networks, but also in the creation of theoretical models capable of describing empirically observed patterns. The number of applications is enormous, including web search engines, evolutionary dynamics, information diffusion on Internet, social networks and blogs, network resilience, network-driven phenomena in epidemiology and computer viruses, networks dynamics, with connections in the social sciences, physics, computational biology, and economics.Introduction to complex systems and networks science: Theory and basic concepts. Properties and characterization of biological, social and technological networks. Network models and random graphs. Efficient representation of large (sparse) networks. Succinct data-structures and coding strategies. Design and analysis of efficient and scalable algorithms for large network processing and analysis, including both sampling and randomization techniques. Databases and distributed platforms for the analysis of large networks. Link analysis and random walks. Community finding and graph partitioning. Ranking algorithms. Vertex relabeling. Dynamical processes on complex networks: The impact of network structure on economic, social and biological systems. Introduction to stochastic processes, Monte-Carlo simulations and large-scale multi-agent systems. Disease spreading and tolerance to attacks. Models of peer-influence and opinion formation. Game theory and population dynamics. Public goods problems, cooperation and reputation dynamics. Decision-making on (static and adaptive) interaction networks.","[BOOK][B] Handbook of Research Methods in Complexity Science: Theory and Applications in complexity science Emily S. Ihara is an Associate Professor of Social Work at George  on brain
dynamics and structure by analys- ing fMRI and EEG data and he is  His research includes
applications in Social and economic systems, business, artificial intelligence and robotics  
",195,5,0.808625149726868,1,0.800766408443451
286,"Human health and social work activities","Optimization for machine learning",0.827294170856476,"This course teaches an overview of modern optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.This course teaches an overview of modern optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.

Basic Contents:

Convexity, Gradient Methods, Proximal algorithms, Stochastic and Online Variants of mentioned methods, Coordinate Descent Methods, Subgradient Methods, Frank-Wolfe, Accelerated Methods, Primal-Dual context and certificates, Lagrange and Fenchel Duality, Second-Order Methods, Quasi-Newton Methods. Gradient-Free and Zero-Order Optimization.

Advanced Contents:

Parallel and Distributed Optimization Algorithms, Synchronous and Asynchronous Communication.

Lower Bounds.

Non-Convex Optimization: Convergence to Critical Points, Saddle-Point methods, Alternating minimization for matrix and tensor factorizations


An optional, graded, mini-project allows to explore the real-world performance aspects of the algorithms and variants of the course.

Keywords
Optimization, Machine learning

Learning Prerequisites
Recommended courses
CS-433 Machine Learning
Important concepts to start the course
Previous coursework in calculus, linear algebra, and probability is required.
Familiarity with optimization and/or machine learning is useful. 
Learning Outcomes
By the end of the course, the student must be able to:
Assess / Evaluate the most important algorithms, function classes, and algorithm convergence guarantees
Compose existing theoretical analysis with new aspects and algorithm variants.
Formulate scalable and accurate implementations of the most important optimization algorithms for machine learning applications
Characterize trade-offs between time, data and accuracy, for machine learning methods
Transversal skills
Use both general and domain specific IT resources and tools
Summarize an article or a technical report.","[HTML][HTML] Automated screening for Fragile X premutation carriers based on linguistic and cognitive computational phenotypes Building on prior approaches that analyze data among multiple genotypes and
multiple phenotypes 22 , we used statistical and machine-learning methods to develop
a feature selection module and a data-driven classifier (Fig  
",195,3,0.837565958499908,1,0.827294170856476
288,"Human health and social work activities","Computational Statistics",0.825306534767151,"To understand and apply computational intensive statistical techniques in applied contexts, with emphasis on sampling and statistical inference for large and complex datasets. The student should develop the ability to correctly apply: multiple hypothesis tests, Monte Carlo methods via Markov chains, resampling methods, parametric and non-parametric estimation of covariance matrices and their application in principal component analysis and outlier detection.1. Sampling and experimental design (extraction of representative samples	from large and	complex datasets).	
2. Statistical inference methods (maximum likelihood, multiple hypothesis testing, false discovery rate).	
3. Bayesian methods (Monte Carlo methods via Markov chains, prediction, diagnostic techniques).	
4. Resampling methods (bootstrap and Jackknife, permutation tests).
5. Covariance	and	correlation matrix estimation. Principal component analysis (parametric and non-parametric estimation, robustness,	sparse and nonlinear principal components,	detection of outliers).","[HTML][HTML] Automated screening for Fragile X premutation carriers based on linguistic and cognitive computational phenotypes Building on prior approaches that analyze data among multiple genotypes and
multiple phenotypes 22 , we used statistical and machine-learning methods to develop
a feature selection module and a data-driven classifier (Fig  
",195,1,0.825306534767151,1,0.825306534767151
295,"Human health and social work activities","Data Administration in Information Systems",0.795870006084442,"The course on Data Administration in Information Systems aims at providing to students the skills needed to manage, optimize and effectively use modern database systems for managing large volumes of data. Students should be able to: 1. understand the internal mechanisms of a relational Database Management System (DBMS),, including storage management, indexing, processing and optimizing queries, transaction management, concurrency control, and recovery management 2. understand the tasks involved in database administration 3. optimize information access in databases that store very large amounts of data 4. acquire basic knowledge about the various architectures of parallel and distributed databases, including conventional (SQL) and unconventional (NoSQL) database systems.The course syllabus for Data Administration in Information Systems mostly includes topics from the Information Management (IM) area in the ACM CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:
1. Storage (sub-)systems
a. Storage technologies (e.g., RAID)
b. Replication
c. Architectures
ACM CS IM/Database Systems > Components of database systems
ACM CS IM/Physical Database Design > Storage and file structure
ACM CCS 2012 Information systems > Information storage systems > Information storage technologies
2. Indexing algorithms and file organization a. Record storage
b. Buffer management
c. Data access
ACM CS IM/Database Systems > Components of database systems ACM CS IM/Database Systems > Design of core DBMS functions ACM CS IM/Indexing
ACM CCS 2012 Information systems > Information storage systems > Record storage systems
ACM CCS 2012 Information systems > Database management system engines > Record and buffer management
3. Query processing
a. Query execution planning b. Algorithms
c. Optimization
ACM CS IM/Database Systems > Components of database systems
ACM CS IM/Database Systems > Design of core DBMS functions
ACM CCS 2012 Information systems > Database management system engines > Database query processing
4. Concurrency control a. Locking protocols
b. Timestamping protocols
c. Multi-version protocols
ACM CS IM/Database Systems > Components of database systems ACM CS IM/Database Systems > Design of core DBMS functions ACM CS IM/Transaction Processing > Concurrency control
ACM CCS 2012 Information systems > Database management system engines > Database transaction processing
5. Data recovery a. Logging
b. Failure of non-volatile storage c. Backups
ACM CS IM/Database Systems > Components of database systems ACM CS IM/Database Systems > Design of core DBMS functions ACM CS IM/Transaction Processing > Failure and recovery
ACM CCS 2012 Information systems > Database management system engines > Database transaction processing > Database recovery
6. Database optimization
a. Schema-level optimization b. Query optimization
ACM CS IM/Physical Database Design > Database efficiency and tuning
ACM CCS 2012 Information systems > Database management system engines > Database query processing
ACM CCS 2012 Information systems > Database design and models > Relational database model
ACM CCS 2012 Information systems > Data structures > Data access methods
7. Index optimization a. Clustering
b. Covering indexes
ACM CS IM/Physical Database Design > Database efficiency and tuning
ACM CS IM/Indexing
ACM CCS 2012 Information systems > Information storage systems > Record storage systems > Record storage alternatives
ACM CCS 2012 Information systems > Information storage systems > Record storage systems > Directory structures
8. Optimizing the hardware and the operating systems a. Threads, buffers and storage
b. Database performance
ACM CS IM/Physical Database Design > Database efficiency and tuning
ACM CS IM/Transaction Processing > Interaction of transaction management with storage, especially buffering ACM CCS 2012 Information systems > Database management system engines > Record and buffer management ACM CCS 2012 Information systems > Database administration > Database performance evaluation
9. Parallel and distributed databases
a. Architectures b. Partitioning
c. Algorithms
d. Systems based on map-reduce
ACM CS IM/Database Systems > Approaches for managing large volumes of data
ACM CS IM/Distributed Databases
ACM CCS 2012 Information systems > Database management system engines > Parallel and distributed DBMSs > Relational parallel and distributed DBMSs
ACM CCS 2012 Information systems > Database management system engines > Parallel and distributed DBMSs > MapReduce-based systems
10. NoSQL databases
a. Key-value storage databases b. Document databases
c. Column-oriented databases
d. Databases for graph data
ACM CS IM/Database Systems > Approaches for managing large volumes of data
ACM CCS 2012 Information systems > Database management system engines > Parallel and distributed DBMSs > Key-value stores
ACM CCS 2012 Information systems > Information retrieval. The evaluation of the Data Administration in Information Systems course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).
Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).
After the exam, students whose difference between the grade of the project (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.
Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics
1, 2 and 3 of the course, the second project will cover topics 4, 5 and 6 of the course, and the third project will cover topics 7, 8, 9 and 10.","Health maintenance advisory technology for specific business sectors, eg utilities or tourism; G06Q50/10Services; G06Q50/22Socialwork  be passed to the query engine 72 as a confirmed result for machine learning algorithms
that  The user condition may be sensed by accessing such data from the server system, at  
",195,2,0.794792950153351,1,0.795870006084442
297,"Human health and social work activities","Enterprise Integration",0.784799039363861,"The main goal of this course is to provide a broad and in-depth view of the concepts, methodologies, and technologies associated with systems integration, including the integration of applications, services, and inter-organizational business processes. The topics addressed in this course are positioned at a key point between the application infrastructure and the business processes in an organization, and the aim is to understand the relationships and dependencies between the two. The course will also provide insight into how it is possible to devise a distributed and integrated application infrastructure. The concrete learning objectives are as follows: 1. To provide an in-depth view of the main concepts and integration solutions in the field of integration; 2. To develop a systematic and process-oriented vision of how integration problems should be addressed; 3. To acquire a practical knowledge of the state-of-the-art integration platforms, based on lab projects; 4. To understand the critical role that integration solutions have in the design and implementation of business processes.The course aims at providing a coherent structure of integration topics that can be found in different parts of the ACM/AIS IS 2010 curriculum, such as “Enterprise Systems” and
“Application Development”. When appropriate, this syllabus is labeled with topics from that curiculum and also from the ACM CCS 2012 taxonomy:
1. Evolution of information systems
a. essential functions of information systems in business organizations;
b. evolution of information systems architecture over the years; point-to-point vs. centralized integration;
c. integration based on the concept of service.
ACM/AIS IS 2010.1 Information Systems in Organizations
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise Information Systems
2. Introduction to integration platforms
a. message exchange;
b. message schema and transformation;
c. ports and adapters;
d. orchestrations;
e. business rules.
ACM/AIS IS 2010.3 Systems Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Service buses;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business rules
3. Messaging systems
a. fundamental concepts;
b. message transactions;
c. message acknowledgments;
d. message correlation;
e. messaging platforms.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise interoperability > Enterprise application integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Message queues
4. Message brokers
a. message-level integration vs. orchestration-level integration;
b. publish-subscribe with message filters;
c. message properties;
d. message correlation;
e. asynchronous messaging.
ACM/AIS IS 2010.AD Application Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
5. Adapters
a. three-tier client-server model;
b. capture of the user interface;
c. integration through files;
d. database access APIs;
e. retrieving data in XML;
f. data access in orchestrations;
g. methods and interfaces;
h. interface discovery and dynamic invocations;
i. Web service invocation in orchestrations.
ACM/AIS IS 2010.3 Data Integration
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Data exchange;
ACM CCS 2012 Information Systems > Data Management Systems > Information Integration > Mediators and Data Integration
ACM CCS 2012 Information Systems > World Wide Web > Web services
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
6. Services and SOA
a. services and applications;
b. service composition;
c. service orchestration;
d. business processes;
e. service design principles;
f. benefits of SOA;
g. support for human workflows.
ACM/AIS IS 2010.3 Service oriented architecture
ACM CCS 2012 Applied Computing > Enterprise Computing > Service-oriented architectures;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Business process management systems
7. Service orchestrations
a. block structure;
b. beginning the flow;
c. message construction;
d. flow control with loops, decisions, and parallelism;
e. orchestrations as sub-processes;
f. concurrent events;
g. correlations;
h. exception handling;
i. transactions and compensation.
ACM/AIS IS 2010.ES Business process integration
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Enterprise application integration tools;
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Middleware business process managers
ACM CCS 2012 Information Systems > Data Management Systems > Middleware for databases > Distributed transaction monitors
8. Inter-organizational integration
a. electronic data exchange;
b. introduction to supply chain management;
c. supply chain coordination;
d. electronic commerce;
e. negotiation protocols.
ACM/AIS IS 2010.1 Supply Chain Management
ACM/AIS IS 2010.ES Production logistics
ACM CCS 2012 Information Systems > World Wide Web > Web applications > Electronic commerce > Electronic data interchange;
ACM CCS 2012 Applied Computing > Enterprise Computing > Business process management > Cross-organizational business processes
9. Internet of things
a. physical world and virtual world integration;
b. traceability systems;
c. sensors and complex event processing;
d. logistics systems based on RFID.
ACM/AIS IS 2010.ES Enterprise Systems > Production logistics
ACM CCS 2012 Applied Computing > Enterprise Computing > Enterprise computing > Event-driven architectures
ACM CCS 2012 Information systems > Information systems applications > Spatial-temporal systems > Data streaming","Health maintenance advisory technology for specific business sectors, eg utilities or tourism; G06Q50/10Services; G06Q50/22Socialwork  be passed to the query engine 72 as a confirmed result for machine learning algorithms
that  The user condition may be sensed by accessing such data from the server system, at  
",195,7,0.81805990423475,1,0.784799039363861
307,"Human health and social work activities","Automated Planning",0.801131427288055,"Automated planning is a branch of Artificial intelligence aimed at obtaining plans (i.e. sequences of actions) for solving complex problems or for governing the behavior of intelligent agents, autonomous robots or unmanned vehicles. Planning techniques have been successfully applied in different domains, including industrial contexts, logistics, computer games, robotics or space exploration. In this seminar we will review the existing approaches for solving classical planning problems, such as state-space search, plan-space search, graph-based techniques or turning classical planning problems into propositional satisfiability problems. The course will then focus on the study of knowledge-based planning methods, such as control rule-based pruning or hierarchical task network-based planning techniques. These approaches exploit the domain knowledge provided by human experts to improve the performance of the planning algorithms. Finally, we will briefly introduce advanced planning algorithms, which are able to generate planning policies that take into account time constraints and/or partial observability conditions, which are common in real world applications.","The promise and the challenge of technology-facilitated methods for assessing behavioral and cognitive markers of risk for suicide among US Army National Guard  Salt Lake City, UT 84108, USA 4 Department of Social Work, University of  BSP also involves
additional data processing steps prior to generating behavioral markers  Recent developments
have extended these efforts by incorporating artificial intelligence techniques resulting in  
",195,5,0.809059846401215,1,0.801131427288055
316,"Human health and social work activities","Intelligent agents",0.828434944152832,"Software agents are widely used to control physical, economic and financial processes. The course presents practical methods for implementing software agents and multi-agent systems, supported by programming exercises, and the theoretical underpinnings including computational game theory.
Content
The course contains 4 main subject areas:

 

1) Basic models and algorithms for individual agents:
game-playing algorithms, reactive agents and reinforcement learning. Models and algorithms for rational, goal-oriented behavior in agents.
2) Multi-agent systems: 
multi-agent planning, distributed algorithms for constraint satisfaction, coordination techniques for multi-agent systems.
3) Self-interested agents:
Models and algorithms for implementing self-interested agents motivated by economic principles: elements of computational game theory, models and algorithms for automated negotiation, social choice, mechanism design, electronic auctions and marketplaces.
4) Implementing multi-agent systems:
Agent platforms, ontologies and markup languages, web services and standards for their definition and indexing.By the end of the course, the student must be able to:
Choose and implement methods for rational decision making in software agents, based on decision processes and AI planning techniques
Choose and implement methods for efficient rational decision making in teams of multiple software agents
Model scenarios with multiple self-interested agents in the language of game theory
Evaluate the feasibility of achieving goals with self-interested agents using game theory
Design, choose and implement mechanisms for self-interested agents using game theory
Implement systems of software agents using agent platforms","Methods and systems for growing and retaining the value of brand drugs by computer predictive model methods specially adapted for specific business sectors, eg utilities or tourism; G06Q50/10
Services; G06Q50/22Social work  [0042]. Learning Machinerefers to a  or semi-automated
process of generating a prediction based on a model, typically combining software and data  
",195,2,0.85044002532959,1,0.828434944152832
319,"Human health and social work activities","Program Analysis for System Security and Reliability",0.823507785797119,"The course introduces modern analysis and synthesis techniques (both, deterministic and probabilistic) and shows how to apply these methods to build reliable and secure systems spanning the domains of blockchain, computer networks and deep learning.        * Understand how classic analysis and synthesis techniques work, including discrete and probabilistic methods.

* Understand how to apply the methods to generate attacks and create defenses against applications in blockchain, computer networks and deep learning.

* Understand the state-of-the-art in the area as well as future trends.        The course will illustrate how the methods can be used to create more secure and reliable systems across four application domains:

Part I: Analysis and Synthesis for Computer Networks:
1. Analysis: Datalog, Batfish
2. Synthesis: CEGIS, SyNET (http://synet.ethz.ch)
3. Probabilistic: (PSI: http://psisolver.org), its applications to networks (Bayonet)

Part II: Blockchain security
1. Introduction to space and tools.
2. Automated symbolic reasoning.
3. Applications: verification of smart contracts (http://www.securify.ch)

Part III: Security and Robustness of Deep Learning:
1. Basics: affine transforms, activation functions
2. Attacks: gradient based method to adversarial generation
3. Defenses: affine domains, AI2 (http://ai2.ethz.ch)

Part IV: Probabilistic Security:
1. Enforcement: PSI + Spire.
2. Graphical models: CRFs, Structured SVM, Pseudo-likelihood.
3. Practical statistical de-obfuscation: DeGuard: http://apk-deguard.com, JSNice: http://jsnice.org, and more.

To gain a deeper understanding, the course will involve a hands-on programming project.","Methods and systems for growing and retaining the value of brand drugs by computer predictive model methods specially adapted for specific business sectors, eg utilities or tourism; G06Q50/10
Services; G06Q50/22Social work  [0042]. Learning Machinerefers to a  or semi-automated
process of generating a prediction based on a model, typically combining software and data  
",195,2,0.813472598791122,1,0.823507785797119
336,"Human health and social work activities","Advanced Topics in Machine Learning",0.796226859092712,"In this seminar, recent papers of the pattern recognition and machine learning literature are presented and discussed. Possible topics cover statistical models in computer vision, graphical models and machine learning.        The seminar ""Advanced Topics in Machine Learning"" familiarizes students with recent developments in pattern recognition and machine learning. Original articles have to be presented and critically reviewed. The students will learn how to structure a scientific presentation in English which covers the key ideas of a scientific paper. An important goal of the seminar presentation is to summarize the essential ideas of the paper in sufficient depth while omitting details which are not essential for the understanding of the work. The presentation style will play an important role and should reach the level of professional scientific presentations.        The seminar will cover a number of recent papers which have emerged as important contributions to the pattern recognition and machine learning literature. The topics will vary from year to year but they are centered on methodological issues in machine learning like new learning algorithms, ensemble methods or new statistical models for machine learning applications. Frequently, papers are selected from computer vision or bioinformatics - two fields, which relies more and more on machine learning methodology and statistical models.","[BOOK][B] Scientific Reasoning and Argumentation: The Roles of Domain-specific and Domain-general Knowledge Library of Congress Cataloging-in-Publication Data Names: Fischer, Frank, 1942- editor  Christian
Ghanem, Theories and Methods of Social Work, Katholische Stiftungshochschule München
(KSH  for the limits of domain-generality based on work in machine learning and natural  
",195,4,0.827056899666786,1,0.796226859092712
337,"Human health and social work activities","Knowledge Representation",0.796195209026337,"When humans reason about the world, we identify objects, we make categories of such objects, and we reason about the relations between the things in the world around us. How can we represent such knowledge in a computer, in such a way that a  computer could reason about the world around it in a similar way?

The field of Knowledge Representation and Reasoning aims to represent knowledge in such a form that a computer system can use it to solve complex tasks such as diagnosing a medical condition or having an intelligent dialog in a natural language. Knowledge representat­­ion and reasoning uses logic as its main mathematical tool, and tries to answer such questions as: how can we design logics that can efficiently reason with very large amounts of knowledge? Which logics are suited for reasoning about space and time? How can we deal with uncertainty and vagueness? How to reason about changes in the world around us?

Knowledge Representation techniques are used in many practical applications. Examples are expert systems for medical diagnosis, decision support systems for judges, and intelligent dialogue systems such as Siri on the iPhone.­","[BOOK][B] Scientific Reasoning and Argumentation: The Roles of Domain-specific and Domain-general Knowledge Library of Congress Cataloging-in-Publication Data Names: Fischer, Frank, 1942- editor  Christian
Ghanem, Theories and Methods of Social Work, Katholische Stiftungshochschule München
(KSH  for the limits of domain-generality based on work in machine learning and natural  
",195,2,0.793018519878387,1,0.796195209026337
338,"Human health and social work activities","Ontological Engineering",0.795840501785278,"The aim of this course is to discuss on the scientific, methodological and technological foundations that need to be considered when building ontologies. In particular, sessions in this course will cover: the concepts of ontologies and ontology-based annotations in the context of the Semantic Web and the Web of Linked Data; the theoretical foundations in ontology development; some of the most widely-known ontologies; the RDF(S) and OWL languages; methodologies, methods and techniques used in ontology development, including requirements specification, planning, conceptualisation, reuse, reengineering, etc.; methods and techniques for ontology-based annotation and for the generation of Linked Data; and relevant applications. Throughout the entire course open research problems will be presented and discussed collaboratively for each subtopic.","[BOOK][B] Scientific Reasoning and Argumentation: The Roles of Domain-specific and Domain-general Knowledge Library of Congress Cataloging-in-Publication Data Names: Fischer, Frank, 1942- editor  Christian
Ghanem, Theories and Methods of Social Work, Katholische Stiftungshochschule München
(KSH  for the limits of domain-generality based on work in machine learning and natural  
",195,1,0.795840501785278,1,0.795840501785278
347,"Human health and social work activities","Natural Language Processing",0.79008936882019,"The purpose of this seminar derives from the need to fill a gap in the teaching of subjects that are, generally speaking, on Language Engineering. On the one hand, when we talk about Engineering, then we talk about design, methodologies, techniques, systems, and components; on the other hand, when we talk about language then we talk about grammars, corpora, dictionaries, etc. Usually, the teaching of these subjects often has a tendency, perhaps excessive, to one side or another. This seminar aims to provide a unified view of both sides, from the fundamentals to applications. The area of Linguistic Engineering is considered to be one of the areas where most research and development efforts will lie in the next few years, if we are to achieve the goal of having machines that really make our lives easier in a simple way. The seminar is focused, in the first part, on the state of the art technologies, followed by a second part where we will explore in depth technologies that allow supporting applications on the market. For practical reasons, the practice work will be focused in word processing technologies.","[PDF][PDF] The future of well-being in a tech-saturated world Dangers: Tiziana Dearing, a professor at the Boston College School of Social Work, said, People's
well  lives, technology companies will find new, invasive ways to exploit data generated on  As
we enter the Artificial Intelligence era we must examine and make transparent how  
",195,3,0.800235470136007,1,0.79008936882019
350,"Human health and social work activities","Data Analystics for Smart Grids",0.840557456016541,"To provide students with the ability to design and develop data analytics solutions capable of exploring large datasets obtained in multiple intelligent network operation contexts. Understanding the relationship between data availability and the extractable information of such data, as well as the impact of information errors on the validity of the underlying network analysis. Developing critical capacity on data acquisition and storage requirements and anticipate realistic advances in the operation of smart grids. 1.	Physical networks, problems associated with electric power systems, and operational technologies (OT): reliability analysis, energy transit, and state estimation.
2.	Opportunities created in the power system by cybernetic networks and information technology (IT): event and incident databases, smart meter measurements, and PMU grid.
3.	Interaction between the physical problems underlying network analysis, OT requirements and the possibilities opened by IT.
4.	Statistical methods and fundamentals of data analytics: a) Dimensionality reduction: tensor analysis and principal components; b) Statistical inference and classification: clustering and regression; c) Dynamic modeling: parameterization of Markov chains.
5.	Development of data analytics algorithms for intelligent networks: a) Asset management supported by reliability analysis: reduction of dimensionality over data, event and incident records; b) Incident prediction based on reliability analysis and meteorological forecasts: statistical inference of records of incidents on meteorological conditions; c) Optimization of operational efficiency supported in the transit of energy: dynamic modeling of meter data; d) Situational supervision based on state estimation: inverse problems with counters data and PMU.
6.	Perspective on future applications of data analytics in smart grids, big data and storage requirements.","Countering Expansion and Organization of Terrorism in Cyberspace making processes. I collected empirical data on the situational factors and the thought and the
decision-making processes of experts by performing secondary data analysis and  DefinitionsArtificial intelligence (AI): The science and engineering of creating intelligent  
",195,67,0.828709795403836,1,0.840557456016541
351,"Human health and social work activities","Applied biostatistics",0.838865101337433,"This course covers topics in applied biostatistics, with an emphasis on practical aspects of data analysis using R statistical software. Topics include types of studies and their design and analysis, high dimensional data analysis (genetic/genomic) and other topics as time and interest permit.Types of studies
Design and analysis of studies
R statistical software
Reproducible research techniques and tools
Report writing
Exploratory data analysis
Liniear modeling (regression, anova)
Generalized linear modeling (logistic, Poission)
Survival analysis
Discrete data analysis
Meta-analysis
High dimensional data analysis (genetics/genomics applications)
Additional topics as time and interest permit
Keywords
Data analysis, reproducible research, statistical methods, R, biostatistical data analysis, statistical data analysis. By the end of the course, the student must be able to:
Interpret analysis results
Justify analysis plan
Plan analysis for a given dataset
Analyze various types of biostatistical data
Synthesize analysis into a written report
Report plan of analysis and results obtained
Transversal skills
Write a scientific or technical report.
Assess one's own level of skill acquisition, and plan their on-going learning goals.
Take feedback (critique) and respond in an appropriate manner.
Use a work methodology appropriate to the task.","Countering Expansion and Organization of Terrorism in Cyberspace making processes. I collected empirical data on the situational factors and the thought and the
decision-making processes of experts by performing secondary data analysis and  DefinitionsArtificial intelligence (AI): The science and engineering of creating intelligent  
",195,4,0.835054457187653,1,0.838865101337433
353,"Human health and social work activities","Simulation Methods",0.825732111930847,"Simulation consists on building computer models that describe the essential behavior of a
system of interest and designing and conducting experiments with such models in order
to draw conclusions from their results in order to support decision-making. Typically, it is
used in the analysis of such complex systems, so it is not possible to make an analytical
analysis, or on the basis of a numerical analysis. Nowadays, simulation is a fundamental
experimental methodology in fields as diverse as economics, statistics, computer science,
chemical engineering, ecology and physics, with huge industrial and commercial
applications, ranging from manufacturing systems to flight simulators, through computer
games, stock prediction and weather forecasting.
In the subject we will show multiple applications in Artificial Intelligence, especially in the
discipline of Decision Analysis","Countering Expansion and Organization of Terrorism in Cyberspace making processes. I collected empirical data on the situational factors and the thought and the
decision-making processes of experts by performing secondary data analysis and  DefinitionsArtificial intelligence (AI): The science and engineering of creating intelligent  
",195,6,0.808148324489594,1,0.825732111930847
400,"Human health and social work activities","DD2423 Image Analysis and Computer Vision",0.799263954162598,"After completing the course with a passing grade the student should be able to:
• identify basic concepts, terminology, models and methods in computer vision and image processing
• develop and evaluate a number of basic methods in computer vision and image processing systematically
• choose and apply methods for processing of image data related to image filtrering, image enhancement, segmentation, classification and representation,
• account for basic methods in computer vision as multiscale representation, detection of edges and other distinctive features, stereo, movement and object recognition to
• later as a working professional be able to decide how basic possibilities and limitations influence the choice of methods in image processing and computer vision for specific applications
• independently be able to implement, analyse and evaluate simple methods for computer vision and image processing
• be able to read and apply professional literature in the area.

Course main content
Overview about aims and methods for image analysis, image processing and computer vision. Orientation about biological seeing and visual perception. Properties of the perspective image formation.

Basic image analysis: signal theoretical methods, filtering, image enhancement, image reconstruction, segmentation, classification, representation.

Basic computer vision: multiscale representation, detection of edges and other distinctive features. Stereo and multi-camera systems. Object recognition, morphology.","[BOOK][B] Virtual Reality and the Criminal Justice System: Exploring the Possibilities for Correctional Rehabilitation Page 13. Introduction 5 Figure 0.1. Sensorama machine. Image courtesy of Katalin Heilig. goggles
for the purpose of streaming data or images to the user  Semi- and fully-immersive systems have
been commonly used when the participant is learning or practicing new skills  
",195,1,0.799263954162598,1,0.799263954162598
401,"Human health and social work activities","Computer vision",0.795282125473022,"Computer Vision aims at modeling the world from digital images acquired using video or infrared cameras, and other imaging sensors. We will focus on images acquired using digital cameras. We will introduce basic processing techniques and discuss their field of applicability.Introduction

History of Computer Vision
Human vs Machine Vision
Image formation
Extracting 2D Features

Contours
Texture
Regions
3D Shape Recovery

From one single image
From multiple images.By the end of the course, the student must be able to:
Choose relevant algorithms in specific situations
Perform simple image-understanding tasks.","[BOOK][B] Virtual Reality and the Criminal Justice System: Exploring the Possibilities for Correctional Rehabilitation Page 13. Introduction 5 Figure 0.1. Sensorama machine. Image courtesy of Katalin Heilig. goggles
for the purpose of streaming data or images to the user  Semi- and fully-immersive systems have
been commonly used when the participant is learning or practicing new skills  
",195,1,0.795282125473022,1,0.795282125473022
402,"Human health and social work activities","3D Vision",0.792023122310638,"The course covers camera models and calibration, feature tracking and matching, camera motion estimation via simultaneous localization and mapping (SLAM) and visual odometry (VO), epipolar and mult-view geometry, structure-from-motion, (multi-view) stereo, augmented reality, and image-based (re-)localization.        After attending this course, students will:
1. understand the core concepts for recovering 3D shape of objects and scenes from images and video.
2. be able to implement basic systems for vision-based robotics and simple virtual/augmented reality applications.
3. have a good overview over the current state-of-the art in 3D vision.
4. be able to critically analyze and asses current research in this area.        The goal of this course is to teach the core techniques required for robotic and augmented reality applications: How to determine the motion of a camera and how to estimate the absolute position and orientation of a camera in the real world. This course will introduce the basic concepts of 3D Vision in the form of short lectures, followed by student presentations discussing the current state-of-the-art. The main focus of this course are student projects on 3D Vision topics, with an emphasis on robotic vision and virtual and augmented reality applications.","[BOOK][B] Virtual Reality and the Criminal Justice System: Exploring the Possibilities for Correctional Rehabilitation Page 13. Introduction 5 Figure 0.1. Sensorama machine. Image courtesy of Katalin Heilig. goggles
for the purpose of streaming data or images to the user  Semi- and fully-immersive systems have
been commonly used when the participant is learning or practicing new skills  
",195,2,0.786151528358459,1,0.792023122310638
404,"Human health and social work activities","Artificial Intelligence for Robotics",0.789513647556305,"This course provides tools from statistics and machine learning enabling the participants to deploy them as part of typical perception pipelines. All methods provided within the course will be discussed in context of and motivated by example applications from robotics. The accompanying exercises will involve implementations and evaluations using typical robotic datasets.        Working knowledge of basic methods from statistics and machine learning.        Probability Recap; Basic Concepts of Machine Learning; Regression; Dimensionality Reduction; Clustering; Support Vector Machines; Deep Learning;","[BOOK][B] Virtual Reality and the Criminal Justice System: Exploring the Possibilities for Correctional Rehabilitation Page 13. Introduction 5 Figure 0.1. Sensorama machine. Image courtesy of Katalin Heilig. goggles
for the purpose of streaming data or images to the user  Semi- and fully-immersive systems have
been commonly used when the participant is learning or practicing new skills  
",195,4,0.830730319023132,1,0.789513647556305
411,"Human health and social work activities","Advanced Machine Learning",0.787738680839539,"Machine learning is a scientific discipline that is concerned with the design and development of algorithms that allow computers to learn from data. A major focus of machine learning is to automatically learn complex patterns and to make intelligent decisions based on them. The set of possible data inputs that feed a learning task can be very large and diverse, which makes modelling and prior assumptions critical problems for the design of relevant algorithms.
This course focuses on the methodology underlying supervised and unsupervised learning, with a particular emphasis on the mathematical formulation of algorithms, and the way they can be implemented and used in practice. We will therefore describe some necessary tools from optimization theory, and explain how to use them for machine learning. Numerical illustrations will be given for most of the studied methods.
We will follow the book from Hastie, Tibshirani and Friedman called ""Elements of Statistical Learning"". This will define the structure of the course even if we will sometime complement the book during the lectures


Syllabus :

Overview of Supervised Learning (Chap 1, new)
 Linear method for regression (Chap 2, compl.)
Linear Methods for Classification (Chap3, compl.), Kernel Smoothing Methods (Chap 6, compl.)
Model Assessment and Selection (Chap 7, new)
Trees (Chap 9, new), Boosting (Chap 10, new)
Averaging (Chap 8, new), Random Forests (Chap 15, new), Ensemble Methods Chap 16, new)
Neural networks (Chap 11, new)
Support Vector Machines (Chap 12, compl.)","Suicide attempts of community adolescents and young adults: an explanatory and predictive epidemiological approach My first association with (co-)authorship is intensive learning (again). That's not the worst first
association  the focus away from risk factors towards risk algorithms, ie Machine Learning (ML).
ML is deemed to be well suited to investigate complex associations in data  
",195,1,0.787738680839539,1,0.787738680839539
415,"Human health and social work activities","Autonomous Robots",0.83679735660553,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","Future Implications of the Psychopathy Construct for Criminology and Criminal Justice Policy and Practice Torture, 6 For example, Rhodes (2002) provides participation observation data showing that 
Sensors and machine learning algorithms have been designed that can measure affective
information  In addition, artificial intelligence advances that can make robots feel so to speak  
",195,10,0.822185301780701,1,0.83679735660553
430,"Agriculture, forestry and fishing","Decision Support Models",0.803921580314636,"At the completion of the course, the student will: be familiar with distinct decision-making strategies and traps in the evaluation of options and in the allocation of resources in private and public contexts; be familiar with key theoretical and methodological concepts of decision-making and decision aid relevant for the best practice of decision engineering; be familiar with models, processes and tools for helping to structure and explore decisions characterized by multiple objectives, uncertainty, complexity and differences of opinion; be familiar with examples of real-world decision analysis and decision conferencing applications in organizations; be familiar with other topics considered relevant for engineering decisions, covering problem structuring methods, heuristics and biases and group decision and negotiation; have developed skills in decision analysis and modeling; • be able to select and use specialized decision support software in different decision contexts.The decision making problematic: Definition of the decision problem. Importance of decision making in
engineering and management. Characteristics of the decision context.
Decision making strategies. Uncertainty and complexity. Value and risk.
What is Decision Analysis (DA)? DA objectives. The seven fundamental steps of DA. DA schools of thought
and theoretical foundations. The problem of decision aiding.
Intervention strategies: From optimization to the learning paradigm. Value and utility analysis. Decision
conference and facilitation.
Concepts, models, techniques and software for decision support:
1. Decision trees and influence diagrams; case studies; PRECISION TREE.
2. Bayesian networks; case studies; NETICA.
3. Probabilities modeling and risk analysis; case studies; @RISK.
4. Cognitive mapping; case studies; DECISION EXPLORER.
5. Multiple criteria evaluation models; case studies; MACBETH.
6. Resource allocation and negotiation; case studies; PROBE and MACBETH.Teaching is mostly organized by groups of models, techniques and software for decision support that can
assist different types of decision problems. For each type of decision problem, teaching is based on the
presentation of methods, models and techniques to assist decision-makers, followed by a discussion of
real world case studies and of key methodological aspects, and on the use of decision support tools. For
some topics students also carry out practical exercises.
Evaluation is done through two groupwork assignments and one individual exam. In one groupwork
students structure problems characterized by uncertainty, build models and implement them in appropriate
software; in another groupwork students build a multicriteria evaluation model to assist a decision-maker
in a real problem.","[HTML][HTML] Analysis of agriculture data using data mining techniques: application of big dataIn agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estimation for the ",180,6,0.812497705221176,1,0.803921580314636
476,"Agriculture, forestry and fishing","Information Security",0.807638585567474,"This course provides an introduction to Information Security. The focus
is on fundamental concepts and models, basic cryptography, protocols and system security, and privacy and data protection. While the emphasis is on foundations, case studies will be given that examine different realizations of these ideas in practice.        Master fundamental concepts in Information Security and their
application to system building. (See objectives listed below for more details).        1. Introduction and Motivation (OBJECTIVE: Broad conceptual overview of information security) Motivation: implications of IT on society/economy, Classical security problems, Approaches to 
defining security and security goals, Abstractions, assumptions, and trust, Risk management and the human factor, Course verview. 2. Foundations of Cryptography (OBJECTIVE: Understand basic 
cryptographic mechanisms and applications) Introduction, Basic concepts in cryptography: Overview, Types of Security, computational hardness, Abstraction of channel security properties, Symmetric 
encryption, Hash functions, Message authentication codes, Public-key distribution, Public-key cryptosystems, Digital signatures, Application case studies, Comparison of encryption at different layers, VPN, SSL, Digital payment systems, blind signatures, e-cash, Time stamping 3. Key Management and Public-key Infrastructures (OBJECTIVE: Understand the basic mechanisms relevant in an Internet context) Key management in distributed systems, Exact characterization of requirements, the role of trust, Public-key Certificates, Public-key Infrastructures, Digital evidence and non-repudiation, Application case studies, Kerberos, X.509, PGP. 4. Security Protocols (OBJECTIVE: Understand network-oriented security, i.e.. how to employ building blocks to secure applications in (open) networks) Introduction, Requirements/properties, Establishing shared secrets, Principal and message origin authentication, Environmental assumptions, Dolev-Yao intruder model and 
variants, Illustrative examples, Formal models and reasoning, Trace-based interleaving semantics, Inductive verification, or model-checking for falsification, Techniques for protocol design, 
Application case study 1: from Needham-Schroeder Shared-Key to Kerberos, Application case study 2: from DH to IKE. 5. Access Control and Security Policies (OBJECTIVES: Study system-oriented security, i.e., policies, models, and mechanisms) Motivation (relationship to CIA, relationship to Crypto) and examples Concepts: policies versus models versus mechanisms, DAC and MAC, Modeling formalism, Access Control Matrix Model, Roll Based Access Control, Bell-LaPadula, Harrison-Ruzzo-Ullmann, Information flow, Chinese Wall, Biba, Clark-Wilson, System mechanisms: Operating Systems, Hardware Security Features, Reference Monitors, File-system protection, Application case studies 6. Anonymity and Privacy (OBJECTIVE: examine protection goals beyond standard CIA and corresponding mechanisms) Motivation and Definitions, Privacy, policies and policy languages, mechanisms, problems, Anonymity: simple mechanisms (pseudonyms, proxies), Application case studies: mix networks and crowds. 7. Larger application case study: GSM, mobility
","0245 Big data and occupational health vigilance: use of french medico-administrative databases for hypothesis generation regarding occupational risks in agriculture Poster Presentation. Methodology. 0245 Big data and occupational health vigilance: use of 
medico-administrative databases for hypothesis generation regarding occupational risks inagriculture  complementary methods relying on exploitation of already existing data, such as  
",180,4,0.811841815710068,1,0.807638585567474
496,"Agriculture, forestry and fishing","Simulation Methods",0.819752275943756,"Simulation consists on building computer models that describe the essential behavior of a
system of interest and designing and conducting experiments with such models in order
to draw conclusions from their results in order to support decision-making. Typically, it is
used in the analysis of such complex systems, so it is not possible to make an analytical
analysis, or on the basis of a numerical analysis. Nowadays, simulation is a fundamental
experimental methodology in fields as diverse as economics, statistics, computer science,
chemical engineering, ecology and physics, with huge industrial and commercial
applications, ranging from manufacturing systems to flight simulators, through computer
games, stock prediction and weather forecasting.
In the subject we will show multiple applications in Artificial Intelligence, especially in the
discipline of Decision Analysis","[PDF][PDF] BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM intelligence dedicated to the study of algorithms for prediction and inference. Learning  about
the data-generating mechanism in practical scenarios. Precision animal agriculture allows
farmers to formulate prompt management practices, and a predictive  
",180,6,0.808148324489594,1,0.819752275943756
498,"Agriculture, forestry and fishing","Bioinformatics",0.816590905189514,"Bioinformatics aims at developing computational methods and algorithms to process biological data and uses mathematical and statistical modelling to generate testable hypotheses about biological entities and processes. The goal of this course is to introduce the basic techniques that support the most recent developments on this field. Additionally, it enables the development of the ability to critically assess research publications in this field. Practical assignments during the course aim at developing the student's ability to develop software for bioinformatics.Introduction, Molecular biology main concepts, Introduction to algorithms and complexity
Graphs and genetics
DNA sequence analysis
Pairwise alignment
Multiple Sequence alignment
Motif finding
NGS data, algorithms and data structures
Probabilistic models
Gene expression data analysis
Data mining
Unsupervised Learning: Clustering and Biclustering
Molecular phylogenetics
Supervised Learning: Decision trees, Bayesian methods
Integrative data analysis
Seminar","[PDF][PDF] BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM intelligence dedicated to the study of algorithms for prediction and inference. Learning  about
the data-generating mechanism in practical scenarios. Precision animal agriculture allows
farmers to formulate prompt management practices, and a predictive  
",180,6,0.80968498190244,1,0.816590905189514
508,"Agriculture, forestry and fishing","Mobile and Wireless Networks",0.808956384658813,"The student has a good insight in the most important characteristics of the PHY, MAC, network and transport layer in wireless and mobile networks.
Moreover, the student has a good overview of wireless and mobile networks in use and to be in use in the near future, in the area of public cellular networks, wireless LAN, wireless personal networks, ad hoc networks and sensor networks.

These learning outcomes correspond with the following general learning outcomes for Master in the computer science:
- Analysis and design of large computer science projects: the student has the ability to divide a large problem into smaller problems and to devise a solution for each of these sub-problems
- The student is able to select techniques, methods and architectures for the problem at hand, taking into account
the specific characteristics of the system under study.
 This course consists of two parts. In a first part, important characteristics of the PHY, MAC, network and transport layer of wireless and mobile networks are considered. In addition a number of important protocols for each of these layers are studied. In a second part mobile and wireless networks are discussed. The following systems are studied: cellular networks: UMTS, LTE, TETRA; wireless personal area networks: bluetooth, zigbee; Wireless LAN: WiFi; Wireless MAN: WiMAX; sensornetworks, ad hoc networks.","Machine learning based data processing and latency reduction in the internet of things for agricultureThe Internet of Things is best stated as a network of things that have the ability to generate and share information between themselves and interact with the environment according to the percepts from this environment. This network between these devices and humans ",180,2,0.802776336669922,1,0.808956384658813
537,"Agriculture, forestry and fishing","Remote Sensing",0.833006918430328,"To provide the students with the ability to manipulate remotely sensed images of varied nature from different planetary surfaces. The student should be able to proceed through the several steps of the remote sensing chain, namely in the processing and analysis of digital images, in order to identify and classify the content of the images.Basics of remote sensing. Eelctromagnetic radiation. Electromagnetic spectrum. Interactions with the atmosphere and the Earth surface. Spectral reflectance of water, soil, rocks, vegetation and other covers. Passive and active sensors. Characteristics of digital images. Spatial, spectral, radiometric and temporal resolutions. Types of plataforms and sensors and their characteristics. Multi and hyperspectral sensors. Thermal sensors. Radar sensors and images. Syntehic Aperture Radar (SAR) adn processing techniques. Geometrical corrections and georreferencing. Image pre-processing techniques. Image segmentation and object definition. Types of features and its extraction. Methods of supervised and non-supervised classification. Applications areas and case studies.","Artificial Intelligence on Remote Sensing Data for Precision Agriculture ApplicationsPrecision agriculture benefits greatly from information provided by high spatial resolution and high temporal frequency remotely sensed images. It requires effective methodologies and algorithms to exact information from the huge volume, dimension and variety of raw ",180,1,0.833006918430328,1,0.833006918430328
539,"Agriculture, forestry and fishing","DD2437 Artificial Neural Networks and Deep Architectures",0.821380019187927,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Artificial Intelligence on Remote Sensing Data for Precision Agriculture ApplicationsPrecision agriculture benefits greatly from information provided by high spatial resolution and high temporal frequency remotely sensed images. It requires effective methodologies and algorithms to exact information from the huge volume, dimension and variety of raw ",180,33,0.829086807641116,1,0.821380019187927
544,"Agriculture, forestry and fishing","Information Visualization",0.851442337036133,"The main goal is to provide students with knowledge in the área of Information Visualization, that allows them to design and develop high-impact visualizations of data and information, to effectively transmit qualitative and quantitative data. The area of Information Visualization will be introduced, after which we’ll teach a methodology for analyzing problema domains and conceiving effective visualizations. Afterwards, we'll d’scuss the different kinds of variables (continuous, nominal, ratio, etc.), data (tabular, networks, text, etc.) and patterns to visualize. Next, we’ll describe the different relevant physiological and psychological factors (memory, visual processing, etc.) relevant for the creation of good visualizations. We’ll study the most common kinds of visualizations adequate for different information types (graphs, time series, etc.) and interaction techniques (focus+context, overview+detail, panning+zoom, brushing, etc). Finally we’ll address issues related with the evaluation of the effectiveness of InfoVis applications.1.        Introduction
2.        Design Methodology
3.        Datasets and variables
4.        Human Factors in InfoVis
5.        Visualization Types
6.        Visualization Techniques
7.        Dynamic visualizations and animations
8.        Item and Attribute reduction 
9.        Legibility and fidelity of visualizations
10.        Evaluation of InfoVis Solutions
11.        Applications.  This course has a lab component in which the project is developed. This project is a visualization of a dataset chosen by the students. In the lab classes the several stages of the visualization design process will be performed and presented (providing the students with timely feedback), including: chouse of dataset, data preparation/cleaning, visualization sketches/proposals, different prototypes, and user evaluation of the final solution.
Projet: The project, developed throughout the semester with help from the lab classes, will consisto n the development of a visualization of a chosen dataset, resorting to web technology and libraries such as D3.js. At the end, students will deliver a prototype and project report.","Big data analytics in agriculture and distribution channel with map reduce using ct image analysis provide best result [3] Precision agriculture presents
great  My proposed solution for this all type of problem use big data for distributed computing  as
use bossiness analytical application like pentaho BI that give 3d data visualization with  
",180,2,0.849806368350983,1,0.851442337036133
549,"Agriculture, forestry and fishing","Computational Intelligence Lab",0.818429827690125,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","Big-Data-Augmented Approach to Emerging Technologies Identification: Case of Agriculture and Food Sector and shortcomings of currently available studies on emerging technologies in agriculture and
food  The opportunities of the new big-data-augmented methodology are shown in comparison
to  with special attention to use of bigger volumes of data, machine learning and ontology  
",180,11,0.82310460914265,1,0.818429827690125
590,"Agriculture, forestry and fishing","Time Series Analysis",0.850259602069855,"This course examines models and statistical techniques used to study time series data. The main objective is to equip students with the methods and software tools they need for carrying out state-of-the-art empirical research on time series, with emphasis on applications in economics and finance.Basics aspects of time-domain and frequency-domain methods, methods for model-based estimation, model selection, diagnostics, forecasting, and computing as they relate to time series analysis. ARMA and seasonal ARIMA models, the Box-Jenkins approach for SARIMA modelling, spectral analysis, computing forecast for a variety of linear methods and models, nonlinear models, ARCH/GARCH models, risk models, state space models and the Kalman Filter, Monte Carlo simulation and other advanced topics if time permitted.","[CITATION][C]  Agriculture Sectors: An Analysis of the Current Models and Results of a Novel Approach Using Machine Learning Techniques with Retail Scanner DataBig data analytics framework for agriculture",180,6,0.835573434829712,1,0.850259602069855
602,"Arts, entertainment and recreation","Computational Intelligence Lab",0.844501972198486,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","[PDF][PDF] Augmented Imagination: Machine Learning Art as AutomatismIn one corner, there are designers focusing on applying the strengths of neural networks to the design field. They dream up new,intelligent, generative tools that, for example, help analyze data or produce a thousand variations of a design in an effort to select the best one ",35,11,0.82310460914265,1,0.844501972198486
603,"Arts, entertainment and recreation","Model Driven Engineering",0.83877170085907,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","[PDF][PDF] Augmented Imagination: Machine Learning Art as AutomatismIn one corner, there are designers focusing on applying the strengths of neural networks to the design field. They dream up new,intelligent, generative tools that, for example, help analyze data or produce a thousand variations of a design in an effort to select the best one ",35,9,0.815234687593248,1,0.83877170085907
604,"Arts, entertainment and recreation","Machine Learning",0.831539690494537,"In Machine Learning we develop algorithms that can improve their performance by learning from experience. Experience often comes in the form of very large amounts of data, or ""Big Data"". The resulting algorithms and models are used for making predictions and for improving decisions. It has become a core technology for a wide variety of applications such as: text and image classification; information retrieval, robot control; discovering causal explanations, social network analysis, customer intelligence; anomaly detection, recommender systems, fraud detection, forecasting and so on.

Due to the increased availability of data from sensors (Internet-of-Things), the range of applications is growing fast. The emphasis in this profile is on algorithms and statistical models that explain why and when algorithms work. We also discuss a number of algorithms in detail, such as clustering, dimensionality reduction, regression and classification, graphical models and deep learning.  The profile has a strong mathematical component, but there is also an emphasis on developing the skills to implement machine learning algorithms through project assignments. As a student in Machine Learning you can do your master's thesis on a fundamental topic. e.g. developing a new general algorithm, but also on a more applied topic, e.g. developing an innovative application. Many students conduct their thesis research as an intern with a company. ","[PDF][PDF] Augmented Imagination: Machine Learning Art as AutomatismIn one corner, there are designers focusing on applying the strengths of neural networks to the design field. They dream up new,intelligent, generative tools that, for example, help analyze data or produce a thousand variations of a design in an effort to select the best one ",35,79,0.825409763975988,1,0.831539690494537
608,"Arts, entertainment and recreation","Simulation Methods",0.781368374824524,"Simulation consists on building computer models that describe the essential behavior of a
system of interest and designing and conducting experiments with such models in order
to draw conclusions from their results in order to support decision-making. Typically, it is
used in the analysis of such complex systems, so it is not possible to make an analytical
analysis, or on the basis of a numerical analysis. Nowadays, simulation is a fundamental
experimental methodology in fields as diverse as economics, statistics, computer science,
chemical engineering, ecology and physics, with huge industrial and commercial
applications, ranging from manufacturing systems to flight simulators, through computer
games, stock prediction and weather forecasting.
In the subject we will show multiple applications in Artificial Intelligence, especially in the
discipline of Decision Analysis","Computer art design based on artificial intelligenceThis article studies the design of computer art based on artificial intelligence in the digital media environment, and mainly discusses the re-creation of the classic animated images. By analyzing and summarizing the animated images, the components needed to form the ",35,6,0.808148324489594,1,0.781368374824524
609,"Arts, entertainment and recreation","3D Vision",0.780279934406281,"The course covers camera models and calibration, feature tracking and matching, camera motion estimation via simultaneous localization and mapping (SLAM) and visual odometry (VO), epipolar and mult-view geometry, structure-from-motion, (multi-view) stereo, augmented reality, and image-based (re-)localization.        After attending this course, students will:
1. understand the core concepts for recovering 3D shape of objects and scenes from images and video.
2. be able to implement basic systems for vision-based robotics and simple virtual/augmented reality applications.
3. have a good overview over the current state-of-the art in 3D vision.
4. be able to critically analyze and asses current research in this area.        The goal of this course is to teach the core techniques required for robotic and augmented reality applications: How to determine the motion of a camera and how to estimate the absolute position and orientation of a camera in the real world. This course will introduce the basic concepts of 3D Vision in the form of short lectures, followed by student presentations discussing the current state-of-the-art. The main focus of this course are student projects on 3D Vision topics, with an emphasis on robotic vision and virtual and augmented reality applications.","Computer art design based on artificial intelligenceThis article studies the design of computer art based on artificial intelligence in the digital media environment, and mainly discusses the re-creation of the classic animated images. By analyzing and summarizing the animated images, the components needed to form the ",35,2,0.786151528358459,1,0.780279934406281
610,"Arts, entertainment and recreation","Human computer interaction",0.820736348628998,"This course starts with a simple premise: if a piece of software is useful, joyful and easy to use, people will want it. We thus teach methods for engaging user experience design.
Content
Basic concepts of human-computer interaction

Introduction to HCI: its aims and goals

Design thinking

Qualitative research

User modeling: persona and empathy diagram

Task analysis

Visual design

Basic concepts of cognitive science

How people reason and mental models

How people learn to use software products

How people perceive the world

How people process information

Prototyping methods for HCI design

Storyboarding

Wireframe prototyping

Interactive prototyping

Video prototyping

Evaluation techniques

Cognitive walkthrough

Heuristic evaluation

 

 

 

Keywords
User experience design, design thinking, usability, design for engaging users, rapid prototyping techniques, evaluation with users, design challenge. Interview users and elicit their needs using the goal-directed design method
Design interfaces and intearctions
Project management: set objectives and device a plan to achieve them
Group work skills: discuss and identify roles, and assume those roles including leadership
Communication: writing and argumentation skills
Design and implement interfaces and intearctions","A System for Evolving Art Using Supervised Learning and Aesthetic AnalogiesAesthetic experience is an important aspect of creativity and our perception of the world around us. Analogy is a tool we use as part of the creative process to translate our perceptions into creative works of art. In this paper we present our research on the ",35,1,0.820736348628998,1,0.820736348628998
612,"Arts, entertainment and recreation","Programming Paradigms",0.792048394680023,"You have to understand the principles of the lambda-calculus and to be able to apply them in small examples: reduction, combinators, lists, etc.
You have to be able to explain how one gets from the lambda-calculus to an implementation, and what is the role therein of such concepts as evaluation strategies, combinators, etc.
You have to be able to explain, based on concrete examples, how a prolog system solves queries, and what is the use of unification, proof trees, etc.
You have to solve simple programming exercises in a functional as well as a logical language.The course covers several  programming paradigms that play an important role in computer science and in particular in artificial intelligence. The emphasis is on functional and logical programming languages.

First the theoretical foundation of the functional paradigm is treated in the form of a brief introduction into the lambda-calculus, then an introduction is given into the language Haskell. In the treatment of the logical paradigm the emphasis is on the language Prolog, its advantages and limitations.

","A System for Evolving Art Using Supervised Learning and Aesthetic AnalogiesAesthetic experience is an important aspect of creativity and our perception of the world around us. Analogy is a tool we use as part of the creative process to translate our perceptions into creative works of art. In this paper we present our research on the ",35,2,0.801595956087112,1,0.792048394680023
614,"Arts, entertainment and recreation","Knowledge Representation",0.789841830730438,"When humans reason about the world, we identify objects, we make categories of such objects, and we reason about the relations between the things in the world around us. How can we represent such knowledge in a computer, in such a way that a  computer could reason about the world around it in a similar way?

The field of Knowledge Representation and Reasoning aims to represent knowledge in such a form that a computer system can use it to solve complex tasks such as diagnosing a medical condition or having an intelligent dialog in a natural language. Knowledge representat­­ion and reasoning uses logic as its main mathematical tool, and tries to answer such questions as: how can we design logics that can efficiently reason with very large amounts of knowledge? Which logics are suited for reasoning about space and time? How can we deal with uncertainty and vagueness? How to reason about changes in the world around us?

Knowledge Representation techniques are used in many practical applications. Examples are expert systems for medical diagnosis, decision support systems for judges, and intelligent dialogue systems such as Siri on the iPhone.­","A System for Evolving Art Using Supervised Learning and Aesthetic AnalogiesAesthetic experience is an important aspect of creativity and our perception of the world around us. Analogy is a tool we use as part of the creative process to translate our perceptions into creative works of art. In this paper we present our research on the ",35,2,0.793018519878387,1,0.789841830730438
615,"Arts, entertainment and recreation","Applications of Artificial Intelligence",0.799025535583496,"The seminar is a compendium of Artificial Intelligence applications naturally taking full advantage of the research potential of professors at DIA and the experience of its members in numerous R&D projects undertaken in recent years. In order to do this, descriptions of all DIA modules (and particularly those who have an applied component and less than basic research) are considered and included in this seminar. In this seminar not only are the topics important to teach, but teaching the very development of Artificial Intelligence applications and projects in the area, exceeding the idea of mere exposition of a theoretical lectures without the applied aspect which is essential in Artificial Intelligence and particularly for industrial use.","Art Design Scheme of Modern Urban Commercial Public Space Using Artificial Intelligence TechnologyThis article takes the urban public environment facilities as the research object, and cites the related subject theory as well as the mature research methods at home and abroad, to explore the specific characteristics of the commercial public space. We sum up the main ",35,1,0.799025535583496,1,0.799025535583496
616,"Arts, entertainment and recreation","Intelligent Agents and Multi-Agent Systems",0.79672235250473,"Multiagent systems are systems consisting of several autonomous entities called agents that interact among themselves in order to solve problems that exceed the individual capabilities of each or solving them in a more efficient way. This interaction is the main object of research in multiagent systems, and it has contributed to different disciplines such as Social Science, Game Theory and Artificial Intelligence. In this module, besides studying these contributions, we will introduce the students to the practice of research in any area related to multi-agent systems, and the preparation of papers describing the results of its research activity","Art Design Scheme of Modern Urban Commercial Public Space Using Artificial Intelligence TechnologyThis article takes the urban public environment facilities as the research object, and cites the related subject theory as well as the mature research methods at home and abroad, to explore the specific characteristics of the commercial public space. We sum up the main ",35,4,0.801681563258171,1,0.79672235250473
617,"Arts, entertainment and recreation","Information Retrieval",0.792146682739258,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Art Design Scheme of Modern Urban Commercial Public Space Using Artificial Intelligence TechnologyThis article takes the urban public environment facilities as the research object, and cites the related subject theory as well as the mature research methods at home and abroad, to explore the specific characteristics of the commercial public space. We sum up the main ",35,48,0.813798369218906,1,0.792146682739258
619,"Arts, entertainment and recreation","Research Methodology",0.788376271724701,"This seminar tries to inform and guide the students about techniques, most common standards and systems for the practice of scientific research and its methodological bases and documentaries. The topics are as follows: General Approach (scientific knowledge and its purpose, problems of scientific research, research works); Scientific Work (choice of subject, setting objectives, formulating hypotheses, choice of work method, choice of tools and resources. Phases of work); Information Search (sources, publications, bibliographical searches, access to scientific documentation, internet, etc.); Work Writing (rules, principles, tips, style, language, etc.) and Presentation and Defence of Work (legal aspects, formal aspects, personal aspects, visual aids to support the presentation)","Art Design Scheme of Modern Urban Commercial Public Space Using Artificial Intelligence TechnologyThis article takes the urban public environment facilities as the research object, and cites the related subject theory as well as the mature research methods at home and abroad, to explore the specific characteristics of the commercial public space. We sum up the main ",35,4,0.810593381524086,1,0.788376271724701
620,"Arts, entertainment and recreation","Biomedical Informatics",0.7991823554039,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","[HTML][HTML] Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Artwith Reflections on Present AIM Background: The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970's led to a major change in the paradigm that ",35,18,0.817355437411202,1,0.7991823554039
624,"Arts, entertainment and recreation","Evolutionary Computing",0.784710168838501,"Evolutionary Computing has three objectives:

1) To learn about computational methods based on Darwinian principles of evolution.
2) To illustrate the usage of such methods as problem solvers and as simulation tools.
3) Togain hands-on experience in performing computational experiments with evolutionary algorithms.

The course is treating various algorithms based on the Darwinian evolution theory. Driven by natural selection (survival of the fittest), an evolution process is being emulated and solutions for a given problem are being ""bred"". During this course all ""dialects"" within evolutionary computing are treated (genetic algorithms, evolution strategies, evolutionary programming, genetic programming). Applications in optimisation, constraint handling, machine learning, and robotics are discussed. Specific subjects handled include: various genetic structures (representations), selection
techniques, sexual and asexual variation operators, (self-)adaptivity. Special attention is paid to methodological aspects, such as algorithm design and tuning. If time permits, subjects in Artificial Life will be handled. Hands-on-experience is gained by a compulsory programming assignment.","[HTML][HTML] Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Artwith Reflections on Present AIM Background: The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970's led to a major change in the paradigm that ",35,1,0.784710168838501,1,0.784710168838501
625,"Arts, entertainment and recreation","DD2437 Artificial Neural Networks and Deep Architectures",0.846255421638489,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Machine Learning in Supply Chain Planning--When Art & Science Converge.This article discusses the important AI subset of Machine Learning and its application to the area of supply chain planning and optimization. It defines machine learning and how it relates to other advanced analytic methods including AI; predictive, prescriptive and ",35,33,0.829086807641116,1,0.846255421638489
628,"Arts, entertainment and recreation","Optimization for machine learning",0.837846875190735,"This course teaches an overview of modern optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.This course teaches an overview of modern optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.

Basic Contents:

Convexity, Gradient Methods, Proximal algorithms, Stochastic and Online Variants of mentioned methods, Coordinate Descent Methods, Subgradient Methods, Frank-Wolfe, Accelerated Methods, Primal-Dual context and certificates, Lagrange and Fenchel Duality, Second-Order Methods, Quasi-Newton Methods. Gradient-Free and Zero-Order Optimization.

Advanced Contents:

Parallel and Distributed Optimization Algorithms, Synchronous and Asynchronous Communication.

Lower Bounds.

Non-Convex Optimization: Convergence to Critical Points, Saddle-Point methods, Alternating minimization for matrix and tensor factorizations


An optional, graded, mini-project allows to explore the real-world performance aspects of the algorithms and variants of the course.

Keywords
Optimization, Machine learning

Learning Prerequisites
Recommended courses
CS-433 Machine Learning
Important concepts to start the course
Previous coursework in calculus, linear algebra, and probability is required.
Familiarity with optimization and/or machine learning is useful. 
Learning Outcomes
By the end of the course, the student must be able to:
Assess / Evaluate the most important algorithms, function classes, and algorithm convergence guarantees
Compose existing theoretical analysis with new aspects and algorithm variants.
Formulate scalable and accurate implementations of the most important optimization algorithms for machine learning applications
Characterize trade-offs between time, data and accuracy, for machine learning methods
Transversal skills
Use both general and domain specific IT resources and tools
Summarize an article or a technical report.","Machine Learning in Supply Chain Planning--When Art & Science Converge.This article discusses the important AI subset of Machine Learning and its application to the area of supply chain planning and optimization. It defines machine learning and how it relates to other advanced analytic methods including AI; predictive, prescriptive and ",35,3,0.837565958499908,1,0.837846875190735
629,"Arts, entertainment and recreation","Deep Learning ",0.837289392948151,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Machine Learning in Supply Chain Planning--When Art & Science Converge.This article discusses the important AI subset of Machine Learning and its application to the area of supply chain planning and optimization. It defines machine learning and how it relates to other advanced analytic methods including AI; predictive, prescriptive and ",35,16,0.825420185923576,1,0.837289392948151
630,"Arts, entertainment and recreation","Autonomous Robots",0.826240539550781,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","[PDF][PDF] AIA. Artificial intelligence for artWe observe the success of artificial neural networks in simulating human performance on a number of tasks: such as image recognition, natural language processing, etc. However, there are limits to state of-the-art AI that separate it from human-like intelligence. Humans ",35,10,0.822185301780701,1,0.826240539550781
632,"Arts, entertainment and recreation","Virtual reality",0.800955653190613,"The goal of VR is to embed the users in a potentially complex virtual environment while ensuring that they are able to react as if this environment were real. The course provides a human perception-action background and describes the key techniques for achieving efficient VR applications.
The first lectures focus more on the technical means (hw & sw) for acheiving the hands-on sessions:

- Visual display (CAVE and stereoscopy)
- Interaction devices and sensors
- Software environment
The proportion of more theoretical VR and Neuroscience background increases over the semester:
- Key Human perception abilities, Cybersickness, Immersion, presence and flow
- Basic 3D interaction techniques: Magic vs Naturalism
- The perception of action
- Haptic interaction
- What makes a virtual human looking alive ?
- Motion capture for full-body interaction
- VR, cognitive science and true experimental design

3D interaction, display, sensors, immersion, presence","[PDF][PDF] AIA. Artificial intelligence for artWe observe the success of artificial neural networks in simulating human performance on a number of tasks: such as image recognition, natural language processing, etc. However, there are limits to state of-the-art AI that separate it from human-like intelligence. Humans ",35,1,0.800955653190613,1,0.800955653190613
634,"Arts, entertainment and recreation","Natural Language",0.794825196266174,"• Learn the basic concepts, main formalisms, techniques and algorithms, knowledge bases and corpora, used in the Natural Language Processing area. • Understand the main tasks involved in the processing of a sentence, paragraph or text and understand the main challenges of each one of these tasks. • Learn the main applications and be able to identify the associated technology. • Understand which are the tasks that can be done considering the current state of the art.Course overview (1h)
Introduction to Natural Language Processing (3h 30)
Basic concepts
Ambiguity and linguistic variability
Associated knowledge
Methodology:
Train/test corpus, 
Cross validation, 
Measures (precision, recall, etc.) 
Regular expressions and automata (1.5h)
N-Grams (4.5 h)
N-grams as language models
Markov assumption and probabilities of an N-gram/sentence
Smoothing techniques
Morphology (9)
Morphology and transducers
Part of speech tagging (POS)
Rule-based and stochastic
HMMS and Viterbi algorithm
Syntax (9h)
Grammars
Context-free grammars
Dependency grammars
Probabilistic grammars
Syntactic analysis
Unification-based
Top-down and Bottom-up
Chat-parsers (Earley e CKY)
Probabilistic
Semantic (9h)
Meaning representation
Lexical semantics
Thematic roles
Semantic disambiguation
Semantic analysis
Compositional semantic analysis
Statistic-based semantic analysis
Classifiers and their application in semantic analysis
Applications (remaining classes)
Information extraction (named entity recognition, etc.)
Text classification
Question/answering systems
Dialogue systems
Machine translation
Speech recognition","[PDF][PDF] AIA. Artificial intelligence for artWe observe the success of artificial neural networks in simulating human performance on a number of tasks: such as image recognition, natural language processing, etc. However, there are limits to state of-the-art AI that separate it from human-like intelligence. Humans ",35,1,0.794825196266174,1,0.794825196266174
641,"Electricity, gas, steam and air conditioning supply","Model Driven Engineering",0.819040298461914,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","[PDF][PDF] Machine Learning Based Prediction of Wind Power Electricity Generation from Seasonal Climate Forecasts Institute for Sustainable Economic Development I Johann Baumgartner Status Quo and Aim <U+25AA>
Mainly conceptual models used for this purpose: <U+25AA> High model set up effort incl. bias correction <U+25AA>
Spatially separated interdependencies hard to model <U+25AA> Machine learning models so far only ",105,9,0.815234687593248,1,0.819040298461914
643,"Electricity, gas, steam and air conditioning supply","Learning theory",0.80301558971405,"Machine learning and data analysis are becoming increasingly central in many sciences and applications. This course concentrates on the theoretical underpinnings of machine learning.
Content
Basics : statistical learning framework, Probably Approximately Correct (PAC) learning, learning with a finite number of classes, Vapnik-Chervonenkis (VC) dimension, non-uniform learnability, complexity of learing.
Neural Nets : representation power of neural nets, learning and stability, PAC Bayes bounds.
Graphical model learning.
Non-negative matrix factorization, Tensor decompositions and factorization.
Learning mixture models.By the end of the course, the student must be able to:
Explain the framework of PAC learning
Explain the importance basic concepts such as VC dimension and non-uniform learnability
Describe basic facts about representation of functions by neural networks
Describe recent results on specific topics e.g., graphical model learning, matrix and tensor factorization, learning mixture models","[PDF][PDF] Machine Learning Based Prediction of Wind Power Electricity Generation from Seasonal Climate Forecasts Institute for Sustainable Economic Development I Johann Baumgartner Status Quo and Aim <U+25AA>
Mainly conceptual models used for this purpose: <U+25AA> High model set up effort incl. bias correction <U+25AA>
Spatially separated interdependencies hard to model <U+25AA> Machine learning models so far only ",105,1,0.80301558971405,1,0.80301558971405
647,"Electricity, gas, steam and air conditioning supply","Simulation Methods",0.798019766807556,"Simulation consists on building computer models that describe the essential behavior of a
system of interest and designing and conducting experiments with such models in order
to draw conclusions from their results in order to support decision-making. Typically, it is
used in the analysis of such complex systems, so it is not possible to make an analytical
analysis, or on the basis of a numerical analysis. Nowadays, simulation is a fundamental
experimental methodology in fields as diverse as economics, statistics, computer science,
chemical engineering, ecology and physics, with huge industrial and commercial
applications, ranging from manufacturing systems to flight simulators, through computer
games, stock prediction and weather forecasting.
In the subject we will show multiple applications in Artificial Intelligence, especially in the
discipline of Decision Analysis","Forecasting Residential Electricity Demand Through Machine Learning and Model SynthesisThis paper aims to develop a predictive model of residential electricity demand using techniques from statistical science, data analysis and econometrics. Residential energy intensity is investigated as a critical component of demand and evaluated as a predictor of ",105,6,0.808148324489594,1,0.798019766807556
652,"Electricity, gas, steam and air conditioning supply","Numerical Methods for Partial Differential Equations",0.819888353347778,"Derivation, properties, and implementation of fundamental numerical methods for a few key partial differential equations: convection-diffusion, heat equation, wave equation, conservation laws. Implementation in C++ based on a finite element library.        Main skills to be acquired in this course:
* Ability to implement advanced numerical methods for the solution of partial differential equations efficiently.
* Ability to modify and adapt numerical algorithms guided by awareness of their mathematical foundations.
* Ability to select and assess numerical methods in light of the predictions of theory
* Ability to identify features of a PDE (= partial differential equation) based model that are relevant for the selection and performance of a numerical algorithm.
* Ability to understand research publications on theoretical and practical aspects of numerical methods for partial differential equations.
* Skills in the efficient implementation of finite element methods on unstructured meshes.

This course is neither a course on the mathematical foundations and numerical analysis of methods nor an course that merely teaches recipes and how to apply software packages.1 Case Study: A Two-point Boundary Value Problem
1.1 Introduction
1.2 A model problem
1.3 Variational approach
1.4 Simplified model
1.5 Discretization
1.5.1 Galerkin discretization
1.5.2 Collocation [optional]
1.5.3 Finite differences
1.6 Convergence
2 Second-order Scalar Elliptic Boundary Value Problems
2.1 Equilibrium models
2.1.1 Taut membrane
2.1.2 Electrostatic fields
2.1.3 Quadratic minimization problems
2.2 Sobolev spaces
2.3 Variational formulations
2.4 Equilibrium models: Boundary value problems
3 Finite Element Methods (FEM)
3.1 Galerkin discretization
3.2 Case study: Triangular linear FEM in two dimensions
3.3 Building blocks of general FEM
3.4 Lagrangian FEM
3.4.1 Simplicial Lagrangian FEM
3.4.2 Tensor-product Lagrangian FEM
3.5 Implementation of FEM in C++
3.5.1 Mesh file format (Gmsh)
3.5.2 Mesh data structures (DUNE)
3.5.3 Assembly
3.5.4 Local computations and quadrature
3.5.5 Incorporation of essential boundary conditions
3.6 Parametric finite elements
3.6.1 Affine equivalence
3.6.2 Example: Quadrilaterial Lagrangian finite elements
3.6.3 Transformation techniques
3.6.4 Boundary approximation
3.7 Linearization [optional]
4 Finite Differences (FD) and Finite Volume Methods (FV) [optional]
4.1 Finite differences
4.2 Finite volume methods (FVM)
5 Convergence and Accuracy
5.1 Galerkin error estimates
5.2 Empirical Convergence of FEM
5.3 Finite element error estimates
5.4 Elliptic regularity theory
5.5 Variational crimes
5.6 Duality techniques [optional]
5.7 Discrete maximum principle [optional]
6 2nd-Order Linear Evolution Problems
6.1 Parabolic initial-boundary value problems
6.1.1 Heat equation
6.1.2 Spatial variational formulation
6.1.3 Method of lines
6.1.4 Timestepping
6.1.5 Convergence
6.2 Wave equations [optional]
6.2.1 Vibrating membrane
6.2.2 Wave propagation
6.2.3 Method of lines
6.2.4 Timestepping
6.2.5 CFL-condition
7 Convection-Diffusion Problems
7.1 Heat conduction in a fluid
7.1.1 Modelling fluid flow
7.1.2 Heat convection and diffusion
7.1.3 Incompressible fluids
7.1.4 Transient heat conduction
7.2 Stationary convection-diffusion problems
7.2.1 Singular perturbation
7.2.2 Upwinding
7.3 Transient convection-diffusion BVP
7.3.1 Method of lines
7.3.2 Transport equation
7.3.3 Lagrangian split-step method
7.3.4 Semi-Lagrangian method
8 Numerical Methods for Conservation Laws
8.1 Conservation laws: Examples
8.2 Scalar conservation laws in 1D
8.3 Conservative finite volume discretization
8.3.1 Semi-discrete conservation form
8.3.2 Discrete conservation property
8.3.3 Numerical flux functions
8.3.4 Montone schemes
8.4 Timestepping
8.4.1 Linear stability
8.4.2 CFL-condition
8.4.3 Convergence
8.5 Higher order conservative schemes [optional]
8.5.1 Slope limiting
8.5.2 MUSCL scheme
8.6. FV-schemes for systems of conservation laws [optional]","Clustering Electricity Big Data for Consumption Modeling Using Comparative Strainer Method for High Accuracy Attainment and Dimensionality ReductionIn smart grid, the relation between grid and customer is bidirectional. Therefore, analyzing load consumption patterns is essential for optimal and efficient operation and planning of smart grid in addition to precise load forecasting. However, emergence of the advanced ",105,1,0.819888353347778,1,0.819888353347778
657,"Electricity, gas, steam and air conditioning supply","Big Data, Law, and Policy",0.808654129505157,"        This course introduces students to societal perspectives on the big data revolution. Discussing important contributions from machine learning and data science, the course explores their legal, economic, ethical, and political implications in the past, present, and future.        This course is intended both for students of machine learning and data science who want to reflect on the societal implications of their field, and for students from other disciplines who want to explore the societal impact of data sciences. The course will first discuss some of the methodological foundations of machine learning, followed by a discussion of research papers and real-world applications where big data and societal values may clash. Potential topics include the implications of big data for privacy, liability, insurance, health systems, voting, and democratic institutions, as well as the use of predictive algorithms for price discrimination and the criminal justice system. Guest speakers, weekly readings and reaction papers ensure a lively debate among participants from various backgrounds.","Exploring big data for development: An electricity sector case study from IndiaThis paper presents exploratory research into data-intensive development that seeks to inductively identify issues and conceptual frameworks of relevance to big data in developing countries. It presents a case study of big data innovations in Stelcorp; a state electricity  ",105,6,0.817278911670049,1,0.808654129505157
659,"Electricity, gas, steam and air conditioning supply","Decision Support Models",0.79306960105896,"At the completion of the course, the student will: be familiar with distinct decision-making strategies and traps in the evaluation of options and in the allocation of resources in private and public contexts; be familiar with key theoretical and methodological concepts of decision-making and decision aid relevant for the best practice of decision engineering; be familiar with models, processes and tools for helping to structure and explore decisions characterized by multiple objectives, uncertainty, complexity and differences of opinion; be familiar with examples of real-world decision analysis and decision conferencing applications in organizations; be familiar with other topics considered relevant for engineering decisions, covering problem structuring methods, heuristics and biases and group decision and negotiation; have developed skills in decision analysis and modeling; • be able to select and use specialized decision support software in different decision contexts.The decision making problematic: Definition of the decision problem. Importance of decision making in
engineering and management. Characteristics of the decision context.
Decision making strategies. Uncertainty and complexity. Value and risk.
What is Decision Analysis (DA)? DA objectives. The seven fundamental steps of DA. DA schools of thought
and theoretical foundations. The problem of decision aiding.
Intervention strategies: From optimization to the learning paradigm. Value and utility analysis. Decision
conference and facilitation.
Concepts, models, techniques and software for decision support:
1. Decision trees and influence diagrams; case studies; PRECISION TREE.
2. Bayesian networks; case studies; NETICA.
3. Probabilities modeling and risk analysis; case studies; @RISK.
4. Cognitive mapping; case studies; DECISION EXPLORER.
5. Multiple criteria evaluation models; case studies; MACBETH.
6. Resource allocation and negotiation; case studies; PROBE and MACBETH.Teaching is mostly organized by groups of models, techniques and software for decision support that can
assist different types of decision problems. For each type of decision problem, teaching is based on the
presentation of methods, models and techniques to assist decision-makers, followed by a discussion of
real world case studies and of key methodological aspects, and on the use of decision support tools. For
some topics students also carry out practical exercises.
Evaluation is done through two groupwork assignments and one individual exam. In one groupwork
students structure problems characterized by uncertainty, build models and implement them in appropriate
software; in another groupwork students build a multicriteria evaluation model to assist a decision-maker
in a real problem.","Exploring big data for development: An electricity sector case study from IndiaThis paper presents exploratory research into data-intensive development that seeks to inductively identify issues and conceptual frameworks of relevance to big data in developing countries. It presents a case study of big data innovations in Stelcorp; a state electricity  ",105,6,0.812497705221176,1,0.79306960105896
665,"Electricity, gas, steam and air conditioning supply","Database Systems",0.846907258033752,"You have insight in the inner workings of database systems
You know how data is internally represented and how queries are efficiently evaluated
You are familiar with various algorithms that make the querying of large amounts of data feasible
You understand why certain design choices are made in the development of database systems and how these choices affect the performance of database systems. In today's digital society, database systems play an important role. Such systems ensure that large quantities of data can be efficiently manipulated and queried. In this course we look at the inner workings of such systems. Efficient algorithms for storing, indexing and querying of data form the backbone of this course. More specifically, the following topics will be considered in the database systems course:

The relational model and SQL
Storage en Indexing
Hash-based indexing (Linear, Extendible)
Tree-based indexing (B+tree, ISAM)
External Sorting
External merge sort
B+ tree sorting
Buffer management
Buffer replacement policies
I/O
Query evaluation
Query optimization
Join algorithms
Cardinality estimation
Query plans
Transaction Management
ACID
Serializability
Two phase locking
Concurrency control
Crash Recovery
Checkpoints
ARIES
","Realization and Research of Intelligent system of client electricity information based on the Big Data Processing TechnologyBig data is the focus in power system currently. In order to analyze and classify the electricity model of clients, identify the avoiding peak space intelligently, extract value-added information of clients and control the electric load actively, it is extremely necessary to ",105,2,0.844314962625504,1,0.846907258033752
672,"Electricity, gas, steam and air conditioning supply","Statistical Modelling of Spatial Data",0.82647043466568,"In environmental sciences one often deals with spatial data. When analysing such data the focus is either on exploring their structure (dependence on explanatory variables, autocorrelation) and/or on spatial prediction. The course provides an introduction to geostatistical methods that are useful for such analyses.        The course will provide an overview of the basic concepts and stochastic models that are used to model spatial data. In addition, participants will learn a number of geostatistical techniques and acquire familiarity with R software that is useful for analyzing spatial data.        After an introductory discussion of the types of problems and the kind of data that arise in environmental research, an introduction into linear geostatistics (models: stationary and intrinsic random processes, modelling large-scale spatial patterns by linear regression, modelling autocorrelation by variogram; kriging: mean square prediction of spatial data) will be taught. The lectures will be complemented by data analyses that the participants have to do themselves.","Revealing Household Characteristics from Electricity Meter Data with Grade Analysis and Machine Learning AlgorithmsIn this article, the Grade Correspondence Analysis (GCA) with posterior clustering and visualization is introduced and applied to extract important features to reveal households' characteristics based on electricity usage data. The main goal of the analysis is to ",105,5,0.829264938831329,1,0.82647043466568
674,"Electricity, gas, steam and air conditioning supply","Applied biostatistics",0.816233038902283,"This course covers topics in applied biostatistics, with an emphasis on practical aspects of data analysis using R statistical software. Topics include types of studies and their design and analysis, high dimensional data analysis (genetic/genomic) and other topics as time and interest permit.Types of studies
Design and analysis of studies
R statistical software
Reproducible research techniques and tools
Report writing
Exploratory data analysis
Liniear modeling (regression, anova)
Generalized linear modeling (logistic, Poission)
Survival analysis
Discrete data analysis
Meta-analysis
High dimensional data analysis (genetics/genomics applications)
Additional topics as time and interest permit
Keywords
Data analysis, reproducible research, statistical methods, R, biostatistical data analysis, statistical data analysis. By the end of the course, the student must be able to:
Interpret analysis results
Justify analysis plan
Plan analysis for a given dataset
Analyze various types of biostatistical data
Synthesize analysis into a written report
Report plan of analysis and results obtained
Transversal skills
Write a scientific or technical report.
Assess one's own level of skill acquisition, and plan their on-going learning goals.
Take feedback (critique) and respond in an appropriate manner.
Use a work methodology appropriate to the task.","Revealing Household Characteristics from Electricity Meter Data with Grade Analysis and Machine Learning AlgorithmsIn this article, the Grade Correspondence Analysis (GCA) with posterior clustering and visualization is introduced and applied to extract important features to reveal households' characteristics based on electricity usage data. The main goal of the analysis is to ",105,4,0.835054457187653,1,0.816233038902283
687,"Electricity, gas, steam and air conditioning supply","Data Analysis and Integration",0.843947470188141,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Design of big data analytics electricity collecting data analysis and intelligent monitoring systemsIn order to strengthen the power of users better characteristics, measurement device and distribution network equipment condition monitoring and analysis, based on electricity acquisition data analysis and intelligent monitoring system based on the integration of ",105,20,0.825102084875107,1,0.843947470188141
712,"Electricity, gas, steam and air conditioning supply","Autonomous Robots",0.788855850696564,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","Development of Automation Algorithm for Step of Designing Technology of Static Electricity Protection ClothingThe article presents the research results, which aim at providing an automation algorithm for the step of the designing technology of static electricity protection clothing. Designing the protective clothing is intended to the creation of such a structure, the properties of which ",105,10,0.822185301780701,1,0.788855850696564
713,"Electricity, gas, steam and air conditioning supply","Probabilistic Artificial Intelligence",0.78547203540802,"This course introduces core modeling techniques and algorithms from statistics, optimization, planning, and control and study applications in areas such as sensor networks, robotics, and the Internet.        How can we build systems that perform well in uncertain environments and unforeseen situations? How can we develop systems that exhibit ""intelligent"" behavior, without prescribing explicit rules? How can we build systems that learn from experience in order to improve their performance? We will study core modeling techniques and algorithms from statistics, optimization, planning, and control and study applications in areas such as sensor networks, robotics, and the Internet. The course is designed for upper-level undergraduate and graduate students.        Topics covered:
- Search (BFS, DFS, A*), constraint satisfaction and optimization
- Tutorial in logic (propositional, first-order)
- Probability
- Bayesian Networks (models, exact and approximative inference, learning) - Temporal models (Hidden Markov Models, Dynamic Bayesian Networks)
- Probabilistic palnning (MDPs, POMPDPs)
- Reinforcement learning
- Combining logic and probability","Development of Automation Algorithm for Step of Designing Technology of Static Electricity Protection ClothingThe article presents the research results, which aim at providing an automation algorithm for the step of the designing technology of static electricity protection clothing. Designing the protective clothing is intended to the creation of such a structure, the properties of which ",105,3,0.82557342449824,1,0.78547203540802
714,"Electricity, gas, steam and air conditioning supply","Energy-Efficient Parallel Computing Systems for Data Analytics",0.783847868442535,"Advanced Parallel Computing Architectures and related design issues. It will cover multi-cores, many-cores, vector engines, GP-GPUs, application-specific processors and heterogeneous compute accelerators. Focus on integrated architectures for data analytics applications. Special emphasis given to energy-efficiency issues and hardware-software design for power and energy minimizazion.        Give in-depth understanding of the links and dependencies between architectures and their energy-efficient implementation and to get a comprehensive exposure to state-of-the-art computing platforma for data anlytics applications. Practical experience will also be gained through practical exercises and mini-projects (hardware and software) assigned on specific topics.        The course will cover advanced parallel computing architectures architectures, with an in-depth view on design challenges related to advanced silicon technology and state-of-the-art system integration options (nanometer silicon technology, novel storage devices, three-dimensional integration, advanced system packaging). The emphasis will be on programmable parallel architectures, namely, multi and many- cores, GPUs, vector accelerators, application-specific processors, heterogeneous platforms, and the complex design choices required to achieve scalability and energy proportionality. The course will will also delve into system design, touching on hardware-software tradeoffs and full-system analysis and optimization taking into account non-functional constraints and quality metrics, such as power consumption, thermal dissipation, reliability and variability. The application focus will be on emerging data analytics both in the cloud at at the edges (near-sensor analytics).","Development of Automation Algorithm for Step of Designing Technology of Static Electricity Protection ClothingThe article presents the research results, which aim at providing an automation algorithm for the step of the designing technology of static electricity protection clothing. Designing the protective clothing is intended to the creation of such a structure, the properties of which ",105,2,0.806568711996078,1,0.783847868442535
715,"Electricity, gas, steam and air conditioning supply","Distributed Artificial Intelligence",0.8343124,"The course aims to reach the following objectives
you are knowledgeable about the characteristics of distributed computing and middleware and their implications for application development
you have an understanding of distributed application architectures in its various paradigms
you can apply this knowledge in a project realizing a distributed application
. This course studies the concepts and theoretical background of existing technologies for building distributed systems. The focus is on the design of aspect of distributed systems, which includes distributed design patterns, distributed technologies, etc. 

The course can be split in a theoretical and more applied part. In the theoretical part, a number of distributed design principles will be studied, together with the algorithms that are needed to achieve them. These include:

Algorithms for distributed transactions
Algorithms for tackling concurrency
Algorithms for avoiding distributed deadlocks
Achieving security in distributed systems
An introduction to cloud computing
Moreover, the applied part of this course will study the following distributed technologies:

Web-based distributed systems based on REST, JSON, etc. 
Web Services
Java-based distributed applications: EJB3, including its multi-tier architecture allowing it to couple it with RESTful Web Services. 
Front-end technology
Android-based mobile application development for distributed systems. 
The studied technologies present a gradual approach in which we increase the complexity and functionality of the particular technology. As such their applicability range from individual web sites to enterprise applications. ","[BOOK][B] Control and Automation Systems for Electricity Distribution Networks (EDN) of the futureThe CIGRÉ C6 Study Committee (Distribution Systems and Dispersed Generation) considers the different aspects of integration of distributed generation. In this context, the JWG C6. 25/B5 has worked to map current functionalities and to identify future needs for the ",105,1,0.8343124,1,0.8343124
718,"Electricity, gas, steam and air conditioning supply","Principles of Distributed Computing",0.8144637,"We study the fundamental issues underlying the design of distributed systems: communication, coordination, fault-tolerance, locality, parallelism, self-organization, symmetry breaking, synchronization, uncertainty. We explore essential algorithmic ideas and lower bound techniques.        Distributed computing is essential in modern computing and communications systems. Examples are on the one hand large-scale networks such as the Internet, and on the other hand multiprocessors such as your new multi-core laptop. This course introduces the principles of distributed computing, emphasizing the fundamental issues underlying the design of distributed systems and networks: communication, coordination, fault-tolerance, locality, parallelism, self-organization, symmetry breaking, synchronization, uncertainty. We explore essential algorithmic ideas and lower bound techniques, basically the ""pearls"" of distributed computing. We will cover a fresh topic every week.        Distributed computing models and paradigms, e.g. message passing, shared memory, synchronous vs. asynchronous systems, time and message complexity, peer-to-peer systems, small-world networks, social networks, sorting networks, wireless communication, and self-organizing systems.

Distributed algorithms, e.g. leader election, coloring, covering, packing, decomposition, spanning trees, mutual exclusion, store and collect, arrow, ivy, synchronizers, diameter, all-pairs-shortest-path, wake-up, and lower bounds","[BOOK][B] Control and Automation Systems for Electricity Distribution Networks (EDN) of the futureThe CIGRÉ C6 Study Committee (Distribution Systems and Dispersed Generation) considers the different aspects of integration of distributed generation. In this context, the JWG C6. 25/B5 has worked to map current functionalities and to identify future needs for the ",105,1,0.8144637,1,0.8144637
732,"Electricity, gas, steam and air conditioning supply","Program Analysis for System Security and Reliability",0.8034374,"The course introduces modern analysis and synthesis techniques (both, deterministic and probabilistic) and shows how to apply these methods to build reliable and secure systems spanning the domains of blockchain, computer networks and deep learning.        * Understand how classic analysis and synthesis techniques work, including discrete and probabilistic methods.

* Understand how to apply the methods to generate attacks and create defenses against applications in blockchain, computer networks and deep learning.

* Understand the state-of-the-art in the area as well as future trends.        The course will illustrate how the methods can be used to create more secure and reliable systems across four application domains:

Part I: Analysis and Synthesis for Computer Networks:
1. Analysis: Datalog, Batfish
2. Synthesis: CEGIS, SyNET (http://synet.ethz.ch)
3. Probabilistic: (PSI: http://psisolver.org), its applications to networks (Bayonet)

Part II: Blockchain security
1. Introduction to space and tools.
2. Automated symbolic reasoning.
3. Applications: verification of smart contracts (http://www.securify.ch)

Part III: Security and Robustness of Deep Learning:
1. Basics: affine transforms, activation functions
2. Attacks: gradient based method to adversarial generation
3. Defenses: affine domains, AI2 (http://ai2.ethz.ch)

Part IV: Probabilistic Security:
1. Enforcement: PSI + Spire.
2. Graphical models: CRFs, Structured SVM, Pseudo-likelihood.
3. Practical statistical de-obfuscation: DeGuard: http://apk-deguard.com, JSNice: http://jsnice.org, and more.

To gain a deeper understanding, the course will involve a hands-on programming project."," In many developed countries, such as America, England, Japan and so on those used the
technology of Artificial Intelligence (AI), such as Neural Network, Data Mining, Machine Learning
and so on to apply to use in electricity energy forecasting for gain the best performance  
Study of electricity load forecasting based on multiple kernels learning and weighted support vector regression machine",105,2,0.8134726,1,0.8034374
737,"Electricity, gas, steam and air conditioning supply","Machine Perception",0.7822116,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Machine learning methods for the analysis of data of an Electricity Distribution Network OperatorOnce every few decades an invention changes the landscape of some aspects of our life. Industrial revolutions improved our everyday lives whilst medical revolutions expanded our lifespans. In the path we're leading, most of sciences will be reduced to computer science ",105,24,0.819487,1,0.7822116
739,"Electricity, gas, steam and air conditioning supply","Hardware Architectures for Machine Learning",0.7812752,"The seminar covers recent results in the increasingly important field of hardware acceleration for data science and machine learning, both in dedicated machines or in data centers.        The seminar aims at students interested in the system aspects of machine learning, who are willing to bridge the gap across traditional disciplines: machine learning, databases, systems, and computer architecture.        The seminar is intended to cover recent results in the increasingly important field of hardware acceleration for data science and machine learning, both in dedicated machines or in data centers.","Machine learning methods for the analysis of data of an Electricity Distribution Network OperatorOnce every few decades an invention changes the landscape of some aspects of our life. Industrial revolutions improved our everyday lives whilst medical revolutions expanded our lifespans. In the path we're leading, most of sciences will be reduced to computer science ",105,4,0.8157783,1,0.7812752
744,"Financial service activities, except insurance and pension funding","Simulation Methods",0.8241099,"Simulation consists on building computer models that describe the essential behavior of a
system of interest and designing and conducting experiments with such models in order
to draw conclusions from their results in order to support decision-making. Typically, it is
used in the analysis of such complex systems, so it is not possible to make an analytical
analysis, or on the basis of a numerical analysis. Nowadays, simulation is a fundamental
experimental methodology in fields as diverse as economics, statistics, computer science,
chemical engineering, ecology and physics, with huge industrial and commercial
applications, ranging from manufacturing systems to flight simulators, through computer
games, stock prediction and weather forecasting.
In the subject we will show multiple applications in Artificial Intelligence, especially in the
discipline of Decision Analysis","Machine learning in finance: A topic modeling approachWe provide a first comprehensive structuring of the literature applying machine learning to finance. We use a probabilistic topic modeling approach to make sense of this diverse body of research spanning across the disciplines of finance, economics, computer sciences, and ",135,6,0.8081483,1,0.8241099
746,"Financial service activities, except insurance and pension funding","Software security",0.8022401,"This course focuses on software security fundamentals, secure coding guidelines and principles, and advanced software security concepts. Students learn to assess and understand threats, learn how to design and implement secure software systems, and get hands-on experience with security pitfalls.

This course focuses on software security fundamentals, secure coding guidelines and principles, and advanced software security concepts. Students will learn to assess and understand threats, learn how to design and implement secure software systems, and get hands-on experience with common security pitfalls.

Software running on current systems is exploited by attackers despite many deployed defence mechanisms and best practices for developing new software. In this course students will learn about current security threats, attack vectors, and defence mechanisms on current systems. The students will work with real world problems and technical challenges of security mechanisms (both in the design and implementation of programming languages, compilers, and runtime systems).

 

Secure software lifecycle: design, implementation, testing, and deployment

Basic software security principles

Reverse engineering : understanding code

Security policies: Memory and Type safety

Software bugs and undefined behavior

Attack vectors: from flaw to compromise

Runtime defense: mitigations

Software testing: fuzzing and sanitization

Focus topic : Web security

Focus topic : Mobile security

 


Software security, mitigation, software testing, sanitization, fuzzing. By the end of the course, the student must be able to:
Explain the top 20 most common weaknesses in software security and understand how such problems can be avoided in software.
Identify common security threats, risks, and attack vectors for software systems.
Assess / Evaluate current security best practices and defense mechanisms for current software systems. Become aware of limitations of existing defense mechanisms and how to avoid them.
Identify security problems in source code and binaries, assess the associated risks, and reason about their severity and exploitability.
Assess / Evaluate the security of given source code or applications.
Transversal skills
Identify the different roles that are involved in well-functioning teams and assume different roles, including leadership roles.
Keep appropriate documentation for group meetings.
Summarize an article or a technical report.
Access and evaluate appropriate sources of information.
Write a scientific or technical report.
Make an oral presentation.","Expert Systems in Finance: Smart Financial Applications in Big Data EnvironmentsThroughout the industry, financial institutions seek to eliminate cumbersome authentication methods, such as PINs, passwords, and security questions, as these antiquated tactics prove increasingly weak. Thus, many organizations now aim to implement emerging ",135,1,0.8022401,1,0.8022401
747,"Financial service activities, except insurance and pension funding","Computer and network security",0.7997158,"Understand and use cryptographic algorithms
Understand and identify security flaws in cryptographic algorithms
Design and develop secure (distributed) software applications
Configure and deploy secure computer network protocols and policies. This course provides a general introduction to all aspects related to security of IT systems, software, and computer networks. It aims to familiarize students with how to detect security flaws in software and systems, how to fix them, and most importantly how to avoid them when implementing their own software or deploying their own systems and networks. During the lectures, students will be provided with the theoretical basis and knowledge required to understand and implement secure solutions. The complementary lab sessions will serve to apply the acquired theoretical knowledge in practice. Several guest lectures are organised to provide a link with industry. Concretely, the following topics will be addressed:

Encryption
Symmetric encryption and confidentiality
Public-key encryption
Message authentication codes and cryptographic hashing algorithms
Algorithms (DES, AES, RSA, SHA, …)
Network security
Key distribution and user authentication
Transport-level encryption (HTTPS, SSH, SSL, TLS)
Wireless network security (WES, WPA)
Email security (PGP, S/MIME)
IP security (IPSec, Internet Key Exchange)
System security
Software attacks (SQL injection, zero-day attacks, etc.)
Viruses and malware
Firewalls","Expert Systems in Finance: Smart Financial Applications in Big Data EnvironmentsThroughout the industry, financial institutions seek to eliminate cumbersome authentication methods, such as PINs, passwords, and security questions, as these antiquated tactics prove increasingly weak. Thus, many organizations now aim to implement emerging ",135,2,0.7919443,1,0.7997158
749,"Financial service activities, except insurance and pension funding","System Security",0.7904413,"        The first part of the lecture covers individual system aspects starting with tamperproof or tamper-resistant hardware in general over operating system related security mechanisms to application software systems, such as host based intrusion detection systems. In the second part, the focus is on system design and methodologies for building secure systems.        In this lecture, students learn about the security requirements and capabilities that are expected from modern hardware, operating systems, and other software environments. An overview of available technologies, algorithms and standards is given, with which these requirements can be met.        The first part of the lecture covers individual system's aspects starting with tamperproof or tamperresistant hardware in general over operating system related security mechanisms to application software systems such as host based intrusion detetction systems. The main topics covered are: tamper resistant hardware, CPU support for security, protection mechanisms in the kernel, file system security (permissions / ACLs / network filesystem issues), IPC Security, mechanisms in more modern OS, such as Capabilities and Zones, Libraries and Software tools for security assurance, etc.

In the second part, the focus is on system design and methodologies for building secure systems. Topics include: patch management, common software faults (buffer overflows, etc.), writing secure software (design, architecture, QA, testing), compiler-supported security, language-supported security, logging and auditing (BSM audit, dtrace, ...), cryptographic support, and trustworthy computing (TCG, SGX).

Along the lectures, model cases will be elaborated and evaluated in the exercises.","Expert Systems in Finance: Smart Financial Applications in Big Data EnvironmentsThroughout the industry, financial institutions seek to eliminate cumbersome authentication methods, such as PINs, passwords, and security questions, as these antiquated tactics prove increasingly weak. Thus, many organizations now aim to implement emerging ",135,3,0.8086568,1,0.7904413
754,"Financial service activities, except insurance and pension funding","Machine Perception",0.8160347,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Machine learning for quantitative finance: fast derivative pricing, hedging and fittingIn this paper, we show how we can deploy machine learning techniques in the context of traditional quant problems. We illustrate that for many classical problems, we can arrive at speed-ups of several orders of magnitude by deploying machine learning techniques based ",135,24,0.819487,1,0.8160347
759,"Financial service activities, except insurance and pension funding","Network Science",0.7893671,"This course provides an introduction to the study of complex networks, including algorithms, models and applications to both artificial and real networks, including social, biological and technological networks, all sharing common features and properties. The course addresses the development of scalable algorithms and data structures so that we can efficiently study large complex networks, but also in the creation of theoretical models capable of describing empirically observed patterns. The number of applications is enormous, including web search engines, evolutionary dynamics, information diffusion on Internet, social networks and blogs, network resilience, network-driven phenomena in epidemiology and computer viruses, networks dynamics, with connections in the social sciences, physics, computational biology, and economics.Introduction to complex systems and networks science: Theory and basic concepts. Properties and characterization of biological, social and technological networks. Network models and random graphs. Efficient representation of large (sparse) networks. Succinct data-structures and coding strategies. Design and analysis of efficient and scalable algorithms for large network processing and analysis, including both sampling and randomization techniques. Databases and distributed platforms for the analysis of large networks. Link analysis and random walks. Community finding and graph partitioning. Ranking algorithms. Vertex relabeling. Dynamical processes on complex networks: The impact of network structure on economic, social and biological systems. Introduction to stochastic processes, Monte-Carlo simulations and large-scale multi-agent systems. Disease spreading and tolerance to attacks. Models of peer-influence and opinion formation. Game theory and population dynamics. Public goods problems, cooperation and reputation dynamics. Decision-making on (static and adaptive) interaction networks.","How Do the Global Stock Markets Influence One Another? Evidence from Finance Big Data and Granger Causality Directed NetworkThe recent financial network analysis approach reveals that the topologies of financial markets have an important influence on market dynamics. However, the majority of existing Finance Big Data networks are built as undirected networks without information on the ",135,5,0.8086251,1,0.7893671
767,"Financial service activities, except insurance and pension funding","Statistical Modelling of Spatial Data",0.8250631,"In environmental sciences one often deals with spatial data. When analysing such data the focus is either on exploring their structure (dependence on explanatory variables, autocorrelation) and/or on spatial prediction. The course provides an introduction to geostatistical methods that are useful for such analyses.        The course will provide an overview of the basic concepts and stochastic models that are used to model spatial data. In addition, participants will learn a number of geostatistical techniques and acquire familiarity with R software that is useful for analyzing spatial data.        After an introductory discussion of the types of problems and the kind of data that arise in environmental research, an introduction into linear geostatistics (models: stationary and intrinsic random processes, modelling large-scale spatial patterns by linear regression, modelling autocorrelation by variogram; kriging: mean square prediction of spatial data) will be taught. The lectures will be complemented by data analyses that the participants have to do themselves.","QuantCloud: big data infrastructure for quantitative finance on the cloudIn this paper, we present the QuantCloud infrastructure, designed for performing big data analytics in modern quantitative finance. Through analyzing market observations, quantitative finance (QF) utilizes mathematical models to search for subtle patterns and ",135,5,0.8292649,1,0.8250631
779,"Financial service activities, except insurance and pension funding","Data Analysis and Integration",0.8286594,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Big Data Analytics and Visualization: FinanceAll finance institutions have seen an explosion in their velocity, variety and volume of their internal 
datasets. New federal regulations requirement require leveraging internal and external data 
linking: [1] Customer service and transactional level data; [2] Social Media activity analysis (Sentimental ",135,20,0.8251021,1,0.8286594
790,"Financial service activities, except insurance and pension funding","Autonomous Robots",0.8510112,"The main aim of robotics is to build intelligent machines that are able to perceive and even model the state of the dynamic environment in which they operate and act with reference to that information. This is how we define the basic control loop that raises a number of challenges to disciplines such as Electronics, Mechanics, Applied Mathematics and, especially, Computer Science, in particular, Artificial Intelligence. In the module, we will study and apply several methods of control, coordination and communication of autonomous mobile robots that use specific tools as a base together with techniques of Artificial Intelligence. These can be summarised as methods based on artificial neural networks, evolutionary techniques and genetic algorithms, fuzzy logic, reinforcement learning, and paradigms of coordination models that use multi-agent systems. As a final aim, we study and provide solutions for mobile robots with wheels, articulated, modular, aerial, and also for multi-robot systems consisting of teams of robots with the previously listed characteristics.","[PDF][PDF] Machine Learning Projection Methods for Macro-Finance ModelsThis paper develops a global solution method to solve large state space macro-finance models using machine learning. Our new method, an artificial neural network expectation algorithm, is not only considerably faster but also as precise and more scalable than the ",135,10,0.8221853,1,0.8510112
791,"Financial service activities, except insurance and pension funding","Optimization for machine learning",0.8475568,"This course teaches an overview of modern optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.This course teaches an overview of modern optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.

Basic Contents:

Convexity, Gradient Methods, Proximal algorithms, Stochastic and Online Variants of mentioned methods, Coordinate Descent Methods, Subgradient Methods, Frank-Wolfe, Accelerated Methods, Primal-Dual context and certificates, Lagrange and Fenchel Duality, Second-Order Methods, Quasi-Newton Methods. Gradient-Free and Zero-Order Optimization.

Advanced Contents:

Parallel and Distributed Optimization Algorithms, Synchronous and Asynchronous Communication.

Lower Bounds.

Non-Convex Optimization: Convergence to Critical Points, Saddle-Point methods, Alternating minimization for matrix and tensor factorizations


An optional, graded, mini-project allows to explore the real-world performance aspects of the algorithms and variants of the course.

Keywords
Optimization, Machine learning

Learning Prerequisites
Recommended courses
CS-433 Machine Learning
Important concepts to start the course
Previous coursework in calculus, linear algebra, and probability is required.
Familiarity with optimization and/or machine learning is useful. 
Learning Outcomes
By the end of the course, the student must be able to:
Assess / Evaluate the most important algorithms, function classes, and algorithm convergence guarantees
Compose existing theoretical analysis with new aspects and algorithm variants.
Formulate scalable and accurate implementations of the most important optimization algorithms for machine learning applications
Characterize trade-offs between time, data and accuracy, for machine learning methods
Transversal skills
Use both general and domain specific IT resources and tools
Summarize an article or a technical report.","[PDF][PDF] Machine Learning Projection Methods for Macro-Finance ModelsThis paper develops a global solution method to solve large state space macro-finance models using machine learning. Our new method, an artificial neural network expectation algorithm, is not only considerably faster but also as precise and more scalable than the ",135,3,0.837566,1,0.8475568
795,"Financial service activities, except insurance and pension funding","Language Engineering",0.8211617,"Language Engineering (LE) is the set of techniques, resources and tools to solve problems by using more or less an automated language. This course aims to introduce students to the overall framework, which is currently the LE. The second part of the subject will explain the two main principles of most language treatment systems, such as the content representation models and the creation and maintenance of lexical resources, both pillars of any system and any use. In the third part of the course the student will be introduced three of the major commercial applications of LE, such as information retrieval (associated with the search for data or items of information in a text) and text mining, where besides extracting data type information, we will extract relationships between them. The existing application on the market, and the more immediate trends (for example the analysis of forums for opinions) will also be discussed and explained.","Application of information systems aimed at big data use in the sphere of state finance management: Concept schemeThe article is devoted to the application of big data technologies in public administration, in particular, in public financial management. The paper outlines the basic principles of effective public financial management and describes the development trends of state ",135,6,0.8069792,1,0.8211617
796,"Financial service activities, except insurance and pension funding","Web Science",0.8209098,"Web Science studies the phenomena related to the analysis and design of sociotechnical systems. Sociology plays an important role in the design of the web of the future. This course introduces the principles of web science. The design systems used in web science are presented, including information retrieval mechanisms, recommender systems and sentiment analysis systems. Then, the terms Social Computing and Citizen Science are defined, paying special attention to artificial societies and trust and reputation mechanisms. Finally, social decision-making mechanisms based on preference aggregation are revised.","Application of information systems aimed at big data use in the sphere of state finance management: Concept schemeThe article is devoted to the application of big data technologies in public administration, in particular, in public financial management. The paper outlines the basic principles of effective public financial management and describes the development trends of state ",135,7,0.8131841,1,0.8209098
815,"Financial service activities, except insurance and pension funding","Statistical Learning Theory",0.8641194,"The course covers advanced methods of statistical learning :Statistical learning theory;variational methods and optimization, e.g., maximum entropy techniques, information bottleneck, deterministic and simulated annealing; clustering for vectorial, histogram and relational data; model selection; graphical models.The course surveys recent methods of statistical learning. The fundamentals of machine learning as presented in the course ""Introduction to Machine Learning"" are expanded and in particular, the theory of statistical learning is discussed.        # Theory of estimators: How can we measure the quality of a statistical estimator? We already discussed bias and variance of estimators very briefly, but the interesting part is yet to come.

# Variational methods and optimization: We consider optimization approaches for problems where the optimizer is a probability distribution. Concepts we will discuss in this context include:

* Maximum Entropy
* Information Bottleneck
* Deterministic Annealing

# Clustering: The problem of sorting data into groups without using training samples. This requires a definition of ``similarity'' between data points and adequate optimization procedures.

# Model selection: We have already discussed how to fit a model to a data set in ML I, which usually involved adjusting model parameters for a given type of model. Model selection refers to the question of how complex the chosen model should be. As we already know, simple and complex models both have advantages and drawbacks alike.

# Statistical physics models: approaches for large systems approximate optimization, which originate in the statistical physics (free energy minimization applied to spin glasses and other models); sampling methods based on these models","The Future of Fuzzy Sets in Finance: New Challenges in Machine Learning and Explainable AITraditional statistical analysis is oriented towards finding linear relationships between the variables under investigation, often accompanied by strict assumptions about the problem and data distributions. Moreover, traditional analysis endorses data reduction as much as ",135,8,0.8403083,1,0.8641194
817,"Financial service activities, except insurance and pension funding","Statistical theory",0.8482988,"The course aims to develop certain key aspects of the theory of statistics, providing a common general framework for statistical methodology. While the main emphasis will be on the mathematical aspects of statistics, an effort will be made to balance rigor and relevance to statistical practice.

Stochastic convergence and its use in statistics: modes of convergence, weak law of large numbers, central limit theorem
Formalization of a statistical problem : parameters, models, parametrizations, sufficiency, ancillarity, completeness
Point estimation: methods of estimation, bias, variance, relative efficiency
Likelihood theory: the likelihood principle, asymptotic properties, misspecification of models, the Bayesian perspective
Optimality: decision theory, minimum variance unbiased estimation, Cramér-Rao lower bound, efficiency, robustness
Testing and Confidence Regions: Neyman-Pearson setup, likelihood ratio tests, UMP tests, duality with confidence intervals, confidence regions, large sample theory, goodness-of-fit testing. By the end of the course, the student must be able to:
Formulate the various elements of a statistical problem rigorously.
Formalize the performance of statistical procedures through probability theory.
Systematize broad classes of probability models and their structural relation to inference
Construct efficient statistical procedures for point/interval estimation and testing in classical contexts.
Derive certain exact (finite sample) properties of fundamental statistical procedures
Derive Derive certain asymptotic (large sample) properties of fundamental statistical procedures.
Formulate fundamental limitations and uncertainty principles of statistical theory.
Prove certain fundamental structural and optimality theorems of statistics.","The Future of Fuzzy Sets in Finance: New Challenges in Machine Learning and Explainable AITraditional statistical analysis is oriented towards finding linear relationships between the variables under investigation, often accompanied by strict assumptions about the problem and data distributions. Moreover, traditional analysis endorses data reduction as much as ",135,1,0.8482988,1,0.8482988
818,"Financial service activities, except insurance and pension funding","Risk, rare events and extremes",0.8473077,"Modelling of rare events, such as stock market crashes, storms and catastrophic structural failures, is important. This course will describe the special models and methods that are relevant to such modelling, including the mathematical bases, statistical tools and applications.
Content
Mathematical bases: behaviour of maxima and threshold exceedances in large samples, both for independent and dependent data. Poisson process modelling.
Statistical methods: modelling using the GEV and GP distributions, for independent and dependent data. Likelihood and Bayesian inference. Non-stationarity. Extremal coefficients. Multivariate extreme-value distributions. Max-stable processes.
Applications: Environmental, financial, and engineering applications. Use of R for extremal modelling.By the end of the course, the student must be able to:
Recognize situations where statistical analysis of extrema is appropriate
Manipulate mathematical objects related to the study of extrema
Analyze empirical data on extremes using appropriate statistical methods
Construct appropriate statistical models for extremal data
Interpret such models in terms of underlying phenomena
Infer properties of real systems in terms of probability models for extremes.","The Future of Fuzzy Sets in Finance: New Challenges in Machine Learning and Explainable AITraditional statistical analysis is oriented towards finding linear relationships between the variables under investigation, often accompanied by strict assumptions about the problem and data distributions. Moreover, traditional analysis endorses data reduction as much as ",135,3,0.8290167,1,0.8473077
819,"Financial service activities, except insurance and pension funding","Multivariate Statistics",0.8461398,"Multivariate Statistics deals with joint distributions of several random variables. This course introduces the basic concepts and provides an overview over classical and modern methods of multivariate statistics. We will consider the theory behind the methods as well as their applications.        After the course, you should be able to:
- describe the various methods and the concepts and theory behind them
- identify adequate methods for a given statistical problem
- use the statistical software ""R"" to efficiently apply these methods
- interpret the output of these methods.        Visualization / Principal component analysis / Multidimensional scaling / The multivariate Normal distribution / Factor analysis / Supervised learning / Cluster analysis","The Future of Fuzzy Sets in Finance: New Challenges in Machine Learning and Explainable AITraditional statistical analysis is oriented towards finding linear relationships between the variables under investigation, often accompanied by strict assumptions about the problem and data distributions. Moreover, traditional analysis endorses data reduction as much as ",135,1,0.8461398,1,0.8461398
833,"Financial service activities, except insurance and pension funding","Data Mining",0.8050281,"Display a comprehensive understanding of different data mining tasks, including classification, clustering, outlier detection, and pattern mining.
Reproduce the main characteristics and limitations of algorithms for addressing data mining tasks.
Select, based on a problem description of a data mining problem, the most appropriate combination of algorithms to solve it.
Analyze the models resulting from a data mining exercise and identify threaths to validity such as model bias, under- and overfitting.
Develop and execute a data mining workflow on a real-life dataset to solve a data-driven analysis problem.After a short introduction to data mining, we study and discuss several advanced data mining techniques. The data mining techniques that will be addressed are divided into the following categories:

Classification:
k-nearest neighbors, decision trees, Bayesian classifiers, LDA, logistic regression, support-vector machines, neural nets, rule-based classifiers, as well as techniques for combining classifiers in ensembles (bagging and boosting)
common issues: under- and overfitting, model-bias, bias-variance decomposition
evaluation techniques for classifiers: hold-out, cross validation
Clustering: k-means and k-medoids, density based clustering (DBSCAN), Expectation-Maximizatiion-based clustering
Outlier detection
Pattern mining: frequent itemset mining, subgroup discovery
During the coverage of these topics, several foundational concepts in machine learning and data mining will be treated, such as bias-variance decomposition, maximum likelihood learning, minimal description length principle, etc.

The course will also contain a practical component in which we will make use of the data mining suite Knime. A group project will be carried out using this data mining tool, or a tool of the students' choice.","Appropriate machine learning techniques for credit scoring and bankruptcy prediction in banking and finance: a comparative studyAbstract Machine learning techniques have been used successfully in several areas such as banking and finance. These techniques are used mainly for prediction, classification and partitioning data into different groups according to a certain common characteristic. In this ",135,9,0.8308697,1,0.8050281
835,"Financial service activities, except insurance and pension funding","Decision Support Models",0.8370127,"At the completion of the course, the student will: be familiar with distinct decision-making strategies and traps in the evaluation of options and in the allocation of resources in private and public contexts; be familiar with key theoretical and methodological concepts of decision-making and decision aid relevant for the best practice of decision engineering; be familiar with models, processes and tools for helping to structure and explore decisions characterized by multiple objectives, uncertainty, complexity and differences of opinion; be familiar with examples of real-world decision analysis and decision conferencing applications in organizations; be familiar with other topics considered relevant for engineering decisions, covering problem structuring methods, heuristics and biases and group decision and negotiation; have developed skills in decision analysis and modeling; • be able to select and use specialized decision support software in different decision contexts.The decision making problematic: Definition of the decision problem. Importance of decision making in
engineering and management. Characteristics of the decision context.
Decision making strategies. Uncertainty and complexity. Value and risk.
What is Decision Analysis (DA)? DA objectives. The seven fundamental steps of DA. DA schools of thought
and theoretical foundations. The problem of decision aiding.
Intervention strategies: From optimization to the learning paradigm. Value and utility analysis. Decision
conference and facilitation.
Concepts, models, techniques and software for decision support:
1. Decision trees and influence diagrams; case studies; PRECISION TREE.
2. Bayesian networks; case studies; NETICA.
3. Probabilities modeling and risk analysis; case studies; @RISK.
4. Cognitive mapping; case studies; DECISION EXPLORER.
5. Multiple criteria evaluation models; case studies; MACBETH.
6. Resource allocation and negotiation; case studies; PROBE and MACBETH.Teaching is mostly organized by groups of models, techniques and software for decision support that can
assist different types of decision problems. For each type of decision problem, teaching is based on the
presentation of methods, models and techniques to assist decision-makers, followed by a discussion of
real world case studies and of key methodological aspects, and on the use of decision support tools. For
some topics students also carry out practical exercises.
Evaluation is done through two groupwork assignments and one individual exam. In one groupwork
students structure problems characterized by uncertainty, build models and implement them in appropriate
software; in another groupwork students build a multicriteria evaluation model to assist a decision-maker
in a real problem.","Influence of contextual factors on the adoption process of Robotic process automation (RPA): Case study at Stora Enso Finance DeliveryThe introduction will first describe the theoretical background for the research, followed by origin of the study and problematizing. These considerations are used to define the purpose statement and research questions. The chapter concludes with limitations and outline for the ",135,6,0.8124977,1,0.8370127
836,"Financial service activities, except insurance and pension funding","Research Methodology",0.8197767,"This seminar tries to inform and guide the students about techniques, most common standards and systems for the practice of scientific research and its methodological bases and documentaries. The topics are as follows: General Approach (scientific knowledge and its purpose, problems of scientific research, research works); Scientific Work (choice of subject, setting objectives, formulating hypotheses, choice of work method, choice of tools and resources. Phases of work); Information Search (sources, publications, bibliographical searches, access to scientific documentation, internet, etc.); Work Writing (rules, principles, tips, style, language, etc.) and Presentation and Defence of Work (legal aspects, formal aspects, personal aspects, visual aids to support the presentation)","Influence of contextual factors on the adoption process of Robotic process automation (RPA): Case study at Stora Enso Finance DeliveryThe introduction will first describe the theoretical background for the research, followed by origin of the study and problematizing. These considerations are used to define the purpose statement and research questions. The chapter concludes with limitations and outline for the ",135,4,0.8105934,1,0.8197767
837,"Financial service activities, except insurance and pension funding","Bayesian Networks",0.8192495,"This module presents Bayesian Networks as graphic tools which are well consolidated and of wide use nowadays to model uncertainty and reason with in intelligent systems. Uncertainty is modelled with probabilities and reasoning is based on Bayes’ rule. It begins by explaining the meaning of the networks to model reasoning with uncertainty, both casual and non-casual, and both from a structural (qualitative) point of view and parametric (quantitative). The next step is to pose questions to the network, in other words, to infer knowledge from observations or data that is being collected. Thus, we can ask, for instance, for the diagnosis of a disease or the most likely explanation for the observed evidence. The algorithms can obtain the exact or an approximate answer, in the latter case probably using Monte Carlo simulation. The network is built by analysing the problem with an expert, but can also be induced from a database. This is a current issue: how to obtain a structure and parameters for the network and for that machine learning methods will be discussed. Finally, by knowing how to build the network and how to use it to perform queries, it will be possible to see its application on decision making and other applications of great interest within Artificial Intelligence: computer vision, automatic classification, filtering of email, etc.","Influence of contextual factors on the adoption process of Robotic process automation (RPA): Case study at Stora Enso Finance DeliveryThe introduction will first describe the theoretical background for the research, followed by origin of the study and problematizing. These considerations are used to define the purpose statement and research questions. The chapter concludes with limitations and outline for the ",135,5,0.810827,1,0.8192495
839,"Financial service activities, except insurance and pension funding","Programming Paradigms",0.8111435,"You have to understand the principles of the lambda-calculus and to be able to apply them in small examples: reduction, combinators, lists, etc.
You have to be able to explain how one gets from the lambda-calculus to an implementation, and what is the role therein of such concepts as evaluation strategies, combinators, etc.
You have to be able to explain, based on concrete examples, how a prolog system solves queries, and what is the use of unification, proof trees, etc.
You have to solve simple programming exercises in a functional as well as a logical language.The course covers several  programming paradigms that play an important role in computer science and in particular in artificial intelligence. The emphasis is on functional and logical programming languages.

First the theoretical foundation of the functional paradigm is treated in the form of a brief introduction into the lambda-calculus, then an introduction is given into the language Haskell. In the treatment of the logical paradigm the emphasis is on the language Prolog, its advantages and limitations.

","Influence of contextual factors on the adoption process of Robotic process automation (RPA): Case study at Stora Enso Finance DeliveryThe introduction will first describe the theoretical background for the research, followed by origin of the study and problematizing. These considerations are used to define the purpose statement and research questions. The chapter concludes with limitations and outline for the ",135,2,0.801596,1,0.8111435
848,"Financial service activities, except insurance and pension funding","Model Driven Engineering",0.8071574,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","Machine Learning for Structured FinanceMachine learning and artificial intelligence have evolved beyond simple hype and have integrated themselves in business and in popular conversation as an increasing number of smart applications profoundly transform the way we work and live. This article defines ",135,9,0.8152347,1,0.8071574
852,"Financial service activities, except insurance and pension funding","DD2424 Deep Learning in Data Science",0.8331833,"After the course, you should be able to:

explain the basic the ideas behind learning, representation and recognition of raw data
account for the theoretical background for the methods for deep learning that are most common in practical contexts
identify the practical applications in different fields of data science where methods for deep learning can be efficient (with special focus on computer vision and language technology)
in order to:

be able to solve problems connected to data representation and recognition
be able to implement, analyse and evaluate simple systems for deep learning for automatic analysis of image and text data
receive a broad knowledge enabling you to learn more about the area and read literature in the area
Course main content
Learning of representations from raw data: images and text
Principles of supervised learning
Elements for different methods for deep learning: convolutional networks and recurrent networks
Theoretical knowledge of and practical experience of training networks for deep learning including optimisation using stochastic gradient descent
New progress in methods for deep learning
Analysis of models and representations
Transferred learning with representations for deep learning
Application examples of deep learning for learning of representations and recognition","Essays on machine learning for economics and financeEconometrics and machine learning are quite close and related concepts. Nowadays, it is always more important to extract value from raw data, and distilling actionable insights from quantitative values as well as qualitative features. In order to deal with these topics, the first ",135,2,0.8312517,1,0.8331833
863,"Financial service activities, except insurance and pension funding","Statistical Methods in Data Mining",0.7941606,"Show the potential of statistical methods in data mining, with particular emphasis on classification, clustering, dimensionality reduction, anomaly detection and partial least squares methods. Develop the ability to apply statistical procedures to the analysis of large data sets, and to show how important those procedures are in decision making. Analyse real problems with specific software and identify suitable methodologies to deal with such problems. By the end of the semester, the students should know the main statistical procedures associated to data mining, and be familiar with other data mining techniques on a user level basis.Introduction. Data Mining Overview.Exploring data: Preprocessing, Visualization and Data Quality Classification

Classification Methods
- Classification with K-Nearest Neighbours
- Classification and Bayes Rule, Naïve Bayes
- Classification Trees
- Discriminant Analysis
- Logistic Regression
Evaluating the Performance of a Classifier
Comparing Classifiers

Clustering
Clustering Methods
- K-Means Clustering, Hierarchical Clustering
- EM for Mixture Model Density Estimation
Cluster Validation

Dimensionality Reduction
Principal Components
Independent Component Analysis
Multidimensional Scaling

Anomaly Detection
Preliminaries
Detecting Outliers
Evaluating the Performance of an Anomaly Detection Rule

Partial Least Squares
Introduction: More Variables than Objects
Partial Least Squares Regression","[PDF][PDF] Applications of Big Data methods in Finance: Index TrackingRESEARCH OBJECTIVES Although the curse of dimensionality does not relate to most financial settings, high-dimensional methods gained some relevance in the recent finance literature. Index tracking aims at finding an optimal sample of stocks able to mimic the ",135,5,0.8101401,1,0.7941606
865,"Financial service activities, except insurance and pension funding","Automated Planning",0.8306193,"Automated planning is a branch of Artificial intelligence aimed at obtaining plans (i.e. sequences of actions) for solving complex problems or for governing the behavior of intelligent agents, autonomous robots or unmanned vehicles. Planning techniques have been successfully applied in different domains, including industrial contexts, logistics, computer games, robotics or space exploration. In this seminar we will review the existing approaches for solving classical planning problems, such as state-space search, plan-space search, graph-based techniques or turning classical planning problems into propositional satisfiability problems. The course will then focus on the study of knowledge-based planning methods, such as control rule-based pruning or hierarchical task network-based planning techniques. These approaches exploit the domain knowledge provided by human experts to improve the performance of the planning algorithms. Finally, we will briefly introduce advanced planning algorithms, which are able to generate planning policies that take into account time constraints and/or partial observability conditions, which are common in real world applications.","Using Robotic Process Automation in Finance organizations: Case studySoftware robotics has emerged as a new technological development over the last few years, offering a lot of potential for optimizing and improving processes. Software robotics offers a new tool also for finance organizations, where there are still number of manual tasks being ",135,5,0.8090598,1,0.8306193
866,"Financial service activities, except insurance and pension funding","DD2425 Robotics and Autonomous Systems",0.8241531,"After completing the course the student should be able to

recall basic concepts in robotics
implement and integrate software components for robots
solve a robotics task with limited resources
identify and discuss ethical and societal aspects of robot technology
in order to

be able to work with autonomous and other complex systems in research and/or development
become better at planning, executing and developing work in project groups.
Course main content
During the course a small, mobile, autonomous robot for performing certain tasks is built. This work is carried out in groups as a project. At the end of the course there is a contest between the robots that the participants have constructed.

The lecture part of the course deals with fundamental concepts in robotics.

The practical part of the course adds hands on experience with sensors, actuators, programming of systems and building of robots.","Using Robotic Process Automation in Finance organizations: Case studySoftware robotics has emerged as a new technological development over the last few years, offering a lot of potential for optimizing and improving processes. Software robotics offers a new tool also for finance organizations, where there are still number of manual tasks being ",135,1,0.8241531,1,0.8241531
867,"Financial service activities, except insurance and pension funding","Biomedical Informatics",0.8191574,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","Using Robotic Process Automation in Finance organizations: Case studySoftware robotics has emerged as a new technological development over the last few years, offering a lot of potential for optimizing and improving processes. Software robotics offers a new tool also for finance organizations, where there are still number of manual tasks being ",135,18,0.8173554,1,0.8191574
881,"Public administration and defence, compulsory social security","Logistics Management and Operations",0.8067397,"The objective of the subject is to provides the knowledge on the fundamentals of logistics management and operations management. The most important methods and techniques used in the area are studied. This will provide the students with the capacity to identify, analyse and solve problems within the area. Learning outcomes: At the completion of the course, the student will be able to: • clearly identify the main aspects that characterize the area of Supply Chain Management and Operations Management; • link the definition of the Operations Management Strategy and the Industry Competitiveness; • describe the development stages of Products and its link to the Processes characterization, both within the manufacturing as well as services sectors; • develop an Operations Planning Process exploring the most important techniques in use. Understand the aggregating planning process, the definition of the master production schedule and the schedule integration; • to define the materials, importance and, based on the type of material, the most suitable inventory management methodology on the materials management area; • explain the basis on lean management and just in time philosophy. • identify the importance of Supply Chain Management considering: inventory, transportation and manufactory cost, design and planning; • define the logistics strategy most suitable for the different types of supply chains; • describe the concepts of Centralization vs. Decentralization and be able to apply them; • explore integration and distribution strategies within supply chains considering which trade-off should be made.Logistics Management and Operations. Operational Strategy. Product Development. Design and Process Selection. Logistics Chain Design: logistics strategy, capacity management, lean production, just-in-time. The planning process: aggregated plan, master production scheduling, scheduling. Inventory management. Synchronous manufacturing and the theory of constraints. Logistics and Distribution. Logistics Chain Coordination.","The application of artificial intelligence in public administration for forecasting high crime risk transportation areas in urban environmentPublic administration has adopted information and communication technology in order to construct new intelligent systems and design new risk prevention strategies in transportation management. The ultimate goal is to improve the quality of the transportation services and ",110,1,0.8067397,1,0.8067397
887,"Public administration and defence, compulsory social security","Advanced Topics in Machine Learning",0.8459262,"In this seminar, recent papers of the pattern recognition and machine learning literature are presented and discussed. Possible topics cover statistical models in computer vision, graphical models and machine learning.        The seminar ""Advanced Topics in Machine Learning"" familiarizes students with recent developments in pattern recognition and machine learning. Original articles have to be presented and critically reviewed. The students will learn how to structure a scientific presentation in English which covers the key ideas of a scientific paper. An important goal of the seminar presentation is to summarize the essential ideas of the paper in sufficient depth while omitting details which are not essential for the understanding of the work. The presentation style will play an important role and should reach the level of professional scientific presentations.        The seminar will cover a number of recent papers which have emerged as important contributions to the pattern recognition and machine learning literature. The topics will vary from year to year but they are centered on methodological issues in machine learning like new learning algorithms, ensemble methods or new statistical models for machine learning applications. Frequently, papers are selected from computer vision or bioinformatics - two fields, which relies more and more on machine learning methodology and statistical models.","Machine learning for public administration research, with application to organizational reputationAbstract Machine learning methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories ",110,4,0.8270569,1,0.8459262
888,"Public administration and defence, compulsory social security","Computational Intelligence Lab",0.8446426,"This laboratory course teaches fundamental concepts in computational science and machine learning with a special emphasis on matrix factorization and representation learning. The class covers techniques like dimension reduction, data clustering, sparse coding, and deep learning as well as a wide spectrum of related use cases and applications.        Students acquire fundamental theoretical concepts and methodologies from machine learning and how to apply these techniques to build intelligent systems that solve real-world problems. They learn to successfully develop solutions to application problems by following the key steps of modeling, algorithm design, implementation and experimental validation. 

This lab course has a strong focus on practical assignments. Students work in groups of two to three people, to develop solutions to three application problems: 1. Collaborative filtering and recommender systems, 2. Text sentiment classification, and 3. Road segmentation in aerial imagery. 

For each of these problems, students submit their solutions to an online evaluation and ranking system, and get feedback in terms of numerical accuracy and computational speed. In the final part of the course, students combine and extend one of their previous promising solutions, and write up their findings in an extended abstract in the style of a conference paper.
","Machine learning for public administration research, with application to organizational reputationAbstract Machine learning methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories ",110,11,0.8231046,1,0.8446426
894,"Public administration and defence, compulsory social security","Language Engineering",0.8117169,"Language Engineering (LE) is the set of techniques, resources and tools to solve problems by using more or less an automated language. This course aims to introduce students to the overall framework, which is currently the LE. The second part of the subject will explain the two main principles of most language treatment systems, such as the content representation models and the creation and maintenance of lexical resources, both pillars of any system and any use. In the third part of the course the student will be introduced three of the major commercial applications of LE, such as information retrieval (associated with the search for data or items of information in a text) and text mining, where besides extracting data type information, we will extract relationships between them. The existing application on the market, and the more immediate trends (for example the analysis of forums for opinions) will also be discussed and explained.","An information system for judicial and public administration using artificial intelligence and geospatial dataThe adoption of information technology in judicial and public administration has become a major need nowadays with the rapid growth of information regarding managerial issues. This paper presents an advanced methodology developed by using Information and ",110,6,0.8069792,1,0.8117169
900,"Public administration and defence, compulsory social security","Big Data, Law, and Policy",0.8280009,"        This course introduces students to societal perspectives on the big data revolution. Discussing important contributions from machine learning and data science, the course explores their legal, economic, ethical, and political implications in the past, present, and future.        This course is intended both for students of machine learning and data science who want to reflect on the societal implications of their field, and for students from other disciplines who want to explore the societal impact of data sciences. The course will first discuss some of the methodological foundations of machine learning, followed by a discussion of research papers and real-world applications where big data and societal values may clash. Potential topics include the implications of big data for privacy, liability, insurance, health systems, voting, and democratic institutions, as well as the use of predictive algorithms for price discrimination and the criminal justice system. Guest speakers, weekly readings and reaction papers ensure a lively debate among participants from various backgrounds.","Public Administration Curriculum-Based Big Data Policy-Analytic Epistemology: Symbolic IoT Action-Learning Solution ModelThe equilibration that underscores the internet of things (IoT) and big data analytics (BDA) cannot be underestimated at the behest of real-life social challenges and significant policy data generated to redress the concerns of epistemic communities, such as political policy ",110,6,0.8172789,1,0.8280009
902,"Public administration and defence, compulsory social security","Fairness, Explainability, and Accountability for ML",0.8062474,"- Familiarize students with the ethical implications of applying Big Data and ML tools to socially-sensitive domains; teach them to think critically about these issues.
- Overview the long-established philosophical, sociological, and economic literature on these subjects.
- Provide students with a tool-box of technical solutions for addressing - at least partially - the ethical and societal issues of ML and Big data.        As ML continues to advance and make its way into different aspects of modern life, both the designers and users of the technology need to think seriously about its impact on individuals and society. We will study some of the ethical implications of applying ML tools to socially sensitive domains, such as employment, education, credit ledning, and criminal justice. We will discuss at length what it means for an algorithm to be fair; who should be held responsible when algorithmic decisions negatively impacts certain demographic groups or individuals; and last but not least, how algorithmic decisions can be explained to a non-technical audience. Throughout the course, we will focus on technical solutions that have been recently proposed by the ML community to tackle the above issues. We will critically discuss the advantages and shortcomings of these proposals in comparison with non-technical alternatives.","Public Administration Curriculum-Based Big Data Policy-Analytic Epistemology: Symbolic IoT Action-Learning Solution ModelThe equilibration that underscores the internet of things (IoT) and big data analytics (BDA) cannot be underestimated at the behest of real-life social challenges and significant policy data generated to redress the concerns of epistemic communities, such as political policy ",110,3,0.8032413,1,0.8062474
914,"Public administration and defence, compulsory social security","How To Write Fast Numerical Code",0.8154895,"This course introduces the student to the foundations and state-of-the-art techniques in developing high performance software for numerical functionality such as linear algebra and others. The focus is on optimizing for the memory hierarchy and for special instruction sets. Finally, the course will introduce the recent field of automatic performance tuning.        Software performance (i.e., runtime) arises through the interaction of algorithm, its implementation, and the microarchitecture the program is run on. The first goal of the course is to provide the student with an understanding of this interaction, and hence software performance, focusing on numerical or mathematical functionality. The second goal is to teach a general systematic strategy how to use this knowledge to write fast software for numerical problems. This strategy will be trained in a few homeworks and semester-long group projects.        The fast evolution and increasing complexity of computing platforms pose a major challenge for developers of high performance software for engineering, science, and consumer applications: it becomes increasingly harder to harness the available computing power. Straightforward implementations may lose as much as one or two orders of magnitude in performance. On the other hand, creating optimal implementations requires the developer to have an understanding of algorithms, capabilities and limitations of compilers, and the target platform's architecture and microarchitecture. 

This interdisciplinary course introduces the student to the foundations and state-of-the-art techniques in high performance software development using important functionality such as linear algebra functionality, transforms, filters, and others as examples. The course will explain how to optimize for the memory hierarchy, take advantage of special instruction sets, and, if time permits, how to write multithreaded code for multicore platforms. Much of the material is based on state-of-the-art research. 

Further, a general strategy for performance analysis and optimization is introduced that the students will apply in group projects that accompany the course. Finally, the course will introduce the students to the recent field of automatic performance tuning.","This paper describes a model of digital governance that reproduces within the system essential features of public administration while establishing logic for their utilization. The ultimate goal is to be able to confine all participants to their respective roles and Administration by Algorithm? Public Management Meets Public Sector Machine Learning",110,1,0.8154895,1,0.8154895
921,"Public administration and defence, compulsory social security","Advanced topics on privacy enhancing technologies",0.8017337,"This course will provide students with the knowledge to tackle the design of privacy-preserving ICT systems. Students will learn about existing technologies to protect privacy, and how to evaluate the protection they provide.The course will delve into the following topics:
Privacy definitions and concepts, and the socioeconomic context of privacy: economics and incentives, ethics, regulation.

Cryptographic privacy solutions: Identity management and anonymous credentials, zero-knowledge proofs, secure multi-party computation, homomorphic encryption, garbled circuits, Private information retrieval (PIR), Oblivious RAM (ORAM)

Anonymization and data hiding: k-anonymity, l-diversity, t-proximity; dummy use, differential privacy and Laplacian noise; composability

Machine learning and privacy: how machine learning can be use to infer private information; and how much information can be learned from machine learning models.

Protection of metadata: anonymous communications systems, location privacy, censorpship resistance.

Online tracking and massive surveillance.

Evaluation of privacy preserving systems - notions, definitions, quantification / computation

Fairness and transparency and their interplay with privacy. Privacy, anonymity, homomorphic encryption, ethics. By the end of the course, the student must be able to:
Select appropriately privacy mechanisms
Develop privacy technologies
Assess / Evaluate privacy protection
Reason about privacy concerns","Security and privacy of big data for social networking services in cloudBig Data (BD) is of great importance especially in wireless telecommunications field. Social Networking (SNg) is one more fast-growing technology that allows users to build their profile and could be described as web applications. Both of them face privacy and security issues ",110,1,0.8017337,1,0.8017337
922,"Public administration and defence, compulsory social security","Mobile and Wireless Networks",0.7965963,"The student has a good insight in the most important characteristics of the PHY, MAC, network and transport layer in wireless and mobile networks.
Moreover, the student has a good overview of wireless and mobile networks in use and to be in use in the near future, in the area of public cellular networks, wireless LAN, wireless personal networks, ad hoc networks and sensor networks.

These learning outcomes correspond with the following general learning outcomes for Master in the computer science:
- Analysis and design of large computer science projects: the student has the ability to divide a large problem into smaller problems and to devise a solution for each of these sub-problems
- The student is able to select techniques, methods and architectures for the problem at hand, taking into account
the specific characteristics of the system under study.
 This course consists of two parts. In a first part, important characteristics of the PHY, MAC, network and transport layer of wireless and mobile networks are considered. In addition a number of important protocols for each of these layers are studied. In a second part mobile and wireless networks are discussed. The following systems are studied: cellular networks: UMTS, LTE, TETRA; wireless personal area networks: bluetooth, zigbee; Wireless LAN: WiFi; Wireless MAN: WiMAX; sensornetworks, ad hoc networks.","Security and privacy of big data for social networking services in cloudBig Data (BD) is of great importance especially in wireless telecommunications field. Social Networking (SNg) is one more fast-growing technology that allows users to build their profile and could be described as web applications. Both of them face privacy and security issues ",110,2,0.8027763,1,0.7965963
924,"Public administration and defence, compulsory social security","Computer and network security",0.7841728,"Understand and use cryptographic algorithms
Understand and identify security flaws in cryptographic algorithms
Design and develop secure (distributed) software applications
Configure and deploy secure computer network protocols and policies. This course provides a general introduction to all aspects related to security of IT systems, software, and computer networks. It aims to familiarize students with how to detect security flaws in software and systems, how to fix them, and most importantly how to avoid them when implementing their own software or deploying their own systems and networks. During the lectures, students will be provided with the theoretical basis and knowledge required to understand and implement secure solutions. The complementary lab sessions will serve to apply the acquired theoretical knowledge in practice. Several guest lectures are organised to provide a link with industry. Concretely, the following topics will be addressed:

Encryption
Symmetric encryption and confidentiality
Public-key encryption
Message authentication codes and cryptographic hashing algorithms
Algorithms (DES, AES, RSA, SHA, …)
Network security
Key distribution and user authentication
Transport-level encryption (HTTPS, SSH, SSL, TLS)
Wireless network security (WES, WPA)
Email security (PGP, S/MIME)
IP security (IPSec, Internet Key Exchange)
System security
Software attacks (SQL injection, zero-day attacks, etc.)
Viruses and malware
Firewalls","Security and privacy of big data for social networking services in cloudBig Data (BD) is of great importance especially in wireless telecommunications field. Social Networking (SNg) is one more fast-growing technology that allows users to build their profile and could be described as web applications. Both of them face privacy and security issues ",110,2,0.7919443,1,0.7841728
928,"Public administration and defence, compulsory social security","Artificial neural networks",0.8326963,"Since 2010 approaches in deep learning have revolutionized fields as diverse as computer vision, machine learning, or artificial intelligence. This course gives a systematic introduction into the main models of deep artificial neural networks: Supervised Learning and Reinforcement Learning.Simple perceptrons for classification
BackProp and Multilayer Perceptrons for deep learning
Statistical Classification by deep networks
Regularization and Tricks of the Trade in deep learning
Error landscape and optimization methods for deep networks 
 Convolutional networks
Sequence prediction and recurrent networks
 Reinforcement Learning 1: Bellman equation and SARSA
 Reinforcement Learning 2: variants of SARSA, Q-learning, n-step-TD learning
 Reinforcement Learning 3: Policy gradient
 Deep reinforcement learning: applications 
 Reinforcement learning and the brain. Deep learning, artificial neural networks, reinforcement learning, TD learning, SARSA.Learning Outcomes
By the end of the course, the student must be able to:
Apply learning in deep networks to real data
Assess / Evaluate performance of learning algorithms
Elaborate relations between different mathematical concepts of learning
Judge limitations of algorithms
Propose algorithms and models for learning in deep networks
Transversal skills
Continue to work through difficulties or initial failure to find optimal solutions.
Manage priorities.
Access and evaluate appropriate sources of information.
Write a scientific or technical report.","Bridging machine learning and cryptography in defence against adversarial attacksIn the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks. However, most of the deep learning architectures are vulnerable to so called adversarial examples. This ",110,1,0.8326963,1,0.8326963
929,"Public administration and defence, compulsory social security","DD2424 Deep Learning in Data Science",0.82932,"After the course, you should be able to:

explain the basic the ideas behind learning, representation and recognition of raw data
account for the theoretical background for the methods for deep learning that are most common in practical contexts
identify the practical applications in different fields of data science where methods for deep learning can be efficient (with special focus on computer vision and language technology)
in order to:

be able to solve problems connected to data representation and recognition
be able to implement, analyse and evaluate simple systems for deep learning for automatic analysis of image and text data
receive a broad knowledge enabling you to learn more about the area and read literature in the area
Course main content
Learning of representations from raw data: images and text
Principles of supervised learning
Elements for different methods for deep learning: convolutional networks and recurrent networks
Theoretical knowledge of and practical experience of training networks for deep learning including optimisation using stochastic gradient descent
New progress in methods for deep learning
Analysis of models and representations
Transferred learning with representations for deep learning
Application examples of deep learning for learning of representations and recognition","Bridging machine learning and cryptography in defence against adversarial attacksIn the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks. However, most of the deep learning architectures are vulnerable to so called adversarial examples. This ",110,2,0.8312517,1,0.82932
938,"Public administration and defence, compulsory social security","Topics in Networks and Distributed Systems",0.7853815,"You have thorough knowledge about the latest academic advances in the field of distributed computing. This can be very broad: ranging from security management, consensus based protocols, virtualization, etc.
You have gained product and algorithmic knowledge of different cloud offerings (e.g., IaaS, Storage as a Service) and how they are designed.
You have insight in the most important research areas in distributed computing
You are able to apply the studied concepts in a distributed systems project
You are able to critically discuss and review now topics in distributed computing.This course is a tour through various state of the art topics in the area of distributed computing. We tackle this both from an academic perspective (focusing mainly on new and challenging research areas) and from an industrial perspective (focusing mainly on the algorithms behind state of the art products in industry). For a great part, the course will consist of guest lectures of both national and international experts, both from universities and companies.Attendance to the guest lectures is mandatory. 

Because the course focuses on the latest advances in the area of distributed computing, the exact list of covered topics can change. Below is a list of confirmed topics:

Map Reduce and Hadoop (guest lecture by Thomas Demoor, Amplidata)
Overview of virtualization and the link with cloud computing
Advanced cloud management techniques
Mobile cloud computing (guest lecture by Pieter Simoens, Ghent University - iMinds)
The use of cloud computing in telecommunication networks (guest lecture by Erwin Six, Alcatel-Lucent)
Computer systems security and reliability (guest lecture by Jan Vykopal, Masaryk Unviversity, Czech Republic)
Flow-Based Measurements (guest lecture by Anna Sperotto, University of Twente)
Consensus protocols in a Storage as a Service product: a view from industry (Guest lecture by Carl D'Halluin of Amplidata)
(Complete list to be confirmed)
The practical side of the course involves hands-on work with the technologies discussed above. This will be in the form of an integrated programming project that runs throughout the duration of the course. 

","Analysis of Military Academy Smart Campus Based on Big DataThis paper compares the digital campus with the smart campus and analysis the framework of smart campus in the big data environment, based on the actual characteristics of military academies. The framework utilizes the information technologies such as Internet of Things ",110,1,0.7853815,1,0.7853815
940,"Public administration and defence, compulsory social security","DD2437 Artificial Neural Networks and Deep Architectures",0.8318554,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Facilitation of Trust in Automation: A Qualitative Study of Behaviour and Attitudes Towards Emerging Technology in Military CultureThe research in this field is limited due to the inherent technological limitations of existing systems, of which has saturated the literature at this point (Barnes, et al., 2014). The core of existing research centres along assessment of emerging and novel interfaces for the pursuit ",110,33,0.8290868,1,0.8318554
943,"Public administration and defence, compulsory social security","Optimization for Data Science",0.8079739,"This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.        Understanding the theoretical and practical aspects of relevant optimization methods used in data science. Learning general paradigms to deal with optimization problems arising in data science.        This course teaches an overview of modern optimization methods, with applications in particular for machine learning and data science.

In the first part of the course, we will discuss how classical first and second order methods such as gradient descent and Newton's method can be adapated to scale to large datasets, in theory and in practice. We also cover some new algorithms and paradigms that have been developed specifically in the context of data science. The emphasis is not so much on the application of these methods (many of which are covered in other courses), but on understanding and analyzing the methods themselves.

In the second part, we discuss convex programming relaxations as a powerful and versatile paradigm for designing efficient algorithms to solve computational problems arising in data science. We will learn about this paradigm and develop a unified perspective on it through the lens of the sum-of-squares semidefinite programming hierarchy. As applications, we are discussing non-negative matrix factorization, compressed sensing and sparse linear regression, matrix completion and phase retrieval, as well as robust estimation.","Facilitation of Trust in Automation: A Qualitative Study of Behaviour and Attitudes Towards Emerging Technology in Military CultureThe research in this field is limited due to the inherent technological limitations of existing systems, of which has saturated the literature at this point (Barnes, et al., 2014). The core of existing research centres along assessment of emerging and novel interfaces for the pursuit ",110,15,0.8172313,1,0.8079739
953,"Public administration and defence, compulsory social security","Data Analysis and Integration",0.8073579,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Application of Big Data Technology in Scientific Research Data Management of Military EnterprisesScientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology ",110,20,0.8251021,1,0.8073579
960,"Public administration and defence, compulsory social security","Intelligent agents",0.8724451,"Software agents are widely used to control physical, economic and financial processes. The course presents practical methods for implementing software agents and multi-agent systems, supported by programming exercises, and the theoretical underpinnings including computational game theory.
Content
The course contains 4 main subject areas:

 

1) Basic models and algorithms for individual agents:
game-playing algorithms, reactive agents and reinforcement learning. Models and algorithms for rational, goal-oriented behavior in agents.
2) Multi-agent systems: 
multi-agent planning, distributed algorithms for constraint satisfaction, coordination techniques for multi-agent systems.
3) Self-interested agents:
Models and algorithms for implementing self-interested agents motivated by economic principles: elements of computational game theory, models and algorithms for automated negotiation, social choice, mechanism design, electronic auctions and marketplaces.
4) Implementing multi-agent systems:
Agent platforms, ontologies and markup languages, web services and standards for their definition and indexing.By the end of the course, the student must be able to:
Choose and implement methods for rational decision making in software agents, based on decision processes and AI planning techniques
Choose and implement methods for efficient rational decision making in teams of multiple software agents
Model scenarios with multiple self-interested agents in the language of game theory
Evaluate the feasibility of achieving goals with self-interested agents using game theory
Design, choose and implement mechanisms for self-interested agents using game theory
Implement systems of software agents using agent platforms","Machine learning techniques for autonomous agents in military simulationsMultum in parvoIn military simulations, software agents are used to represent individuals, weapon platforms or aggregates thereof. Modeling the behavioral capabilities and limitations of such agents may be time-consuming, requiring extensive interaction with subject matter experts and ",110,2,0.85044,1,0.8724451
964,"Public administration and defence, compulsory social security","Distributed intelligent systems",0.829514,"The goal of this course is to provide methods and tools for modeling distributed intelligent systems as well as designing and optimizing coordination strategies. The course is a well-balanced mixture of theory and practical activities using simulation and real hardware platforms.
 

Introduction to key concepts such as self-organization and software and hardware tools used in the course
Examples of natural, artificial and hybrid distributed intelligent systems
Modeling methods: sub-microscopic, microscopic, macroscopic, multi-level; spatial and non-spatial; mean field, approximated and exact approaches
Machine-learning methods: single- and multi-agent techniques; expensive optimization problems and noise resistance
Coordination strategies and distributed control: direct and indirect schemes; algorithms and methods; performance evaluation
Application examples in distributed sensing and action
 


Artificial intelligence, swarm intelligence, distributed robotics, sensor networks, modeling, machine-learning, control. By the end of the course, the student must be able to:
Design a reactive control algorithm
Formulate a model at different level of abstraction for a distributed intelligent system
Analyze a model of a distributed intelligent system
Analyze a distributed coordination strategy/algorithm
Design a distributed coordination strategy/algorithm
Implement code for single robot and multi-robot systems
Carry out systematic performance evaluation of a distributed intelligent system
Apply modeling and design methods to specific problems requiring distributed sensing and action
Optimize a controller or a set of possibly coordinated controllers using model-based or data-driven methods
Transversal skills
Demonstrate a capacity for creativity.
Access and evaluate appropriate sources of information.
Collect data.
Plan and carry out activities in a way which makes optimal use of available time and other resources.
Make an oral presentation.
Write a scientific or technical report.
Evaluate one's own performance in the team, receive and respond appropriately to feedback.","Machine learning techniques for autonomous agents in military simulationsMultum in parvoIn military simulations, software agents are used to represent individuals, weapon platforms or aggregates thereof. Modeling the behavioral capabilities and limitations of such agents may be time-consuming, requiring extensive interaction with subject matter experts and ",110,1,0.829514,1,0.829514
972,"Public administration and defence, compulsory social security","Simulation Methods",0.7999075,"Simulation consists on building computer models that describe the essential behavior of a
system of interest and designing and conducting experiments with such models in order
to draw conclusions from their results in order to support decision-making. Typically, it is
used in the analysis of such complex systems, so it is not possible to make an analytical
analysis, or on the basis of a numerical analysis. Nowadays, simulation is a fundamental
experimental methodology in fields as diverse as economics, statistics, computer science,
chemical engineering, ecology and physics, with huge industrial and commercial
applications, ranging from manufacturing systems to flight simulators, through computer
games, stock prediction and weather forecasting.
In the subject we will show multiple applications in Artificial Intelligence, especially in the
discipline of Decision Analysis","[PDF][PDF] Developments in Artificial IntelligenceOpportunities and Challenges for Military Modeling and SimulationOne of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is"" Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"". Simulation might play a significant role to play in these developments as it ",110,6,0.8081483,1,0.7999075
977,"Public administration and defence, compulsory social security","Decision Support Sytems",0.8220729,"The DSS are interactive computer systems, the aim of which is to help decision
makers in the use of data and models to solve unstructured problems.
The DSS emerged in 1970s to solve complex situations in which individuals have to
choose between several possible alternatives and follow the optimal or a satisfactory
one. For this decision making, the experience, common sense or intuitions of experts
are not enough since often multiple conflicting criteria usually exist including,
uncertainty, several decision makers and various stages. The endless versatility of
real-world human decision problems has triggered necessary efforts in multiple areas
in order to build a sequence of coherent schemes (patterns), increasingly broader to
approach decision making problems correctly. This module will focus on exposing the
foundations and applications of the main lines of current development of Decision
Processes, studying different tools and software that have emerged in recent years
for the modelling and evaluation of decision making problems in uncertain
environments","Are We Flooding Pilots with Data?Effects of Situational Awareness Automation Support Concepts on Decision-Making in Modern Military Air OperationsWithin highly dynamic situations, the amount of relevant information that a pilot needs to process to make an informed decision can be substantial. With an ever increasing amount of data available to the pilot there is a real risk that not all relevant data can be taken into ",110,2,0.8154595,1,0.8220729
980,"Public administration and defence, compulsory social security","Information Security",0.8369449,"This course provides an introduction to Information Security. The focus
is on fundamental concepts and models, basic cryptography, protocols and system security, and privacy and data protection. While the emphasis is on foundations, case studies will be given that examine different realizations of these ideas in practice.        Master fundamental concepts in Information Security and their
application to system building. (See objectives listed below for more details).        1. Introduction and Motivation (OBJECTIVE: Broad conceptual overview of information security) Motivation: implications of IT on society/economy, Classical security problems, Approaches to 
defining security and security goals, Abstractions, assumptions, and trust, Risk management and the human factor, Course verview. 2. Foundations of Cryptography (OBJECTIVE: Understand basic 
cryptographic mechanisms and applications) Introduction, Basic concepts in cryptography: Overview, Types of Security, computational hardness, Abstraction of channel security properties, Symmetric 
encryption, Hash functions, Message authentication codes, Public-key distribution, Public-key cryptosystems, Digital signatures, Application case studies, Comparison of encryption at different layers, VPN, SSL, Digital payment systems, blind signatures, e-cash, Time stamping 3. Key Management and Public-key Infrastructures (OBJECTIVE: Understand the basic mechanisms relevant in an Internet context) Key management in distributed systems, Exact characterization of requirements, the role of trust, Public-key Certificates, Public-key Infrastructures, Digital evidence and non-repudiation, Application case studies, Kerberos, X.509, PGP. 4. Security Protocols (OBJECTIVE: Understand network-oriented security, i.e.. how to employ building blocks to secure applications in (open) networks) Introduction, Requirements/properties, Establishing shared secrets, Principal and message origin authentication, Environmental assumptions, Dolev-Yao intruder model and 
variants, Illustrative examples, Formal models and reasoning, Trace-based interleaving semantics, Inductive verification, or model-checking for falsification, Techniques for protocol design, 
Application case study 1: from Needham-Schroeder Shared-Key to Kerberos, Application case study 2: from DH to IKE. 5. Access Control and Security Policies (OBJECTIVES: Study system-oriented security, i.e., policies, models, and mechanisms) Motivation (relationship to CIA, relationship to Crypto) and examples Concepts: policies versus models versus mechanisms, DAC and MAC, Modeling formalism, Access Control Matrix Model, Roll Based Access Control, Bell-LaPadula, Harrison-Ruzzo-Ullmann, Information flow, Chinese Wall, Biba, Clark-Wilson, System mechanisms: Operating Systems, Hardware Security Features, Reference Monitors, File-system protection, Application case studies 6. Anonymity and Privacy (OBJECTIVE: examine protection goals beyond standard CIA and corresponding mechanisms) Motivation and Definitions, Privacy, policies and policy languages, mechanisms, problems, Anonymity: simple mechanisms (pseudonyms, proxies), Application case studies: mix networks and crowds. 7. Larger application case study: GSM, mobility
","Article deals with a set of problems linked to a Engineer Force Protection Provision algorithm design and evaluation of input factors series. This algorithm is generally compatible with The NATO Force Protection Process Model adjusting it to a part of engineer forces' decision Intrusion Detection of Data Platform Based on Extreme Learning Machine in Civil and Military Integration",110,4,0.8118418,1,0.8369449
984,"Public administration and defence, compulsory social security","System Security",0.818242,"        The first part of the lecture covers individual system aspects starting with tamperproof or tamper-resistant hardware in general over operating system related security mechanisms to application software systems, such as host based intrusion detection systems. In the second part, the focus is on system design and methodologies for building secure systems.        In this lecture, students learn about the security requirements and capabilities that are expected from modern hardware, operating systems, and other software environments. An overview of available technologies, algorithms and standards is given, with which these requirements can be met.        The first part of the lecture covers individual system's aspects starting with tamperproof or tamperresistant hardware in general over operating system related security mechanisms to application software systems such as host based intrusion detetction systems. The main topics covered are: tamper resistant hardware, CPU support for security, protection mechanisms in the kernel, file system security (permissions / ACLs / network filesystem issues), IPC Security, mechanisms in more modern OS, such as Capabilities and Zones, Libraries and Software tools for security assurance, etc.

In the second part, the focus is on system design and methodologies for building secure systems. Topics include: patch management, common software faults (buffer overflows, etc.), writing secure software (design, architecture, QA, testing), compiler-supported security, language-supported security, logging and auditing (BSM audit, dtrace, ...), cryptographic support, and trustworthy computing (TCG, SGX).

Along the lectures, model cases will be elaborated and evaluated in the exercises.","Article deals with a set of problems linked to a Engineer Force Protection Provision algorithm design and evaluation of input factors series. This algorithm is generally compatible with The NATO Force Protection Process Model adjusting it to a part of engineer forces' decision Intrusion Detection of Data Platform Based on Extreme Learning Machine in Civil and Military Integration",110,3,0.8086568,1,0.818242
989,"Water supply, sewerage, waste management and remediation activities","A Network Tour of Data Science",0.830242,"In the last decade, our information society has mutated into a data society, where the volume of worldwide data doubles every 1.5 years. How to make sense of such tremendous volume of data? Developing effective techniques to extract meaningful information from large-scale and high-dimensional dataset has become essential for the success of business, government and science.The goal of this course is to provide a broad introduction to effective algorithms in data science and network analysis. A major effort will be given to show that existing data analysis techniques can be defined and enhanced on graphs. Graphs encode complex structures like cerebral connection, stock exchange, and social network. Strong mathematical tools have been developed based on linear and non-linear graph spectral harmonic analysis to advance the standard data analysis algorithms. Main topics of the course are networks, unsupervised and supervised learning, recommendation, visualization, sparse representation, multi-resolution analysis, neuron network, and large-scale computing.","Machine learning in coupled wildfire-water supply risk assessment: Data science toolkitThe frontier of wildfire-related risk assessment is moving into data science territory, and with good reason. Computational statistics, built on a foundation of high resolution remote sensing data, ground data, and theory, forms the basis of powerful risk assessment tools ",30,53,0.8248198,1,0.830242
994,"Water supply, sewerage, waste management and remediation activities","Data Analysis and Integration",0.7861007,"The course on Data Analysis and Integration aims at teaching the students the most important concepts of data integration according to two different perspectives: virtual data integration, where the data sources can be accessed through a mediator-based architecture; and materialized data integration, where a materialized data repository (named data warehouse) is populated with data coming from the data sources. Additionally, the course will teach techniques that can be used to exploit information: OLAP (On-line Analytical Processing) and reporting in a warehoused architecture, and mash-up systems in a virtual architecture. The data integration processes aim at supplying, among other applications, a uniform view over a set of autonomous and heterogeneous data sources, making it easy the access to source data for analysis and visualization purposes. Their application domains are diverse, ranging from the Business Intelligence systems to scientific research systems (e.g., Bioinformatics).The course syllabus for Data Analysis and Integration includes mostly topics from the Information Management (IM) area in the ACM/IEEE CS 2013 Curriculum. The course topics, which are presented next, are labeled with the topics from this curriculum, and with the topics from the ACM CCS 2012 taxonomy, for further clarification:

1.        Main challenges of data integration processes; data integration paradigms. Heterogeneous data sources: XML data management and processing. 
ACM/IEEE CS 2013 IM/Information Management Concepts > Declarative and navigational queries  
ACM/IEEE CS 2013 IM/Information Management Concepts > Information capture and representation  
ACM/IEEE CS 2013 IM/Data Modelling > Semi-structured data model 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Semi-structured data 
2.        Heterogeneous data sources: (sensor) data stream management and processing. Virtual data integration: wrappers and mediators; query expression manipulation.
ACM/IEEE CS 2013 IM/Database Systems > Systems supporting structured and/or stream content 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge 
ACM CCS 2012 Information systems > Database management system engines > Stream management
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Streams
3.        Query answering using views; source descriptions.
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
4.        Schema mapping languages: global-as-view and local-as-view; schema mapping and matching.  
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Mediators and data integration 
5.        Wrappers: manual and automatic construction. 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Wrappers (data mining)
ACM CCS2012 Information systems > WorldWideWeb > Web mining > Data extraction and integration 
ACM CCS 2012 Information systems > World Wide Web > Web mining > Site Wrapping 
6.        Data warehousing: multi-dimensional modeling and data warehouse conception.
ACM/IEEE CS 2013 IM/Data Modelling > Spreadsheet models
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data warehouses 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data warehouses 
7.        OLAP (Online-Analytical Processing) and ETL (Extraction-Transformation-Loading).  
ACM/IEEE CS 2013 IM/DataMining > Data Visualization
ACM/IEEE CS 2013 IM/Storage and Retrieval > Information Summarization and Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Database management system engines > Online analytical processing engines 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Extraction, transformation and loading 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Data analytics 
ACM CCS 2012 Information systems > Information systems applications > Decision support systems > Online analytical processing 
8.        Caching and partial materialization; reporting. Data Exchange: declarative warehousing. 
ACM/IEEE CS 2013 IM/Data Mining > Data Visualization 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Exchange 
9.        Data cleaning: taxonomy of data quality problems; data quality dimensions. 
 ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
 ACM CCS 2012 Information systems > Data Management Systems > Information integration > Data Cleaning
 ACM CCS 2012 Information systems > Information Systems Applications > Data mining > Data cleaning 
10.        Approximate duplicate detection: string and data matching algorithms.
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning 
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Deduplication 
11.        Data fusion. Mash-ups: motivation, creation and application. 
ACM/IEEE CS 2013 IM/Data Mining > Data Cleaning
ACM CCS 2012 Information systems > Data Management Systems > Information integration > Entity Resolution 
12.        Data Provenance and Applications 
ACM/IEEE CS 2013 CN/Data, Information, and Knowledge > Digital content creation/capture and preservation 
ACM CCS 2012 Information systems > Data Management Systems > Data model extensions > Data Provenance. The evaluation of the Data Analysis and Integration course consists of a final exam (55% of the final grade, minimum grade of 9.5) and three small projects developed throughout the semester (45% of final grade, minimum grade of 9.5 in the average of the three projects).

Working students (i.e., students enrolled in recognized professional activities) may choose an evaluation method based only on the exam (100% of the final grade).

After the exam, students whose difference between the project grade (i.e., the average grade for the projects, on a scale between 0 and 20) and the exam grade is higher than 5 values, will have as final grade the lowest between the exam and project grades. Students in this situation may propose themselves to an oral evaluation, to be undertaken at the end of the semester and after the second exam. In this case, the final grade may vary between the lowest between the exam and project grades and the average obtained from the exam and project grades, according to the performance of the student.

Each small project will contain practical problems, for which students must find programmatic solutions (e.g., SQL statements), and theoretical questions. The first project will cover topics 1-3 of the course, the second project will cover topics 4-7 of the course, and the third project will cover topics 8-10. ","Preparing for the Use of Big Data in Denmark's Waste Management SectorThis project explored the challenges and opportunities associated with prospective implementations of big data analytics in Denmark's waste industry. We found that while some waste management companies collect detailed data, they do not use or share their ",30,20,0.8251021,1,0.7861007
995,"Water supply, sewerage, waste management and remediation activities","Biomedical Informatics",0.8160574,"Biomedical informatics tries to analyse problems in medical practice from the viewpoint of information management (medical and biological) and find the best solutions by using computers. Therefore, the emphasis is on the handling of data, information and knowledge, and not on the techniques and methods used. Many of the current problems of biomedicine have their root cause in the defects of analysing and managing information, which might have better solutions with proper systems from medical informatics. Technologies are not the ultimate goal of biomedical informatics; however, it is indeed important to use methods that would not only allow building the best applications, but also sharing and reusing skills and knowledge by encouraging collaboration between research groups. These joint efforts are stimulated by the growth of the Internet and new techniques of Artificial Intelligence, database, programming and Software Engineering, which facilitate communication between applications and groups. The use of new technology-based systems (e.g. the Semantic Web or Grid) is contributing to a breakthrough in biomedical informatics.","A framework of developing a big data platform for construction waste management: a Hong Kong studyBig data has shown great potentials in improving management discretion in many areas. The applications of big data in areas such as finance, computer science, health care and medical science have made continued success. Despite of big data's potentials, its ",30,18,0.8173554,1,0.8160574
1002,"Water supply, sewerage, waste management and remediation activities","Big Data, Law, and Policy",0.8191643,"        This course introduces students to societal perspectives on the big data revolution. Discussing important contributions from machine learning and data science, the course explores their legal, economic, ethical, and political implications in the past, present, and future.        This course is intended both for students of machine learning and data science who want to reflect on the societal implications of their field, and for students from other disciplines who want to explore the societal impact of data sciences. The course will first discuss some of the methodological foundations of machine learning, followed by a discussion of research papers and real-world applications where big data and societal values may clash. Potential topics include the implications of big data for privacy, liability, insurance, health systems, voting, and democratic institutions, as well as the use of predictive algorithms for price discrimination and the criminal justice system. Guest speakers, weekly readings and reaction papers ensure a lively debate among participants from various backgrounds.","Remediation, convergence, and big data: Conceptual limits of cross-platform social mediaThe era of multiplatform media and big data provide new opportunities to reconsider data access by media companies. Outlined here is the discussion surrounding data access from media institutional logic and user-centric perspectives in the contexts of digitalization and ",30,6,0.8172789,1,0.8191643
1004,"Water supply, sewerage, waste management and remediation activities","Data visualization",0.8086314,"Understanding why and how to present complex data interactively in an effective manner has become a crucial skill for any data scientist. In this course, you will learn how to design, judge, build and present your own interactive data visualizations.Tentative course schedule

Week 1: Introduction to Data visualization Web development

Week 2: Javascript

Week 3: More Javascript

Week 4: Data Data driven documents (D3.js)

Week 5: Interaction, filtering, aggregation (UI /UX). Advanced D3 / javascript libs

Week 6: Perception, cognition, color Marks and channels

Week 7: Designing visualizations (UI/UX) Project introduction Dos and don¿ts for data-viz

Week 8: Maps (theory) Maps (practice)

Week 9: Text visualization

Week 10: Graphs

Week 11: Tabular data viz Music viz

Week 12: Introduction to scientific visualisation

Week 13: Storytelling with data / data journalism Creative coding

Week 14: Wrap-Up. Data viz, visualization, data science. Learning Outcomes
By the end of the course, the student must be able to:
Judge visualization in a critical manner and suggest improvements.
Design and implement visualizations from the idea to the final product according to human perception and cognition
Know the common data-viz techniques for each data domain (multivariate data, networks, texts, cartography, etc) with their technical limitations
Create interactive visualizations int he browser using HTM5 and Javascript
Transversal skills
Communicate effectively, being understood, including across different languages and cultures.
Negotiate effectively within the group.
Resolve conflicts in ways that are productive for the task and the people concerned.","Remediation, convergence, and big data: Conceptual limits of cross-platform social mediaThe era of multiplatform media and big data provide new opportunities to reconsider data access by media companies. Outlined here is the discussion surrounding data access from media institutional logic and user-centric perspectives in the contexts of digitalization and ",30,10,0.8192539,1,0.8086314
1006,"Water supply, sewerage, waste management and remediation activities","System Security",0.8172872,"        The first part of the lecture covers individual system aspects starting with tamperproof or tamper-resistant hardware in general over operating system related security mechanisms to application software systems, such as host based intrusion detection systems. In the second part, the focus is on system design and methodologies for building secure systems.        In this lecture, students learn about the security requirements and capabilities that are expected from modern hardware, operating systems, and other software environments. An overview of available technologies, algorithms and standards is given, with which these requirements can be met.        The first part of the lecture covers individual system's aspects starting with tamperproof or tamperresistant hardware in general over operating system related security mechanisms to application software systems such as host based intrusion detetction systems. The main topics covered are: tamper resistant hardware, CPU support for security, protection mechanisms in the kernel, file system security (permissions / ACLs / network filesystem issues), IPC Security, mechanisms in more modern OS, such as Capabilities and Zones, Libraries and Software tools for security assurance, etc.

In the second part, the focus is on system design and methodologies for building secure systems. Topics include: patch management, common software faults (buffer overflows, etc.), writing secure software (design, architecture, QA, testing), compiler-supported security, language-supported security, logging and auditing (BSM audit, dtrace, ...), cryptographic support, and trustworthy computing (TCG, SGX).

Along the lectures, model cases will be elaborated and evaluated in the exercises.","Apparatus and method of leveraging semi-supervised machine learning principals to perform root cause analysis and derivation for remediation of issues in a Embodiments of the innovation relate to a host device having a memory and a processor, the host device configured to determine an anomaly associated with an attribute of a computer environment resource of the computer infrastructure. The host device is configured ",30,3,0.8086568,1,0.8172872
1009,"Water supply, sewerage, waste management and remediation activities","DD2437 Artificial Neural Networks and Deep Architectures",0.8086674,"After completing the course the student should be able to

describe the structure and function of the most common artificial neural network (ANN) types, e.g. multi-layer perceptron, recurrent network, self-organizing maps, Boltzmann machine, deep belief network, autoencoder, and provide examples of their applications
explain mechanisms of supervised/unsupervised learning from data and information processing in different ANN architectures, and also account for derivations of the basic ANN algorithms discussed in the course
demonstrate when and how deep architectures lead to increased performance in pattern recognition and data mining problems
quantitatively analyse the process and outcomes of learning in ANNs, and account for their shortcomings, limitations
apply, validate and evaluate suggested types of ANNs in typical small problems in the realm of regression, prediction, pattern recognition, scheduling and optimisation
devise and implement ANN approaches to selected problems in pattern recognition, system identification or predictive analytics using commonly available development tools, and critically examine their applicability
in order to

obtain an understanding of the technical potential as well as advantages and limitations of today's learning, adaptive and self-organizing systems,
acquire the ANN practitioner’s competence to apply and develop ANN based solutions to data analytics problems.
Course main content
The course is concerned with computational problems in massively parallel artificial neural network (ANN) architectures, which rely on distributed simple computational nodes and robust learning algorithms that iteratively adjust the connections between the nodes heavily using the available data samples. The learning rule and network architecture determine specific computational properties of the ANN. The course offers an opportunity to develop the conceptual and theoretical understanding of computational capabilities of ANNs starting from simpler systems and progressively studying more advanced architectures, and hence exploring the breadth of learning types – from strictly supervised to purely explorative unsupervised mode. The course content therefore includes among others multi-layer perceptrons (MLPs), self-organising maps (SOMs), Boltzmann machines, Hopfield networks and state-of-the-art deep neural networks (DNNs) along with the corresponding learning algorithms. An important objective of the course is for the students to gain practical experience of selecting, developing, applying and validating suitable networks and algorithms to effectively address a broad class of regression, classification, temporal prediction, data modelling, explorative data analytics or clustering problems. Finally, the course provides revealing insights into the principles of generalisation capabilities of ANNs, which underlie their predictive power.
","Apparatus and method of leveraging semi-supervised machine learning principals to perform root cause analysis and derivation for remediation of issues in a Embodiments of the innovation relate to a host device having a memory and a processor, the host device configured to determine an anomaly associated with an attribute of a computer environment resource of the computer infrastructure. The host device is configured ",30,33,0.8290868,1,0.8086674
1017,"Real estate activities","Systems for data science",0.7989482,"The course covers fundamental principles for understanding and building systems for managing and analyzing large amounts of data.Programming methods, including parallel programming:

Data-parallel programming: Collection abstractions and modern collection libraries.¿
Data-flow parallelism vs. message passing. The bulk-synchronous parallel programming model.
SQL and relational algebra. Expressing advanced problems as queries.
Big data systems design and implementation:

Scalability. Synchrony. Distributed systems architectures.
Data locality. Memory hierarchies. New hardware. Sequential versus random access to secondary storage. Partitioning and replication. Data layouts ¿ column stores.
Massively parallel processing operations ¿ joins and sorting
Query optimization. Index selection. Physical database design. Database tuning.
Challenges of big data machine learning systems.
Changing data:

Introduction to transaction processing: purpose, anomalies serializability; concurrency
Commits and consensus.¿
Eventual consistency. The CAP theorem. NoSQL and NewSQL systems.
Online / Streaming / Real-time analytics:

Data stream processing. Windows. Load shedding.
""Small data""/online aggregation: Sampling and approximating aggregates.
Incremental and online query processing: incremental view maintenance and materialized views.
¿Data warehousing: The data warehousing workflow, ETL. OLAP, Data Cubes¿
Keywords
Databases, data-parallel programming, NoSQL systems, query processing.Learning Outcomes
By the end of the course, the student must be able to:
Choose systems parameters, data layouts, query plans, and application designs for database systems and applications.
Develop data-parallel analytics programs that make use of modern clusters and cloud offerings to scale up to very large workloads.
Analyze the trade-offs between various approaches to large-scala data management and analytics, depending on efficiency, scalability, and latency needs
Choose the most appropriate existing systems architecture and technology for a task","Automation of the technical due diligence with artificial intelligence in the real estate industryOver the real estate lifecycle numerous documents and data are generated. The majority of building-related data is collected in day-to-day operations, such as maintenance protocols, contracts or energy consumptions. Previous successes in the classification already help to ",60,21,0.8212095,1,0.7989482
1031,"Real estate activities","Decision Support Sytems",0.808846,"The DSS are interactive computer systems, the aim of which is to help decision
makers in the use of data and models to solve unstructured problems.
The DSS emerged in 1970s to solve complex situations in which individuals have to
choose between several possible alternatives and follow the optimal or a satisfactory
one. For this decision making, the experience, common sense or intuitions of experts
are not enough since often multiple conflicting criteria usually exist including,
uncertainty, several decision makers and various stages. The endless versatility of
real-world human decision problems has triggered necessary efforts in multiple areas
in order to build a sequence of coherent schemes (patterns), increasingly broader to
approach decision making problems correctly. This module will focus on exposing the
foundations and applications of the main lines of current development of Decision
Processes, studying different tools and software that have emerged in recent years
for the modelling and evaluation of decision making problems in uncertain
environments","Artificial intelligence and machine learning: current applications in real estateReal estate meets machine learning: real contribution or just hype? Creating and managing the built environment is a complicated task fraught with difficult decisions, challenging relationships, and a multitude of variables. Today's technology experts are building ",60,2,0.8154595,1,0.808846
1032,"Real estate activities","Deep Learning ",0.7944924,"The discipline of machine learning in artificial intelligence provides useful solutions to be used, for example, in data science or in the development of autonomous systems (e.g., robots or unmanned vehicles). With the arrival of big data and the increase of computational power (e.g., parallelization using graphics processing units - GPUs), the approach of deep learning has emerged from the connectionist branch of machine learning with new architectures, algorithms and hybridations with other techniques such as evolutionary algorithms. Deep learning has proved to be significantly better than other approaches to solve problems that cope with large amounts of data as it is required, for example, in computer vision (image or video processing), in speech understanding, or in classification, besides other problems. This seminar presents a theoretical and practical view of deep learning. The seminar presents first the foundations of artificial neural networks with both supervised and unsupervised learning. Then, the seminar presents different types of deep architectures (e.g., convolutional neural networks, etc.) and application domains (e.g., computer vision). To complement the practical view, the seminar also presents specialized software tools for deep learning and describes how to use them in practical problems.","Artificial intelligence and machine learning: current applications in real estateReal estate meets machine learning: real contribution or just hype? Creating and managing the built environment is a complicated task fraught with difficult decisions, challenging relationships, and a multitude of variables. Today's technology experts are building ",60,16,0.8254202,1,0.7944924
1034,"Real estate activities","Deep Learning",0.7892066,"Deep learning is an area within machine learning that deals with algorithms and models that automatically induce multi-level data representations.        In recent years, deep learning and deep networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing. This class will cover the mathematical foundations of deep learning and provide insights into model design, training, and validation. The main objective is a profound understanding of why these methods work and how. There will also be a rich set of hands-on tasks and practical projects to familiarize students with this emerging technology.This is an advanced level course that requires some basic background in machine learning. More importantly, students are expected to have a very solid mathematical foundation, including linear algebra, multivariate calculus, and probability. The course will make heavy use of mathematics and is not (!) meant to be an extended tutorial of how to train deep networks with tools like Torch or Tensorflow, although that may be a side benefit.

The participation in the course is subject to the following conditions:
1) The number of participants is limited to 300 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge, see exhaustive list below:

Machine Learning
https://ml2.inf.ethz.ch/courses/ml/

Computational Intelligence Lab
http://da.inf.ethz.ch/teaching/2018/CIL/ 

Learning and Intelligent Systems/Introduction to Machine Learning
https://las.inf.ethz.ch/teaching/introml-S18

Statistical Learning Theory
http://ml2.inf.ethz.ch/courses/slt/

Computational Statistics
https://stat.ethz.ch/lectures/ss18/comp-stats.php

Probabilistic Artificial Intelligence
https://las.inf.ethz.ch/teaching/pai-f17

Data Mining: Learning from Large Data Sets
https://las.inf.ethz.ch/teaching/dm-f17","Artificial intelligence and machine learning: current applications in real estateReal estate meets machine learning: real contribution or just hype? Creating and managing the built environment is a complicated task fraught with difficult decisions, challenging relationships, and a multitude of variables. Today's technology experts are building ",60,13,0.8194674,1,0.7892066
1042,"Real estate activities","Advanced Algorithms",0.8379683,"This is an advanced course on the design and analysis of algorithms, covering a range of topics and techniques not studied in typical introductory courses on algorithms.        This course is intended to familiarize students with (some of) the main tools and techniques developed over the last 15-20 years in algorithm design, which are by now among the key ingredients used in developing efficient algorithms.        the lectures will cover a range of topics, including the following: graph sparsifications while preserving cuts or distances, various approximation algorithms techniques and concepts, metric embeddings and probabilistic tree embeddings, online algorithms, multiplicative weight updates, streaming algorithms, sketching algorithms, and a bried glance at MapReduce algorithms.","Comparison of expert algorithms with machine learning models for real estate appraisalMachine learning models require numerous training examples to provide reliable predictions of real estate prices. Expert algorithms could be applied wherever only several training samples are available. The accuracy of two expert algorithms based on the sales ",60,1,0.8379683,1,0.8379683
1043,"Real estate activities","Model Driven Engineering",0.8337996,"You should be able to build a model for a simple application in each of the formalisms discussed. When building these models you pay sufficiently attention to their quality: do they have a clear structure, is the level of abstraction the right one, do they contain sufficient information to express relevant properties.
You are also expected to show that you are able to use the different tools for simulation, verification and transformation for the models produced, and that you can explain the pros and cons of the various models.
The purpose of the course is to introduce you to a few (say, 3) typical modeling languages used in software engineering, and the tools that are based on them. In the model-driven approach to software development, a software system is seen as a cluster of models, on various levels of abstraction and with various characteristics. Each of these models captures certain features or aspects of the systems, allows its own kind of analysis, and has its own tools available. In this way one may apply the many sophisticated tools and theories that have been developed for particular models by the research community. It is clear, however, that this will not work without powerful tools for integrating the various models, transforming them into one another, generating code from them, and keeping them consistent. The course introduces students to this area, concentrating on the use of a concrete, rule based  transformation engine.","Comparison of expert algorithms with machine learning models for real estate appraisalMachine learning models require numerous training examples to provide reliable predictions of real estate prices. Expert algorithms could be applied wherever only several training samples are available. The accuracy of two expert algorithms based on the sales ",60,9,0.8152347,1,0.8337996
1044,"Real estate activities","Probabilistic Artificial Intelligence",0.8310184,"This course introduces core modeling techniques and algorithms from statistics, optimization, planning, and control and study applications in areas such as sensor networks, robotics, and the Internet.        How can we build systems that perform well in uncertain environments and unforeseen situations? How can we develop systems that exhibit ""intelligent"" behavior, without prescribing explicit rules? How can we build systems that learn from experience in order to improve their performance? We will study core modeling techniques and algorithms from statistics, optimization, planning, and control and study applications in areas such as sensor networks, robotics, and the Internet. The course is designed for upper-level undergraduate and graduate students.        Topics covered:
- Search (BFS, DFS, A*), constraint satisfaction and optimization
- Tutorial in logic (propositional, first-order)
- Probability
- Bayesian Networks (models, exact and approximative inference, learning) - Temporal models (Hidden Markov Models, Dynamic Bayesian Networks)
- Probabilistic palnning (MDPs, POMPDPs)
- Reinforcement learning
- Combining logic and probability","Comparison of expert algorithms with machine learning models for real estate appraisalMachine learning models require numerous training examples to provide reliable predictions of real estate prices. Expert algorithms could be applied wherever only several training samples are available. The accuracy of two expert algorithms based on the sales ",60,3,0.8255734,1,0.8310184
1052,"Real estate activities","Statistical Modelling of Spatial Data",0.8467315,"In environmental sciences one often deals with spatial data. When analysing such data the focus is either on exploring their structure (dependence on explanatory variables, autocorrelation) and/or on spatial prediction. The course provides an introduction to geostatistical methods that are useful for such analyses.        The course will provide an overview of the basic concepts and stochastic models that are used to model spatial data. In addition, participants will learn a number of geostatistical techniques and acquire familiarity with R software that is useful for analyzing spatial data.        After an introductory discussion of the types of problems and the kind of data that arise in environmental research, an introduction into linear geostatistics (models: stationary and intrinsic random processes, modelling large-scale spatial patterns by linear regression, modelling autocorrelation by variogram; kriging: mean square prediction of spatial data) will be taught. The lectures will be complemented by data analyses that the participants have to do themselves.","Machine Learning Vs. Spatial Econometric Models: Modeling the Impact of Transportation Infrastructure on Real Estate PricesLinear regression with Ordinary Least Squares and spatial econometric models are statistical methods widely employed to measure the impact of transportation infrastructure locations on real estate prices. Efthymiou and Antoniou (1, 2, 3) developed different types of ",60,5,0.8292649,1,0.8467315
1054,"Real estate activities","Time Series Analysis",0.8291319,"Statistical analysis and modeling of observations in temporal order, which exhibit dependence. Stationarity, trend estimation, seasonal decomposition, autocorrelations,
spectral and wavelet analysis, ARIMA-, GARCH- and state space models. Implementations in the software R.        Understanding of the basic models and techniques used in time series analysis and their implementation in the statistical software R.        This course deals with modeling and analysis of variables which change randomly in time. Their essential feature is the dependence between successive observations.
Applications occur in geophysics, engineering, economics and finance. Topics covered: Stationarity, trend estimation, seasonal decomposition, autocorrelations,
spectral and wavelet analysis, ARIMA-, GARCH- and state space models. The models and techniques are illustrated using the statistical software R.","Machine Learning Vs. Spatial Econometric Models: Modeling the Impact of Transportation Infrastructure on Real Estate PricesLinear regression with Ordinary Least Squares and spatial econometric models are statistical methods widely employed to measure the impact of transportation infrastructure locations on real estate prices. Efthymiou and Antoniou (1, 2, 3) developed different types of ",60,6,0.8355734,1,0.8291319
1055,"Real estate activities","Language Engineering",0.8231615,"Language Engineering (LE) is the set of techniques, resources and tools to solve problems by using more or less an automated language. This course aims to introduce students to the overall framework, which is currently the LE. The second part of the subject will explain the two main principles of most language treatment systems, such as the content representation models and the creation and maintenance of lexical resources, both pillars of any system and any use. In the third part of the course the student will be introduced three of the major commercial applications of LE, such as information retrieval (associated with the search for data or items of information in a text) and text mining, where besides extracting data type information, we will extract relationships between them. The existing application on the market, and the more immediate trends (for example the analysis of forums for opinions) will also be discussed and explained.","Broad application of artificial intelligence for document classification, information extraction and predictive analytics in real estateReal estate represents a major share of economic activities and wealth in all economies. Due to the lack of widely acknowledged standards, however, the structuring, providing and managing of a life cycle-comprehensive building documentation yet remain challenging ",60,6,0.8069792,1,0.8231615
1058,"Real estate activities","Information Retrieval",0.8149164,"The way we access, provide, and exchange information has changed dramatically with the rise of the Internet. Information retrieval studies and invents methods and techniques for the design, implementation, and use of information processing technology in the context of a variety of Internet applications, ranging from search engines to text analysis.

Information Retrieval has developed from a number of research areas, including Computer Science, Library Science, Artificial Intelligence, Data Mining, and Natural Language Processing. While Information Retrieval builds on techniques from a variety of research areas, there are a number of research problems that are specific to the Web applications, such as the design of Internet search engines, efficient linking of related information across the Web, improving information extraction from social networking sites, and the access of foreign language information. In addition, the sheer scale of the Internet opens up tremendous opportunities for data mining approaches, while at the same time posing interesting research challenges with respect to robustness and scalability.

Within the Information Retrieval profile you will be familiarized with several data mining, natural language processing, and link-based techniques that are not only relevant to this profile but also to many other Artificial Intelligence applications. It covers the well-established techniques within the area but is also looks forward, discussing the science behind cutting-edge technologies and anticipating Web technologies that yet have to be fully realized. 

 ","Broad application of artificial intelligence for document classification, information extraction and predictive analytics in real estateReal estate represents a major share of economic activities and wealth in all economies. Due to the lack of widely acknowledged standards, however, the structuring, providing and managing of a life cycle-comprehensive building documentation yet remain challenging ",60,48,0.8137984,1,0.8149164
1060,"Real estate activities","Massively Parallel Algorithms",0.8056562,"Data sizes are growing faster than the capacities of single processors. This makes it almost a certainty that the future of computation will rely on parallelism. In this new graduate-level course, we discuss the expanding body of work on the theoretical foundations of modern parallel computation, with an emphasis on the algorithmic tools and techniques for large-scale processing.        This course will familiarize the students with the algorithmic tools and techniques in modern parallel computation. In particular, we will discuss the growing body of algorithmic results in the Massively Parallel Computation (MPC) model. This model is a mathematical abstraction of some of the popular large-scale processing settings such as MapReduce, Hadoop, Spark, etc. By the end of the semester, the students will know all the standard tools of this area, as well as the state of the art on a number of the central problems. Our hope is that the course prepares the students for independent research at the frontier of this area, and we will attempt to move in that direction with the course projects. 

The course assumes no particular familiarity with parallel computation and should be accesible to any student with sufficient theoretical/algorithmic background. In particular, we expect that all students are comfortable with the basics of algorithmics designs and analysis, as well as probability theory.        The course will cover a sampling of the recent developments (and open questions) at the frontier of research in massively/modern parallel computation. the material will be based on compilation of recent papers on this area, which will be provided throughout the semester.","[HTML][HTML] Urban Tech on the Rise: Machine Learning Disrupts the Real Estate Industry. Featuring interviews of: Marc Rutzen and Jasjeet Thind by Stanislas Chaillou The practice of AI-powered Urban Analytics is taking off within the real estate industry. Data science and algorithmic logic are close to the forefront of new urban development practices. How close? is the questionexperts predict that digitization will go far beyond intelligent ",60,1,0.8056562,1,0.8056562
1067,"Real estate activities","Artificial Intelligence for Robotics",0.8627103,"This course provides tools from statistics and machine learning enabling the participants to deploy them as part of typical perception pipelines. All methods provided within the course will be discussed in context of and motivated by example applications from robotics. The accompanying exercises will involve implementations and evaluations using typical robotic datasets.        Working knowledge of basic methods from statistics and machine learning.        Probability Recap; Basic Concepts of Machine Learning; Regression; Dimensionality Reduction; Clustering; Support Vector Machines; Deep Learning;","A machine learning approach to big data regression analysis of real estate prices for inferential and predictive purposesThe hedonic price regressions have mainly been used for inference. In contrast, machine learning employed on big data has a great potential for prediction. To contribute to the integration of these two strategies, this article proposes a machine learning approach to the ",60,4,0.8307303,1,0.8627103
1071,"Real estate activities","Algorithmic Foundations of Data Science",0.8084383,"You have insight in probabilistic approximation algorithms
You know various algorithms and techniques that compute properties of big quantities of data in an efficient way
You understand how the quality of approximations can be improved by means of hash functions and aggregates of different runs of the algorithms.
You can analyse new algorithms by means of Chernoff and Chebyshev bounds.This course is about algorithms that have been developed for big data. The algorithms are mostly sketch or summary-based, i.e., based on a small summary of the data, properties of interest are computed in an approximate way. The correctness of these algorithms often require some kind of probabilistic analysis. As a consequence, the course involves a substantial amount of mathematics and of probability theory in particular.

The course covers algorithms for

Approximate counting (Morris, KMV, (Hyper)Loglog)
Approximate frequency (Misra-Gries, Count Min, Count Sketch)
Heavy hitters 
Approximate moments (AGMS sketch)
Property testing (sorting, clustering)
Sublinear approximation algorithms (number of connected components)
Locality Sensitive Hashing (LSH)
This list may change from year to year.","Comparing three machine learning algorithms in the task of appraising commercial real estateIn a unique opportunity to examine rare appraisal data from the commercial real estate sector, the accuracy of three machine learning algorithms is compared in the task of appraising commercial real estate. The algorithms; random forests, support vector ",60,1,0.8084383,1,0.8084383
1074,"Real estate activities","Machine Perception",0.7978703,"Recent developments in neural network (aka “deep learning”) have drastically advanced the performance of machine perception systems in a variety of areas including drones, self-driving cars and intelligent UIs. This course is a deep dive into details of the deep learning algorithms and architectures for a variety of perceptual tasks.        Students will learn about fundamental aspects of modern deep learning approaches for perception. Students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in learning-based computer vision, robotics and HCI. The final project assignment will involve training a complex neural network architecture and applying it on a real-world dataset of human motion.

The core competency acquired through this course is a solid foundation in deep-learning algorithms to process and interpret human input into computing systems. In particular, students should be able to develop systems that deal with the problem of recognizing people in images, detecting and describing body parts, inferring their spatial configuration, performing action/gesture recognition from still images or image sequences, also considering multi-modal data, among others.        We will focus on teaching how to set up the problem of machine perception, the learning algorithms (e.g. backpropagation), practical engineering aspects as well as advanced deep learning algorithms including generative models.

The course covers the following main areas:
I) Machine-learning algorithms for input recognition, computer vision and image classification (human pose, object detection, gestures, etc.)
II) Deep-learning models for the analysis of time-series data (temporal sequences of motion)
III) Learning of generative models for synthesis and prediction of human activity. 

Specific topics include: 
• Deep learning basics:
○ Neural Networks and training (i.e., backpropagation)
○ Feedforward Networks
○ Recurrent Neural Networks
• Deep Learning techniques user input recognition:
○ Convolutional Neural Networks for classification
○ Fully Convolutional architectures for dense per-pixel tasks (i.e., segmentation)
○ LSTMs & related for time series analysis
○ Generative Models (GANs, Variational Autoencoders)
• Case studies from research in computer vision, HCI, robotics and signal processing.         This is an advanced grad-level course that requires a background in machine learning. Students are expected to have a solid mathematical foundation, in particular in linear algebra, multivariate calculus, and probability. The course will focus on state-of-the-art research in deep-learning and is not meant as extensive tutorial of how to train deep networks with Tensorflow..

Please take note of the following conditions:
1) The number of participants is limited to 100 students (MSc and PhDs).
2) Students must have taken the exam in Machine Learning (252-0535-00) or have acquired equivalent knowledge
3) All practical exercises will require basic knowledge of Python and will use libraries such as TensorFlow, scikit-learn and scikit-image. We will provide introductions to TensorFlow and other libraries that are needed but will not provide introductions to basic programming or Python. 

The following courses are strongly recommended as prerequisite:
* ""Machine Learning"" 
* ""Visual Computing"" or ""Computer Vision"" 

The course will be assessed by a final written examination in English. No course materials or electronic devices can be used during the examination. Note that the examination will be based on the contents of the lectures, the associated reading materials and the exercises.","Comparing three machine learning algorithms in the task of appraising commercial real estateIn a unique opportunity to examine rare appraisal data from the commercial real estate sector, the accuracy of three machine learning algorithms is compared in the task of appraising commercial real estate. The algorithms; random forests, support vector ",60,24,0.819487,1,0.7978703
