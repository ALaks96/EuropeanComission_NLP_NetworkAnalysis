{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did in prototype 1, we use spark dataframes instead of pandas dataframes to match research article content to AI course content. In this script we use a mix of spark dataframes and RDD's and as in poc1 we train a word2vec model on our data (loagGoogleModel is broken). We compute the cosine similarity for each combination of vector representation of strings, and when the score is higher than a threshold we record the match. We then retrieve all the additionnal information from each dataset and join it in one full dataset. \n",
    "Given that we can't use Google's Word2Vec model here, the results are a bit less acurate. Although we gain time with the pre-processing steps and word2vec training and fitting. This also allows our NLP to be scaled to much larger datasets than the one we're currently working now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "your_path = os.getcwd()\n",
    "findspark.init(your_path + 'spark-2.4.3-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from pyspark.sql import udf\n",
    "from pyspark.sql.types import StructType,StringType,StructField,IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram, Word2Vec, HashingTF, IDF, MinHashLSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading spark dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectorSchema = StructType([# StructField('index', IntegerType(), True)\\,\n",
    "                            StructField(\"Sector\", StringType(), True)\\\n",
    "                           ,StructField(\"text\", StringType(), True)])\n",
    "sectors_pd = pd.read_csv('proto2/articles_full.csv', index_col = 0)\n",
    "sectors = spark.createDataFrame(sectors_pd,schema=sectorSchema)\n",
    "\n",
    "#sectors = (spark.read\n",
    "#    .schema(sectorSchema)\n",
    "#    .option(\"header\", \"true\")\n",
    "#    .csv(\"proto2/articles_full.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "courseSchema = StructType([# StructField('index', IntegerType(), True)\\,\n",
    "                            StructField(\"Courses\", StringType(), True)\\\n",
    "                           ,StructField(\"text\", StringType(), True)])\n",
    "\n",
    "courses_pd = pd.read_csv('proto2/courses_full.csv', index_col = 0)\n",
    "courses = spark.createDataFrame(courses_pd, schema = courseSchema)\n",
    "\n",
    "#courses = (spark.read\n",
    "#    .schema(courseSchema)\n",
    "#    .option(\"header\", \"true\")\n",
    "#    .option(\"mode\", \"DROPMALFORMED\")\n",
    "#    .csv(\"proto2/courses_full.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining pipeline for string similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining stopwords for pre-processing (faster than loading from nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopW = ['i','me','my', 'myself','we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    " 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these',\n",
    " 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    " 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    " 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    " 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    " 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',\n",
    " 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm',\n",
    " 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\",\n",
    " 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan',\n",
    " \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\",\"pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = courses.select(\"text\").toDF(\"text\")\n",
    "\n",
    "model_course = Pipeline(stages=[\n",
    "    RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'text', outputCol = 'tokens'),\n",
    "    StopWordsRemover(stopWords = stopW, inputCol = 'tokens', outputCol = 'tokens_sw'),\n",
    "    NGram(n=2, inputCol=\"tokens_sw\", outputCol=\"ngrams\"),\n",
    "    Word2Vec(vectorSize = 300, minCount = 2, inputCol = 'tokens_sw',outputCol = 'vectors')\n",
    "]).fit(course)\n",
    "\n",
    "course_hashed = model_course.transform(course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = sectors.select(\"text\").toDF(\"text\")\n",
    "\n",
    "sector_hashed = model_course.transform(sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(v1, v2): \n",
    "    return  np.absolute(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|              tokens|           tokens_sw|              ngrams|             vectors|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|This seminar trie...|[this, seminar, t...|[seminar, tries, ...|[seminar tries, t...|[0.00591259398497...|\n",
      "|This seminar will...|[this, seminar, w...|[seminar, cover, ...|[seminar cover, c...|[0.00306318020793...|\n",
      "|The seminar provi...|[the, seminar, pr...|[seminar, provide...|[seminar provides...|[0.01423976779253...|\n",
      "|The DSS are inter...|[the, dss, are, i...|[dss, interactive...|[dss interactive,...|[0.00690956534051...|\n",
      "|In this module, â€œ...|[in, this, module...|[module, satisfic...|[module satisfici...|[0.01031608912197...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|              tokens|           tokens_sw|              ngrams|             vectors|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[PDF][PDF] The Im...|[pdf, pdf, the, i...|[impact, technolo...|[impact technolog...|[0.01357673565871...|\n",
      "|Business intellig...|[business, intell...|[business, intell...|[business intelli...|[-0.0058916396122...|\n",
      "|[PDF][PDF] Is Ind...|[pdf, pdf, is, in...|[india, digitally...|[india digitally,...|[3.38288784191450...|\n",
      "|[PDF][PDF] Bankru...|[pdf, pdf, bankru...|[bankruptcy, risk...|[bankruptcy risk,...|[0.01206909369150...|\n",
      "|Influence of Arti...|[influence, of, a...|[influence, artif...|[influence artifi...|[0.00635130581036...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course_hashed.show(5)\n",
    "sector_hashed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_test1 = course_hashed.count()\n",
    "length_test2 = sector_hashed.count()\n",
    "test1 = course_hashed.select(\"vectors\").rdd\n",
    "test2 = sector_hashed.select(\"vectors\").rdd\n",
    "test1.cache()\n",
    "test2.cache()\n",
    "test3 = test1.collect()\n",
    "test4 = test2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Legal and accounting activities'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sectors_pd.loc[4,'Sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(columns = [\"Sector\",'Article',\"Courses\"])\n",
    "l = 0\n",
    "\n",
    "for i in range(0,length_test1): \n",
    "    for j in range(0,length_test2): \n",
    "        if np.linalg.norm(test3[i][0]) != 0 and np.linalg.norm(test4[j][0]) != 0:\n",
    "            score = cossim(test3[i][0],test4[j][0])\n",
    "            if score > 0.3:\n",
    "                temp.loc[l,['Courses']] = courses_pd.loc[i,[\"Courses\"]]\n",
    "                temp.loc[l,['Sector','Article']] = sectors_pd.loc[j,['Sector','text']]\n",
    "                temp.loc[l,'Similarity'] = score\n",
    "                l += 1\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('NLP_results_proto2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sector pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = sectors.select(\"text\").toDF(\"text\").filter(col('text').isNotNull())\n",
    "\n",
    "# Word2Vec treained on article strings\n",
    "model_sector = Pipeline(stages=[\n",
    "    RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'text', outputCol = 'tokens'),\n",
    "    StopWordsRemover(stopWords = stopW, inputCol = 'tokens', outputCol = 'tokens_sw'),\n",
    "    NGram(n=2, inputCol=\"tokens_sw\", outputCol=\"ngrams\"),\n",
    "    Word2Vec(vectorSize = 100, minCount = 2, inputCol = 'tokens_sw',outputCol = 'vectors')\n",
    "    #HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\"),\n",
    "    #IDF(inputCol=\"rawFeatures\", outputCol=\"features\"),\n",
    "    #MinHashLSH(inputCol=\"rawFeatures\", outputCol=\"lsh\",seed=10)\n",
    "]).fit(sector)\n",
    "\n",
    "# Generating columns defined above\n",
    "sector_hashed = model_sector.transform(sector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating dataframe with matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = model.stages[-1].approxSimilarityJoin(sector_hashed, course_hashed,threshold = 1000000000, distCol=\"EuclideanDistance\")\n",
    "#.select('datasetA','datasetB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for Graph construction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches.withColumn(\"Articles\", matches[\"datasetA\"][\"text\"]).withColumn(\"AI_topics\", matches[\"datasetB\"][\"text\"]).select(\"Articles\",'AI_topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching course labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_join = matches.join(courses, matches.AI_topics == courses.text,how='left') \n",
    "\n",
    "# Free space in cache !\n",
    "matches.unpersist()\n",
    "courses.unpersist()\n",
    "\n",
    "left_join = left_join.select(\"Articles\",'Courses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching skill labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = left_join.join(sectors, left_join.Articles == sectors.text, how = \"left\")\n",
    "\n",
    "# Free space in cache !\n",
    "left_join.unpersist()\n",
    "sectors.unpersist()\n",
    "\n",
    "full_data = full_data.select(\"Courses\",\"Sector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.filter(col('Courses').isNotNull()).filter(col('Sector').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             Courses|              Sector|\n",
      "+--------------------+--------------------+\n",
      "|DT2119 Speech and...|Accomodation and ...|\n",
      "|DISTRIBUTED ALGOR...|Accomodation and ...|\n",
      "|       Linear models|Accomodation and ...|\n",
      "|Optimization for ...|Accomodation and ...|\n",
      "|Project Managemen...|Accomodation and ...|\n",
      "|Algebraic Methods...|Accomodation and ...|\n",
      "|Information Security|Accomodation and ...|\n",
      "|Computability and...|Accomodation and ...|\n",
      "|Shape Modeling an...|Accomodation and ...|\n",
      "|  Probability Theory|Accomodation and ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724.6993787288666\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas conversion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably the most time consuming process of all... If we could somehow avoid having to transfer data as pandas that would be great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = full_data.toPandas()\n",
    "#full_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Courses</th>\n",
       "      <th>Sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT2119 Speech and Speaker Recognition</td>\n",
       "      <td>Accomodation and food service activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DISTRIBUTED ALGORITHMS</td>\n",
       "      <td>Accomodation and food service activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear models</td>\n",
       "      <td>Accomodation and food service activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optimization for Data Science</td>\n",
       "      <td>Accomodation and food service activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Project Management and Risk Control</td>\n",
       "      <td>Accomodation and food service activities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Courses  \\\n",
       "0  DT2119 Speech and Speaker Recognition   \n",
       "1                 DISTRIBUTED ALGORITHMS   \n",
       "2                          Linear models   \n",
       "3          Optimization for Data Science   \n",
       "4    Project Management and Risk Control   \n",
       "\n",
       "                                     Sector  \n",
       "0  Accomodation and food service activities  \n",
       "1  Accomodation and food service activities  \n",
       "2  Accomodation and food service activities  \n",
       "3  Accomodation and food service activities  \n",
       "4  Accomodation and food service activities  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('pyspark__results_proto2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
